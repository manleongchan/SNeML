{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Qt4Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate, interpolate, signal, optimize, stats\n",
    "import cPickle as pickle\n",
    "import lal\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Dropout, BatchNormalization, Flatten\n",
    "from keras.optimizers import Nadam, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import shuffle\n",
    "import pyfftw test\n",
    "import progressbar\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "np.set_printoptions(edgeitems=30, linewidth=160)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for reading simulated SNe waveforms\n",
    "# This code will apply shift to the waveform \n",
    "# samples so that the waveform will always be in the certre +- user customized percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 35.86732719,  44.88071646,  39.44947064,  21.42652943,  13.82658444,  40.20599484,  38.93997062,  95.53031816,  50.38032446,  53.54835976,\n",
       "        56.08423625,  65.17075757,  38.58005884,  33.09333341,  60.59384608,  55.38745052,  48.82042468,  42.18784833,  40.81272286,  46.52884454,\n",
       "        44.64800653,  26.04086609,  16.45763778,  67.08421566,  83.36538094,  74.23510727,  40.3141488 ,  29.14660134,  44.45890709,  40.30240809, ...,\n",
       "       120.5244607 , 116.92591721, 109.42524406, 108.26817496, 120.08794876, 115.38480878, 105.09266416,  87.52083979, 121.32004252, 116.43146497,\n",
       "       116.57478802,   4.57726805,   4.57730856,   4.58054214, 116.62490142, 116.77186416, 116.95489755, 115.88243766, 116.8195391 , 115.55214351,\n",
       "       119.48801883, 119.58046383,   4.58057705,   4.57861104,   4.579349  , 119.0648358 , 119.70945444, 102.2728345 ,  84.50635092, 119.07976288])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the file that contains the simulated CCSN waveforms\n",
    "filename = './Data/GWdatabase.h5'\n",
    "\n",
    "# Read the simulated CCSN waveforms\n",
    "waveformfile = h5py.File(filename, 'r')\n",
    "\n",
    "\n",
    "# The first level keys of the h5 file\n",
    "reduced_data = waveformfile.keys()[0]\n",
    "waveformfilekey = waveformfile.keys()[1]\n",
    "yeofrho = waveformfile.keys()[2]\n",
    "\n",
    "waveformfamily = []\n",
    "waveformfamily_keys = []\n",
    "\n",
    "# Since there are 1824 different simulated CCSN waveform. \n",
    "# Each of which is saved in a different waveformfile key \n",
    "# So the loop below is to retreive all the keys with which the waveform strain data is accessed,\n",
    "# and save it to waveformfamily.\n",
    "# Each waveform family has 5 different keys, so the second part is to retrieve these 5 keys, and save them\n",
    "# to waveformfamily_keys.\n",
    "\n",
    "for i, key in enumerate(waveformfile[waveformfilekey].keys()):\n",
    "    waveformfamily.append(key)\n",
    "    if i == 0:\n",
    "        for j, _ in enumerate(waveformfile[waveformfilekey][waveformfamily[i]].keys()):\n",
    "            waveformfamily_keys.append(waveformfile[waveformfilekey][waveformfamily[i]].keys()[j])\n",
    "originalSNR = np.array(waveformfile[reduced_data][u'SNR(aLIGOfrom10kpc)'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 19,
=======
   "execution_count": 48,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "[u'M_IC(Msun)', u'beta_IC', u'rho_c(g|ccm)', u'strain*dist(cm)', u't-tb(s)']"
      ]
     },
     "execution_count": 19,
=======
       "array([ 13.91828308,   7.53851445,   7.67075525,  17.21671688,  24.13653764,  23.36790117,  33.73246322,  37.57476953,  46.64803883,  56.05362662,\n",
       "        65.06236615,  73.08372241,  79.58905953,  84.63560321,  89.2514441 ,  93.57308329,  97.44961613, 100.7381346 , 104.6485566 , 101.78594498,\n",
       "         3.59355578,  40.0643214 ,  38.7630644 ,  14.16602466,  18.41255522,  24.39641835,  25.50317897,  31.01802585,  37.64197241,  45.34195317, ...,\n",
       "       111.45755458, 114.32552952, 116.83980105, 116.03984614, 118.3792987 , 117.03881574,   1.59042088,   6.24930103,  13.34329414,  22.84844311,\n",
       "        34.29792602,  47.35460913,  61.50317577,  75.39459752,  86.81600418,  93.98425645,  98.37780499, 100.6008361 , 102.88321273,   1.90905236,\n",
       "         7.23169219,  14.97182149,  25.14503731,  37.81186446,  51.65822124,  66.50179174,  79.9744327 ,  88.77845005,  91.41016949,  -1.        ])"
      ]
     },
     "execution_count": 48,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
<<<<<<< Updated upstream
   "source": []
=======
   "source": [
    "originalSNR"
   ]
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd17402f790>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108507, 197, 13156, 1416)\n"
     ]
    }
   ],
   "source": [
    "# This is to set some parameters for the training.\n",
    "# Since the waveforms are stored in the unit of strain * distan\n",
    "# the waveform amplitudes need to be divided by a distance.\n",
    "\n",
    "# Convection factor between par sec and meters\n",
    "PctMe = lal.PC_SI\n",
    "\n",
    "# The distance the waveform will be divided by, in centimeters\n",
    "Dist = 10.0 * 1e3 * PctMe * 1e2\n",
    "\n",
    "# Since the waveform samples come in different lengths, \n",
    "# so every waveform sample will be set to the longest length.\n",
    "# findmax/findmin is a variable that saves the longest/shortest length of the waveform samples.\n",
    "# k/kmin is the index referring to the longest/shortest waveform sample.\n",
    "findmax = 0\n",
    "k = 0 \n",
    "findmin = 1e10\n",
    "kmin = 0\n",
    "#length = np.zeros(len(waveformfamily))\n",
    "#waveformfamily = [waveformfamily[0]]\n",
    "\n",
    "\n",
    "# Since the waveform contains 1824 waveforms, which are different both in the morophology and the duration,\n",
    "# training a network with all these waveforms may make it hard to debug. So one may want to limit the variation\n",
    "# in the waveform samples by limiting the number of waveform samples put in the training. \n",
    "no_waves_considered = 1824\n",
    "for i in range(len(waveformfamily[0:no_waves_considered])):\n",
    "    waveformnumber = i\n",
    "\n",
    "    ts = np.array(waveformfile[waveformfilekey][waveformfamily[waveformnumber]][u't-tb(s)']) \n",
    "    #waves = np.array(waveforms[waveformkey][waveformfamily[waveformnumber]][u'strain*dist(cm)']) / Dist \n",
    "    if findmax < len(ts):\n",
    "        findmax = len(ts)\n",
    "        k = i\n",
    "    if findmin > len(ts):\n",
    "        findmin = len(ts)\n",
    "        kmin = i\n",
    "\n",
    "print(findmax, k, findmin, kmin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simulated waveforms are sampled with a sampling rate equal to 65535 Hz, \n",
    "# coupled with the longest waveform is ~1.66s, this makes the longest waveform contains 1e5 elements. \n",
    "# Since this code will make other waveforms the same length as the longest length, this requires huge amount of memory,\n",
    "# and makes training very slow and difficult. \n",
    "# Therefore, this codes uses scipy.signal.decimate to down sample the waveforms\n",
    "\n",
    "\n",
    "def padandextractwave(waveformfile, waveformfilekey, waveformfamily, strainkey, wavemaxlength, Dist, no_waves_considered, R):\n",
    "    # Number of simulated waveforms considered\n",
    "    noofwaves = len(waveformfamily[0:no_waves_considered])\n",
    "    \n",
    "    msg = 'Reading waveforms from file and downsampling them by a factor of %s............' %(R)\n",
    "    print(msg)\n",
    "    bar = progressbar.ProgressBar(max_value = no_waves_considered)\n",
    "    \n",
    "    # downsample factor, the downsampled waveform will have length = original length / R\n",
    "    \n",
    "    # Vector used to save the downsampled waveform\n",
    "    downsampled_waveforms = np.array([np.zeros(wavemaxlength / R) for i in range(noofwaves)])\n",
    "    \n",
    "    for i, whichsimulation in enumerate(waveformfamily[0:no_waves_considered]):\n",
    "        \n",
    "        # convert the unit of the waveform from strain*distance to strain\n",
    "        wave = np.array(waveformfile[waveformfilekey][whichsimulation][strainkey]) / Dist\n",
    "        wavelength = len(wave)\n",
    "        \n",
    "        # Pad the waveform with zero so that it has the same length as the longest waveform, \n",
    "        # or whatever length is set by wavemaxlength\n",
    "        temporary = np.pad(wave, (0, wavemaxlength - wavelength), 'constant', constant_values = 0)\n",
    "        \n",
    "        # down sample\n",
    "        downsampled_waveforms[i] = signal.decimate(temporary, R, ftype='iir')\n",
    "        bar.update(i + 1)\n",
    "        \n",
    "    return downsampled_waveforms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13564"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SNewaves[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 1824) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading waveforms from file and downsampling them by a factor of 8............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (1823 of 1824) |################### | Elapsed Time: 0:01:46 ETA:   0:00:00"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "\n",
    "# Since the original longest waveform length may not be dividable by the down sample vector, \n",
    "# this is to ensure that the length will be dividable. \n",
    "R = 8\n",
    "findmax = 108512\n",
    "\n",
    "findmax = np.ceil(findmax/8.0) * 8\n",
    "\n",
    "# the assumed observation/simulation duration for every waveform \n",
    "Tobs = findmax / 65535.0\n",
    "#start = time.time()\n",
    "SNewaves = padandextractwave(waveformfile, waveformfilekey, waveformfamily, u'strain*dist(cm)', int(findmax), Dist, no_waves_considered, R)\n",
    "#elapsed = time.time() - start\n",
    "#print(elapsed)\n",
    "# Using the downsampled waveform to compute the new sampling rate\n",
    "New_sr = (len(SNewaves[0]) - 1) / Tobs\n",
    "# the new spacing in time\n",
    "New_dt = 1.0 / New_sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASDtxt(x):\n",
    "    \"\"\"This function reads the following noise curves given a detector name.\"\"\"\n",
    "    return {\n",
    "        'LET':'./ASD/ET_D.txt',\n",
    "        'LCE':'./ASD/CE.txt',\n",
    "        'H1': './ASD/ligoII_NS.txt',\n",
    "        'L1': './ASD/ligoII_NS.txt',\n",
    "        'V1': './ASD/virgoII.txt',\n",
    "        'I2': './ASD/ligoII_NS.txt',\n",
    "        'KAGRA': './ASD/ligoII_NS.txt',\n",
    "        'ET_1': './ASD/ET_D.txt',\n",
    "        'ET_2': './ASD/ET_D.txt',\n",
    "        'ET_3': './ASD/ET_D.txt',\n",
    "        'A2': './ASD/ligoII_NS.txt',\n",
    "        'A2.5': './ASD/ligoII_NS.txt',\n",
    "    }[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readnos(detector, f_points):\n",
    "    \"\"\"This function interpolates the noise given the frequency samples.\"\"\"\n",
    "    nos_file = ASDtxt(detector)\n",
    "    f_str = []\n",
    "    ASD_str = []\n",
    "    file = open(nos_file, 'r')\n",
    "    readFile = file.readlines()\n",
    "    file.close()\n",
    "    f = []\n",
    "    ASD = []\n",
    "    \n",
    "    for line in readFile:\n",
    "        p = line.split()\n",
    "        f_str.append(float(p[0]))\n",
    "        ASD_str.append(float(p[1]))\n",
    "    f = np.log10(np.array(f_str))\n",
    "    ASD = np.log10(np.array(ASD_str))\n",
    "    nosinterpolate = interpolate.splrep(f, ASD, w=1.0*np.ones(len(ASD)), s=0)\n",
    "    \n",
    "    nos = interpolate.splev(np.log10(f_points), nosinterpolate, der = 0, ext = 3)\n",
    "    nos = 10**nos\n",
    "    \n",
    "    return nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisegenerator(Tobs, det, SR, df, dt):\n",
    "    \"\"\"This function generates noise based on amplitude spectral density\"\"\"\n",
    "    \n",
    "    # The number of time stamps\n",
    "    Ns = Tobs * SR \n",
    "    \n",
    "    # The number of the frequency samples\n",
    "    Nf = int(Ns // 2 + 1)\n",
    "    \n",
    "    # The frequency sample\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # read ASD\n",
    "    ASD = readnos(det, fs)\n",
    "    #plt.loglog(fs, ASD)\n",
    "    #plt.show()\n",
    "    #dd\n",
    "    \n",
    "    PSD = ASD ** 2\n",
    "    # scale the ASD by the observation time, and this will be the highest amplitude of the generated noise\n",
    "    Amp = np.sqrt(0.25 *Tobs * PSD)\n",
    "    \n",
    "    \n",
    "    idx = np.argwhere(PSD==0.0)\n",
    "    Amp[idx] = 0.0\n",
    "    \n",
    "    real_nos = Amp * np.random.normal(0.0, 1.0, Nf)\n",
    "    img_nos = Amp * np.random.normal(0.0, 1.0, Nf)\n",
    "    \n",
    "    # This is to ensure there is no strange behaviour from noise at low frequency.\n",
    "    # This is because the interpolation function will interpolate strange values at frequencies betweem 1 - 10Hz.\n",
    "    low_cutoff = 20\n",
    "    high_cutoff = 2048\n",
    "    \n",
    "    idx_1 =  int(low_cutoff/df)\n",
    "    real_nos[0:idx_1] = 0\n",
    "    img_nos[0:idx_1] = 0\n",
    "    idx_2 = int(high_cutoff/df)\n",
    "    real_nos[idx_2:] = 0\n",
    "    img_nos[idx_2:] = 0\n",
    "    \n",
    "    nos = real_nos + 1j * img_nos\n",
    "\n",
    "    \n",
    "    # Fourier transiform converts the generated noise to the tme domain\n",
    "    fftinput = pyfftw.empty_aligned(len(nos), dtype='complex128')\n",
    "    \n",
    "    fft_object = pyfftw.builders.irfft(fftinput)\n",
    "\n",
    "    nos_realization = Ns* fft_object(nos) * df\n",
    "\n",
    "    return nos_realization, fs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Tobs_2 = 100.0\n",
    "SR_2 = 4096.0 * 4\n",
    "dt_2 = 1.0/SR_2\n",
    "df_2 = 1.0/(Tobs_2)\n",
    "N_2 = Tobs_2 * SR_2\n",
    "asd_2, ns_2, fs_2 = noisegenerator(Tobs_2, 'H1', SR_2, df_2, dt_2)\n",
    "\n",
    "fftinput_2 = pyfftw.empty_aligned(len(ns_2), dtype='complex128')     \n",
    "fft_object_2 = pyfftw.builders.rfft(fftinput_2)  \n",
    "ns_2_to_f = fft_object_2(ns_2) * dt_2\n",
    "\n",
    "fftinput_2_t = pyfftw.empty_aligned(len(ns_2_to_f), dtype='complex128')     \n",
    "fft_object_2_t = pyfftw.builders.irfft(fftinput_2_t)  \n",
    "ns_2_to_f_to_t = fft_object_2_t(ns_2_to_f / asd_2) / dt_2\n",
    "\n",
    "plt.plot(np.arange(len(ns_2_to_f_to_t)) * dt_2, ns_2_to_f_to_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tobs_1 = 10.0\n",
    "SR_1 = 4096.0 * 1\n",
    "dt_1 = 1.0/SR_1\n",
    "df_1 = 1.0/(Tobs_1)\n",
    "N_1 = Tobs_1 * SR_1\n",
    "\n",
    "asd_1, ns_1, fs_1, nos = noisegenerator(Tobs_1, 'H1', SR_1, df_1, dt_1)\n",
    "\n",
    "fftinput_1 = pyfftw.empty_aligned(len(ns_1), dtype='complex128')     \n",
    "fft_object_1 = pyfftw.builders.rfft(fftinput_1)  \n",
    "ns_1_to_f = fft_object_1(ns_1) * dt_1\n",
    "\n",
    "fftinput_1_t = pyfftw.empty_aligned(len(ns_1_to_f), dtype='complex128')     \n",
    "fft_object_1_t = pyfftw.builders.irfft(fftinput_1_t)  \n",
    "ns_1_to_f_to_t = fft_object_1_t(ns_1_to_f / asd_1) * dt_1\n",
    "\n",
    "\n",
    "#plt.plot( np.fft.irfft(nos/asd_1)/dt_1)\n",
    "#plt.loglog(fs_1,abs(asd_1))\n",
    "#plt.loglog(fs_1,abs(nos))\n",
    "#plt.loglog(fs_1,abs(np.fft.rfft(ns_1) * dt_1))\n",
    "#plt.xscale('log')\n",
    "#plt.plot(np.arange(len(ns_1_to_f_to_t)) * dt_1, ns_1_to_f_to_t)\n",
    "#plt.plot(ns_1)\n",
    "\n",
    "fontsize = 15\n",
    "plt.loglog(fs_1, asd_1, linewidth = 2, color = 'b', label = 'ASD from a txt file')\n",
    "#plt.plot(dt_1 * np.arange(len(ns_1)), ns_1)\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.xlabel('Observation time(s)', fontsize = fontsize)\n",
    "plt.ylabel('Strain', fontsize = fontsize)\n",
    "plt.grid()\n",
    "\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "\n",
    "text = ax.yaxis.get_offset_text()\n",
    "text.set_size(fontsize)\n",
    "\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "#plt.xlim([0, 10])\n",
    "#plt.ylim([-1.2e-21, 1.2e-21])\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#plt.xlim([0,1])\n",
    "#plt.show()\n",
    "#plt.plot(test)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_to_set_SNR(preset_SNR, SNewaves, dt, Det):\n",
    "    \n",
    "    df = 1.0 / (len(SNewaves[0]) * dt)\n",
    "    fftinput_for_snr = pyfftw.empty_aligned(len(SNewaves[0]), dtype='complex128')     \n",
    "    fft_object_for_snr = pyfftw.builders.rfft(fftinput_for_snr)      \n",
    "    \n",
    "    Nf = int((len(SNewaves[0]) // 2 + 1))\n",
    "    \n",
    "    # frequency samples\n",
    "    fs = np.arange(Nf) * df\n",
    "    O_SNR = np.zeros(len(SNewaves))\n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "    msg = 'Rescaling the amplitude of the waveforms so that their optimal SNR is %s.........' %(preset_SNR)\n",
    "    print(msg)\n",
    "    bar = progressbar.ProgressBar(max_value = len(SNewaves))\n",
    "    \n",
    "    for i, wave in enumerate(SNewaves):\n",
    "        temporary_wave_in_f = fft_object_for_snr(wave) * dt\n",
    "        O_SNR[i] = np.sqrt( 4.0 * sum( abs(temporary_wave_in_f) ** 2 / ASD ** 2 ) * df )\n",
    "        SNR_factor = preset_SNR / O_SNR[i]\n",
    "        \n",
    "        SNewaves[i] = SNR_factor * wave\n",
    "        #print(temporary_snr)\n",
    "        #print(  np.sqrt(4.0 * sum(abs(fft_object_for_snr(SNewaves[i]) * dt) **2 / ASD ** 2) * df))\n",
    "        bar.update(i)\n",
    "    \n",
    "    return SNewaves, O_SNR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(seed, ts, dt, Sr, percentage, Det, SNewaves, N_rz, multiplication):\n",
    "    \"\"\"This function generates the data for training/validation/testing.\"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # The number of sample will be equal to the number of N_rz(noise realizations)\n",
    "    data = np.array([np.zeros_like(ts) for i in range(N_rz)])\n",
    "    \n",
    "    # Signal to noise ratio\n",
    "    #SNR = np.zeros(N_rz)\n",
    "    \n",
    "    # Number of time stamps\n",
    "    Ns = len(ts)\n",
    "    \n",
    "    # Number of frequency samples\n",
    "    Nf = int(Ns //2 + 1)\n",
    "    \n",
    "    # Observation time\n",
    "    Tobs = ts[-1] + dt\n",
    "    \n",
    "    # spacing in the frequency domain\n",
    "    df = 1.0/Tobs\n",
    "    \n",
    "    # frequency samples\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "    \n",
    "    \n",
    "    toolbar_width = N_rz\n",
    "\n",
    "    \n",
    "    \n",
    "    msg = 'Generating noise realizations.......'\n",
    "    print(msg)\n",
    "    # setup toolbar\n",
    "    bar = progressbar.ProgressBar(max_value=toolbar_width)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Generate noise\n",
    "    for i in range(N_rz):\n",
    "        #if (i+1) % 1000 == 0 & i != N_rz - 1:\n",
    "        #   msg = 'The %s th to %s th noise realizations are now being generated.' %(i+1, i+1000)\n",
    "        #    print(msg)\n",
    "        data[i], _ = noisegenerator(Tobs, Det, Sr, df, dt)\n",
    "        bar.update(i+1)\n",
    "\n",
    "\n",
    "\n",
    "    msg = 'Adding noise to signals and converting them back to the time domain after whitening them in the frequency domain.....'\n",
    "    print(msg)\n",
    "    bar_2 = progressbar.ProgressBar(max_value=toolbar_width)\n",
    "    \n",
    "    \n",
    "    if ts[-1] == signal_duration:   \n",
    "\n",
    "        for i in range(multiplication):\n",
    "            for j in range(len(SNewaves)):\n",
    "\n",
    "                count = i * len(SNewaves) + j\n",
    "                #if (count + 1) % 1000 == 0 and count < 4999:\n",
    "                #    msg = 'The %s th to %s th samples of the data set are now being generated.' %(count + 1,count + 1000)\n",
    "                #    print(msg)\n",
    "                data[count] += SNewaves[j]\n",
    "\n",
    "\n",
    "                fftinput_1 = pyfftw.empty_aligned(len(data[count]), dtype='complex128')\n",
    "                fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "                temporary = fft_object_1(data[count]) * 1.0/New_sr\n",
    "                temporary = temporary / ASD \n",
    "\n",
    "\n",
    "                #SNR[count] = np.sqrt(4.0 * sum(abs(temporary[int(100/df): int(500/df)]) ** 2 * df))\n",
    "                #SNR_factor = SNR_set / SNR[count]\n",
    "                #temporary = temporary * SNR_factor\n",
    "                #if SNR_factor > 1:\n",
    "                #    print(SNR_factor,count) \n",
    "                #print(SNR_factor, np.sqrt(4.0 * sum(abs(temporary) ** 2 * df)))\n",
    "                fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "                fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "                data[count] = Ns * fft_object_2(temporary) * df\n",
    "                bar_2.update( count + 1)\n",
    "    elif ts[-1] > signal_duration:\n",
    "        for i in range(multiplication):\n",
    "            for j in range(len(SNewaves)):\n",
    "\n",
    "                count = i * len(SNewaves) + j\n",
    "                #if (count + 1) % 1000 == 0 and count < 4999:\n",
    "                #    msg = 'The %s th to %s th samples of the data set are now being generated.' %(count + 1,count + 1000)\n",
    "                #    print(msg)\n",
    "                # This is to draw a random and determine     \n",
    "                random_shift_percentage = np.random.uniform(-percentage, percentage)\n",
    "                original_starting_point = sample_length / 2 - signal_length / 2\n",
    "                shifted_starting_point = int(original_starting_point * (1 + random_shift_percentage))\n",
    "                \n",
    "                data[count][shifted_starting_point: shifted_starting_point + signal_length] = data[count][shifted_starting_point: shifted_starting_point + signal_length] + SNewaves[j]\n",
    "        \n",
    "                fftinput_1 = pyfftw.empty_aligned(len(data[count]), dtype='complex128')\n",
    "                fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "                temporary = fft_object_1(data[count]) * 1.0/New_sr\n",
    "                temporary = temporary / ASD \n",
    "\n",
    "\n",
    "                #SNR[count] = np.sqrt(4.0 * sum(abs(temporary[int(100/df): int(500/df)]) ** 2 * df))\n",
    "                #SNR_factor = SNR_set / SNR[count]\n",
    "                #temporary[int(100/df): int(500/df)] = temporary[int(100/df): int(500/df)] * SNR_factor\n",
    "                \n",
    "                #if SNR_factor > 1:\n",
    "                #    print(SNR_factor,count) \n",
    "                #print(SNR_factor, np.sqrt(4.0 * sum(abs(temporary) ** 2 * df)))\n",
    "                \n",
    "                fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "                fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "                data[count] = Ns * fft_object_2(temporary) * df\n",
    "                bar_2.update( count + 1 )\n",
    "    else:\n",
    "        raise Exception('The sample length should be longer than or equal to the signal length') \n",
    "\n",
    "            \n",
    "    for i in range(multiplication * len(SNewaves), N_rz):\n",
    "        fftinput_1 = pyfftw.empty_aligned(len(data[i]), dtype='complex128')\n",
    "        fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "        temporary = fft_object_1(data[i]) * 1.0/New_sr\n",
    "        temporary = temporary / ASD \n",
    "        \n",
    "        fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "        fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "        data[i] = Ns * fft_object_2(temporary) * df\n",
    "        bar_2.update(i + 1)\n",
    "            \n",
    "            \n",
    "    return data #SNR\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[1])\n",
    "\n",
    "#plt.plot(data[1824])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data / 1.5e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time stamps \n",
    "signal_length = len(SNewaves[0])\n",
    "signal_duration = (signal_length - 1) * New_dt\n",
    "\n",
    "# applying pad to make the sample longer. This is for the purpose of shifting the signal, so that the signal will appear to be in the centre +- user customised percentage\n",
    "# If no padding is to be applied\n",
    "sample_length = signal_length * 2.0\n",
    "\n",
    "# time stamps after pad\n",
    "ts = np.arange(sample_length) * New_dt\n",
    "sample_duration = ts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_sample[0][0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% (52 of 1824) |                      | Elapsed Time: 0:00:00 ETA:   0:00:03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaling the amplitude of the waveforms so that their optimal SNR is 8.0.........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (1809 of 1824) |################### | Elapsed Time: 0:00:03 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "SNR_set = 8.0\n",
    "SNewaves,  O_SNR = rescale_to_set_SNR(SNR_set, SNewaves, New_dt, 'H1')\n",
    "plt.plot(SNewaves[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (15 of 10000) |                     | Elapsed Time: 0:00:00 ETA:   0:01:10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating noise realizations.......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (43 of 10000) |                     | Elapsed Time: 0:00:00 ETA:   0:00:23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to signals and converting them back to the time domain after whitening them in the frequency domain.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (9963 of 10000) |################## | Elapsed Time: 0:00:24 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "multiplication = 3\n",
    "shift_percentage = 0.2\n",
    "seed = 10\n",
    "Det = 'H1'\n",
    "\n",
    "# Number of noise realization. This will be the final number of data samples for training + validation + testing\n",
    "N_rz = 10000\n",
    "data = data_generator(seed, ts, New_dt, New_sr, shift_percentage, Det, SNewaves, N_rz, multiplication)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_for_test = 100\n",
    "nos_portion = (len(data) - multiplication * len(SNewaves) - save_for_test)/2\n",
    "\n",
    "\n",
    "nos_start = multiplication * len(SNewaves)\n",
    "\n",
    "train_start_index_signal = 0\n",
    "train_end_index_signal = (multiplication * len(SNewaves) - save_for_test) / 2\n",
    "\n",
    "train_start_index_noise = nos_start\n",
    "train_end_index_noise = nos_start + nos_portion\n",
    "\n",
    "\n",
    "val_start_index_signal = train_end_index_signal\n",
    "val_end_index_signal = multiplication * len(SNewaves) - save_for_test\n",
    "\n",
    "val_start_index_noise = train_end_index_noise\n",
    "val_end_index_noise = train_end_index_noise + nos_portion\n",
    "\n",
    "\n",
    "test_start_index_signal = val_end_index_signal\n",
    "test_end_index_signal =  val_end_index_signal + save_for_test\n",
    "\n",
    "test_start_index_noise = val_end_index_noise\n",
    "test_end_index_noise =  val_end_index_noise + save_for_test\n",
    "\n",
    "\n",
    "train_sample = np.concatenate((data[train_start_index_signal:train_end_index_signal], \n",
    "                               data[train_start_index_noise:train_end_index_noise]))\n",
    "\n",
    "train_label = np.concatenate((np.ones(train_end_index_signal - train_start_index_signal), \n",
    "                              np.zeros(train_end_index_noise - train_start_index_noise)))\n",
    "\n",
    "val_sample = np.concatenate((data[val_start_index_signal:val_end_index_signal], \n",
    "                            data[val_start_index_noise:val_end_index_noise]))\n",
    "\n",
    "val_label = np.concatenate((np.ones(val_end_index_signal - val_start_index_signal), \n",
    "                            np.zeros(val_end_index_noise - val_start_index_noise)))\n",
    "\n",
    "\n",
    "test_sample = np.concatenate((data[test_start_index_signal : test_end_index_signal], \n",
    "                             data[test_start_index_noise : test_end_index_noise]))\n",
    "\n",
    "test_label = np.concatenate((np.ones(test_end_index_signal - test_start_index_signal), \n",
    "                            np.zeros(test_end_index_noise - test_start_index_noise)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(train_sample, train_label, val_sample, val_label, test_sample, test_label,  shuffle_times):\n",
    "    \n",
    "    for i in range(shuffle_times):\n",
    "        state = np.random.randint(0,100)\n",
    "        train_sample, train_label = shuffle(train_sample, train_label, random_state=state)\n",
    "\n",
    "        state = np.random.randint(0,100)\n",
    "        test_sample, test_label = shuffle(test_sample, test_label, random_state=state)\n",
    "\n",
    "        state = np.random.randint(0,100)\n",
    "        val_sample, val_label = shuffle(val_sample, val_label, random_state=state)\n",
    "\n",
    "    return train_sample, train_label, val_sample, val_label, test_sample, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample, train_label, val_sample, val_label, test_sample, test_label = shuffle_data(train_sample, train_label, val_sample, val_label, test_sample, test_label,  1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Below is the CNN part of this code\"\"\" \n",
    "\n",
    "batch_size = 30      # number of time series per batch\n",
    "num_classes = 2      # signal or background\n",
    "epochs = 50          # number of full passes of the dataset\n",
    "outdir = './weights' # directory to store results in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sample_for_training = len(train_sample)\n",
    "number_of_sample_for_testing = len(test_sample)\n",
    "number_of_sample_for_validation = len(val_sample)\n",
    "training_sample_length = len(train_sample[0])\n",
    "\n",
    "train_sample = train_sample.reshape(number_of_sample_for_training, 1, training_sample_length)\n",
    "test_sample = test_sample.reshape(number_of_sample_for_testing, 1, training_sample_length)\n",
    "val_sample = val_sample.reshape(number_of_sample_for_validation, 1, training_sample_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_image_data_format('channels_first')\n",
    "training_sample_length = len(train_sample[0][0])\n",
    "\n",
    "train_sample = train_sample.reshape(-1, 1, 1, training_sample_length)\n",
    "val_sample = val_sample.reshape(-1, 1, 1, training_sample_length)\n",
    "test_sample = test_sample.reshape(-1, 1, 1, training_sample_length)\n",
    "\n",
    "input_shape = train_sample.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = keras.utils.to_categorical(train_label , num_classes)\n",
    "val_label = keras.utils.to_categorical(val_label, num_classes)\n",
    "test_label = keras.utils.to_categorical(test_label, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/lib/python2.7/dist-packages/keras/models.pyc'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.models.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 8, 1, 27065)       520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 1, 3383)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 1, 3368)       2064      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 1, 561)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8976)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                287264    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 289,914\n",
      "Trainable params: 289,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()    # define the type of keras model\n",
    "\n",
    "# add the layers\n",
    "# conv1\n",
    "model.add(Conv2D(8, (1,64), activation='elu', input_shape=input_shape))\n",
    "# maxpool2\n",
    "model.add(MaxPool2D((1,8)))\n",
    "# conv2\n",
    "model.add(Conv2D(16, (1,16), activation='elu'))\n",
    "# maxpool2\n",
    "model.add(MaxPool2D((1,6)))\n",
    "# the input the fully connected layer must be 1-D vector\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "# add the output layer with softmax actiavtion for classication\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# print a summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 16, 1, 27065)      1040      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 16, 1, 6766)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 1, 6751)       4112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 16, 1, 1687)       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 26992)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                863776    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 868,994\n",
      "Trainable params: 868,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()    # define the type of keras model\n",
    "\n",
    "# add the layers\n",
    "# conv1\n",
    "model.add(Conv2D(16, (1,64), activation='elu', input_shape=input_shape))\n",
    "# maxpool2\n",
    "model.add(MaxPool2D((1,4)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, (1,16), activation='elu'))\n",
    "# maxpool2\n",
    "model.add(MaxPool2D((1,4)))\n",
    "\n",
    "\n",
    "# the input the fully connected layer must be 1-D vector\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "# add the output layer with softmax actiavtion for classication\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# print a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=SGD(lr = 0.01),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelCheck = ModelCheckpoint('{0}/best_weights.hdf5'.format(outdir), monitor='val_acc', verbose=0, \n",
    "                save_best_only=True,save_weights_only=True, mode='auto', period=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4900 samples, validate on 4900 samples\n",
      "Epoch 1/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.7084 - acc: 0.5196 - val_loss: 0.6918 - val_acc: 0.5494\n",
      "Epoch 2/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.6832 - acc: 0.5580 - val_loss: 0.6905 - val_acc: 0.5216\n",
      "Epoch 3/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.6684 - acc: 0.5951 - val_loss: 0.6853 - val_acc: 0.5480\n",
      "Epoch 4/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.6544 - acc: 0.6247 - val_loss: 0.6822 - val_acc: 0.5624\n",
      "Epoch 5/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.6354 - acc: 0.6565 - val_loss: 0.6751 - val_acc: 0.5751\n",
      "Epoch 6/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.6086 - acc: 0.7112 - val_loss: 0.6720 - val_acc: 0.5706\n",
      "Epoch 7/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.5784 - acc: 0.7412 - val_loss: 0.6541 - val_acc: 0.6020\n",
      "Epoch 8/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.5370 - acc: 0.7635 - val_loss: 0.6482 - val_acc: 0.6088\n",
      "Epoch 9/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.4967 - acc: 0.7841 - val_loss: 0.6359 - val_acc: 0.6304\n",
      "Epoch 10/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.4510 - acc: 0.8178 - val_loss: 0.6394 - val_acc: 0.6329\n",
      "Epoch 11/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.4055 - acc: 0.8339 - val_loss: 0.6503 - val_acc: 0.6378\n",
      "Epoch 12/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.3606 - acc: 0.8629 - val_loss: 0.6568 - val_acc: 0.6429\n",
      "Epoch 13/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.3151 - acc: 0.8859 - val_loss: 0.6808 - val_acc: 0.6424\n",
      "Epoch 14/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.2829 - acc: 0.8996 - val_loss: 0.7059 - val_acc: 0.6431\n",
      "Epoch 15/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.2451 - acc: 0.9157 - val_loss: 0.7339 - val_acc: 0.6355\n",
      "Epoch 16/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.2086 - acc: 0.9367 - val_loss: 0.7736 - val_acc: 0.6308\n",
      "Epoch 17/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.1732 - acc: 0.9531 - val_loss: 0.8220 - val_acc: 0.6349\n",
      "Epoch 18/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.1464 - acc: 0.9622 - val_loss: 0.8757 - val_acc: 0.6388\n",
      "Epoch 19/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.1236 - acc: 0.9700 - val_loss: 0.9294 - val_acc: 0.6357\n",
      "Epoch 20/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.1085 - acc: 0.9776 - val_loss: 0.9689 - val_acc: 0.6333\n",
      "Epoch 21/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.0885 - acc: 0.9798 - val_loss: 1.0277 - val_acc: 0.6361\n",
      "Epoch 22/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0785 - acc: 0.9833 - val_loss: 1.0608 - val_acc: 0.6288\n",
      "Epoch 23/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0686 - acc: 0.9882 - val_loss: 1.1317 - val_acc: 0.6359\n",
      "Epoch 24/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0561 - acc: 0.9910 - val_loss: 1.1560 - val_acc: 0.6304\n",
      "Epoch 25/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0526 - acc: 0.9894 - val_loss: 1.2029 - val_acc: 0.6335\n",
      "Epoch 26/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0474 - acc: 0.9912 - val_loss: 1.2503 - val_acc: 0.6312\n",
      "Epoch 27/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0406 - acc: 0.9937 - val_loss: 1.2768 - val_acc: 0.6322\n",
      "Epoch 28/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0376 - acc: 0.9941 - val_loss: 1.3081 - val_acc: 0.6294\n",
      "Epoch 29/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0322 - acc: 0.9961 - val_loss: 1.3706 - val_acc: 0.6314\n",
      "Epoch 30/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.0300 - acc: 0.9965 - val_loss: 1.3949 - val_acc: 0.6314\n",
      "Epoch 31/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0250 - acc: 0.9982 - val_loss: 1.4340 - val_acc: 0.6316\n",
      "Epoch 32/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0244 - acc: 0.9965 - val_loss: 1.4465 - val_acc: 0.6312\n",
      "Epoch 33/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0240 - acc: 0.9973 - val_loss: 1.4773 - val_acc: 0.6300\n",
      "Epoch 34/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0205 - acc: 0.9978 - val_loss: 1.5039 - val_acc: 0.6302\n",
      "Epoch 35/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0187 - acc: 0.9980 - val_loss: 1.5573 - val_acc: 0.6306\n",
      "Epoch 36/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0165 - acc: 0.9980 - val_loss: 1.5596 - val_acc: 0.6329\n",
      "Epoch 37/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0156 - acc: 0.9988 - val_loss: 1.5880 - val_acc: 0.6329\n",
      "Epoch 38/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0137 - acc: 0.9982 - val_loss: 1.6238 - val_acc: 0.6320\n",
      "Epoch 39/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0136 - acc: 0.9994 - val_loss: 1.6246 - val_acc: 0.6318\n",
      "Epoch 40/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0131 - acc: 0.9988 - val_loss: 1.6741 - val_acc: 0.6312\n",
      "Epoch 41/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0135 - acc: 0.9990 - val_loss: 1.6755 - val_acc: 0.6294\n",
      "Epoch 42/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0124 - acc: 0.9988 - val_loss: 1.6937 - val_acc: 0.6314\n",
      "Epoch 43/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0120 - acc: 0.9988 - val_loss: 1.7201 - val_acc: 0.6316\n",
      "Epoch 44/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0096 - acc: 0.9998 - val_loss: 1.7455 - val_acc: 0.6288\n",
      "Epoch 45/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0115 - acc: 0.9988 - val_loss: 1.7576 - val_acc: 0.6300\n",
      "Epoch 46/50\n",
      "4900/4900 [==============================] - 6s 1ms/step - loss: 0.0103 - acc: 0.9988 - val_loss: 1.7725 - val_acc: 0.6296\n",
      "Epoch 47/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0102 - acc: 0.9988 - val_loss: 1.7854 - val_acc: 0.6312\n",
      "Epoch 48/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0088 - acc: 0.9998 - val_loss: 1.7978 - val_acc: 0.6327\n",
      "Epoch 49/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0079 - acc: 0.9996 - val_loss: 1.8211 - val_acc: 0.6320\n",
      "Epoch 50/50\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0085 - acc: 0.9996 - val_loss: 1.8456 - val_acc: 0.6298\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_sample, train_label,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(val_sample, val_label),\n",
    "                    callbacks = [modelCheck]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 525us/step\n",
      "('Test loss:', 0.801226994395256)\n",
      "('Test accuracy:', 0.6250000119209289)\n"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "model.load_weights('{0}/best_weights.hdf5'.format(outdir))\n",
    "# evaluate\n",
    "eval_results = model.evaluate(test_sample, test_label,\n",
    "                              sample_weight=None,\n",
    "                              batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', eval_results[0])\n",
    "print('Test accuracy:', eval_results[1])\n",
    "signal_preds = model.predict(test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "\n",
    "fig , axs = plt.subplots(2,1, sharex = True)\n",
    "axs = axs.ravel()\n",
    "# plot history\n",
    "axs[0].plot(history.history['loss'], label = 'Loss', linewidth = 2, color = 'b')\n",
    "axs[0].plot(history.history['val_loss'], label = 'Validation Loss', linewidth = 2, color = 'r')\n",
    "axs[1].plot(history.history['acc'], label = 'Accuracy', linewidth = 2, color = 'b')\n",
    "axs[1].plot(history.history['val_acc'], label = 'Validation Accurarcy', linewidth = 2, color = 'r')\n",
    "# set labels\n",
    "axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "# legends\n",
    "axs[0].legend(fontsize = fontsize)\n",
    "axs[1].legend(fontsize = fontsize)\n",
    "# grids\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "axs[0].set_xlim([0, epochs])\n",
    "axs[0].set_ylim(bottom = 0)\n",
    "\n",
    "axs[1].set_xlim([0, epochs])\n",
    "axs[1].set_ylim(top = 1)\n",
    "\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "for ax in axs:\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa, ta, _ = metrics.roc_curve(test_label[:,1], signal_preds[:,1])\n",
    "fig = plt.figure()\n",
    "plt.plot(fa, ta, linewidth = 2, color = 'b')\n",
    "plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "plt.title('ROC curve for SNR %s'%(SNR_set), fontsize = fontsize)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_demo_image():\n",
    "    from matplotlib.cbook import get_sample_data\n",
    "    import numpy as np\n",
    "    f = get_sample_data(\"axes_grid/bivariate_normal.npy\", asfileobj=False)\n",
    "    z = np.load(f)\n",
    "    # z is a numpy array of 15x15\n",
    "    return z, (-3,4,-4,3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5,4])\n",
    "\n",
    "# prepare the demo image\n",
    "Z, extent = get_demo_image()\n",
    "Z2 = np.zeros([150, 150], dtype=\"d\")\n",
    "ny, nx = Z.shape\n",
    "Z2[30:30+ny, 30:30+nx] = Z\n",
    "\n",
    "# extent = [-3, 4, -4, 3]\n",
    "ax.imshow(Z2, extent=extent, interpolation=\"nearest\",\n",
    "          origin=\"lower\")\n",
    "\n",
    "axins = zoomed_inset_axes(ax, 6, loc=1) # zoom = 6\n",
    "#axins.imshow(Z2, extent=extent, interpolation=\"nearest\",\n",
    "#             origin=\"lower\")\n",
    "\n",
    "# sub region of the original image\n",
    "x1, x2, y1, y2 = -1.5, -0.9, -2.5, -1.9\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "\n",
    "plt.xticks(visible=False)\n",
    "plt.yticks(visible=False)\n",
    "\n",
    "# draw a bbox of the region of the inset axes in the parent axes and\n",
    "# connecting lines between the bbox and the inset axes area\n",
    "#mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting SNR\n",
    "fontsize = 26\n",
    "plt.scatter(np.arange(1824)+1, originalSNR, color='b', label = 'Waveform SNR from waveform file')\n",
    "plt.scatter(np.arange(1824)+1, SNR, color = 'r', label='SNR as computed using the optimal SNR formula')\n",
    "plt.xlabel('Index of waveform samples', fontsize=fontsize)\n",
    "plt.ylabel('SNR for signal from 100Hz to 500hz', fontsize =fontsize)\n",
    "plt.grid()\n",
    "plt.legend(fontsize=fontsize)\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting waveforms between original and downsampled\n",
    "from mpl_toolkits.axes_grid.inset_locator import inset_axes\n",
    "\n",
    "plt.plot(np.arange(len(original[0])) * original_dt, original[0], linewidth = 2, color = 'b', label = 'Original')\n",
    "\n",
    "plt.plot(np.arange(len(SNewaves[0])) * New_dt, SNewaves[0], linewidth = 2, color = 'r', label = 'Downsampled by a factor of 8')\n",
    "plt.legend()\n",
    "fontsize =26\n",
    "plt.xlabel('Time (s)', fontsize = fontsize)\n",
    "plt.ylabel('Strain', fontsize =fontsize)\n",
    "plt.legend(fontsize = fontsize, loc = 'lower right')\n",
    "plt.xlim([0.16, 0.3])\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "text = ax.yaxis.get_offset_text()\n",
    "text.set_size(fontsize)\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "inset_axes = inset_axes(ax, width=\"40%\", # width = 30% of parent_bbox\n",
    "                    height=2.0, # height : 1 inch\n",
    "                    loc=1)\n",
    "plt.grid()\n",
    "plt.plot(np.arange(len(original[0])) * original_dt, original[0], linewidth = 2, color = 'b', label = 'Original')\n",
    "\n",
    "plt.plot(np.arange(len(SNewaves[0])) * New_dt, SNewaves[0], linewidth = 2, color = 'r', label = 'Downsampled by a factor of 8')\n",
    "plt.xlim([0.165, 0.195])\n",
    "plt.ylim([-4e-22, 3e-22])\n",
    "\n",
    "x = [0.165, 0.180, 0.195]\n",
    "y = [-1.75e-22, -1.5e-22, -1.25e-22, -1.0e-22]\n",
    "\n",
    "plt.xticks(x)\n",
    "\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "text = ax.yaxis.get_offset_text()\n",
    "text.set_size(fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(data[12])) * New_dt, data[12], linewidth = 2, color = 'b', label = 'Whitened signal example')\n",
    "plt.legend()\n",
    "fontsize =26\n",
    "plt.xlabel('Time (s)', fontsize = fontsize)\n",
    "plt.ylabel('Strain', fontsize =fontsize)\n",
    "plt.legend(fontsize = fontsize, loc = 'upper right')\n",
    "#plt.xlim([0.16, 0.3])\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "text = ax.yaxis.get_offset_text()\n",
    "text.set_size(fontsize)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fontsize =22\n",
    "fig , axs = plt.subplots(2,1, sharex = True)\n",
    "axs = axs.ravel()\n",
    "# plot history\n",
    "axs[0].plot(history.history['loss'], linewidth =2, color ='r', label = 'loss')\n",
    "axs[0].plot(history.history['val_loss'], linewidth =2, color ='b',label = 'val loss')\n",
    "axs[1].plot(history.history['acc'], linewidth =2, color ='r',label = 'acc')\n",
    "axs[1].plot(history.history['val_acc'], linewidth =2, color ='b',label = 'val_acc')\n",
    "# set labels\n",
    "axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "# legends\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "# grids\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "\n",
    "\n",
    "ax = axs[0]\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "text = ax.yaxis.get_offset_text()\n",
    "text.set_size(fontsize)\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "text = ax.yaxis.get_offset_text()\n",
    "text.set_size(fontsize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toolbar_width = 10000/100\n",
    "\n",
    "# setup toolbar\n",
    "bar = progressbar.ProgressBar(max_value=10000)\n",
    "\n",
    "# Generate noise\n",
    "for i in range(10000):\n",
    "    #if (i+1) % 1000 == 0 & i != N_rz - 1:\n",
    "    #    msg = 'The %s th to %s th noise realizations are now being generated.' %(i+1, i+1000)\n",
    "    #    print(msg)\n",
    "    #data[i], _ = noisegenerator(Tobs, Det, Sr, df, dt)\n",
    "    #print((i+1)%100)\n",
    "    if (i+1) % 100 == 0:\n",
    "    # update the bar\n",
    "        print(( float(i+1) / N_rz))\n",
    "        bar.update( int(float(i+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycbc.noise\n",
    "import pycbc.psd\n",
    "import pylab\n",
    "\n",
    "# The color of the noise matches a PSD which you provide\n",
    "flow = 20.0\n",
    "delta_f = 1.0 / 10\n",
    "flen = int(2048 / delta_f) + 1\n",
    "psd = pycbc.psd.aLIGOZeroDetHighPower(flen, delta_f, flow)\n",
    "\n",
    "# Generate 32 seconds of noise at 4096 Hz\n",
    "delta_t = 1.0 / 4096\n",
    "tsamples = int(10 / delta_t)\n",
    "ts = pycbc.noise.noise_from_psd(tsamples, delta_t, psd, seed=127)\n",
    "\n",
    "#pylab.plot(ts.sample_times, ts)\n",
    "plt.loglog(np.arange(flen) * delta_f, np.sqrt(np.array(psd)), linewidth = 2, color = 'r', label = 'Pycbc')\n",
    "\n",
    "fontsize = 15\n",
    "#plt.plot(dt_1 * np.arange(len(ns_1)), ns_1)\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.xlabel('Observation time(s)', fontsize = fontsize)\n",
    "plt.ylabel('Strain', fontsize = fontsize)\n",
    "plt.grid()\n",
    "\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "\n",
    "text = ax.yaxis.get_offset_text()\n",
    "text.set_size(fontsize)\n",
    "plt.legend(loc = 'upper right', fontsize =fontsize)\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "plt.xlim([10, 2000])\n",
    "#plt.ylim([-1.2e-21, 1.2e-21])\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.fft.irfft(np.fft.rfft(np.array(ts))[960:65535]*delta_t/np.sqrt(np.array(psd)[960:65535])) /delta_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65535])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(psd>0)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
