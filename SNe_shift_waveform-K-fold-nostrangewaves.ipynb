{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1337)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2674)\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib\n",
    "matplotlib.use('Qt4Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate, interpolate, signal, optimize, stats\n",
    "import cPickle as pickle\n",
    "import lal\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Dropout, BatchNormalization, Flatten\n",
    "from keras.optimizers import Nadam, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pyfftw\n",
    "import progressbar\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "np.set_printoptions(edgeitems=30, linewidth=160)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for reading simulated SNe waveforms\n",
    "# This code will apply shift to the waveform \n",
    "# samples so that the waveform will always be in the certre +- user customized percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the file that contains the simulated CCSN waveforms\n",
    "filename = './Data/GWdatabase.h5'\n",
    "\n",
    "# Read the simulated CCSN waveforms\n",
    "waveformfile = h5py.File(filename, 'r')\n",
    "\n",
    "\n",
    "# The first level keys of the h5 file\n",
    "reduced_data = waveformfile.keys()[0]\n",
    "waveformfilekey = waveformfile.keys()[1]\n",
    "yeofrho = waveformfile.keys()[2]\n",
    "\n",
    "waveformfamily = []\n",
    "waveformfamily_keys = []\n",
    "\n",
    "# Since there are 1824 different simulated CCSN waveform. \n",
    "# Each of which is saved in a different waveformfile key \n",
    "# So the loop below is to retreive all the keys with which the waveform strain data is accessed,\n",
    "# and save it to waveformfamily.\n",
    "# Each waveform family has 5 different keys, so the second part is to retrieve these 5 keys, and save them\n",
    "# to waveformfamily_keys.\n",
    "\n",
    "for i, key in enumerate(waveformfile[waveformfilekey].keys()):\n",
    "    waveformfamily.append(key)\n",
    "    if i == 0:\n",
    "        for j, _ in enumerate(waveformfile[waveformfilekey][waveformfamily[i]].keys()):\n",
    "            waveformfamily_keys.append(waveformfile[waveformfilekey][waveformfamily[i]].keys()[j])\n",
    "originalSNR = np.array(waveformfile[reduced_data][u'SNR(aLIGOfrom10kpc)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56839, 670, 13156, 1416)\n"
     ]
    }
   ],
   "source": [
    "# This is to set some parameters for the training.\n",
    "# Since the waveforms are stored in the unit of strain * distan\n",
    "# the waveform amplitudes need to be divided by a distance.\n",
    "\n",
    "# Convection factor between par sec and meters\n",
    "PctMe = lal.PC_SI\n",
    "\n",
    "# The distance the waveform will be divided by, in centimeters\n",
    "Dist = 10.0 * 1e3 * PctMe * 1e2\n",
    "\n",
    "# Since the waveform samples come in different lengths, \n",
    "# so every waveform sample will be set to the longest length.\n",
    "# findmax/findmin is a variable that saves the longest/shortest length of the waveform samples.\n",
    "# k/kmin is the index referring to the longest/shortest waveform sample.\n",
    "findmax = 0\n",
    "k = 0 \n",
    "findmin = 1e10\n",
    "kmin = 0\n",
    "#length = np.zeros(len(waveformfamily))\n",
    "#waveformfamily = [waveformfamily[0]]\n",
    "with open(\"abnormalwaves.pkl\") as f:\n",
    "    mark_nonzeros, mark_suddenjump = pickle.load(f)\n",
    "abn_index = np.concatenate([mark_nonzeros, mark_suddenjump])    \n",
    "\n",
    "# Since the waveform contains 1824 waveforms, which are different both in the morophology and the duration,\n",
    "# training a network with all these waveforms may make it hard to debug. So one may want to limit the variation\n",
    "# in the waveform samples by limiting the number of waveform samples put in the training. \n",
    "no_waves_considered = 1824\n",
    "#allwave = []\n",
    "#mark_nonzero=[]\n",
    "for i in range(len(waveformfamily[0:no_waves_considered])):\n",
    "    if i in abn_index:\n",
    "        continue\n",
    "    waveformnumber = i\n",
    "\n",
    "    ts = np.array(waveformfile[waveformfilekey][waveformfamily[waveformnumber]][u't-tb(s)']) \n",
    "    #waves = np.array(waveformfile[waveformfilekey][waveformfamily[waveformnumber]][u'strain*dist(cm)']) / Dist \n",
    "    #allwave.append(waves)\n",
    "    if findmax < len(ts):\n",
    "        findmax = len(ts)\n",
    "        k = i\n",
    "    if findmin > len(ts):\n",
    "        findmin = len(ts)\n",
    "        kmin = i\n",
    "\n",
    "print(findmax, k, findmin, kmin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_to_set_SNR_single_waveform(preset_SNR, original_waveform, dt, Det):\n",
    "    \n",
    "    ds = len(original_waveform) * dt\n",
    "    df = 1.0 / ds\n",
    "    \n",
    "    Nf = int((len(original_waveform) // 2 + 1))\n",
    "    fs = np.arange(Nf) * df   \n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "    \n",
    "    fftinput_for_snr = pyfftw.empty_aligned(len(original_waveform), dtype='complex128')     \n",
    "    fft_object_for_snr = pyfftw.builders.rfft(fftinput_for_snr)      \n",
    "    \n",
    "    wave_f = fft_object_for_snr(original_waveform) * dt\n",
    "    \n",
    "    temporary_snr = np.sqrt( 4.0 * sum( abs(wave_f) ** 2 / ASD ** 2 ) * df )\n",
    "    SNR_factor = preset_SNR / temporary_snr\n",
    "\n",
    "    wave = SNR_factor * original_waveform\n",
    "    return wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This part is for finding \"unnatural/abnomral waves\"\n",
    "mark_nonzeros = []\n",
    "mark_suddenjump = []\n",
    "\n",
    "wave = []\n",
    "for count, i in enumerate(allwave):\n",
    "    wave.append(rescale_to_set_SNR_single_waveform(10, i, 1.0/65535, 'H1'))\n",
    "    if abs(wave[count][0]) > 0.2e-21:\n",
    "        mark_nonzeros.append(count)\n",
    "    if np.amax(abs(np.diff(wave[count]))) > 1.0e-21:\n",
    "        mark_suddenjump.append(count)\n",
    "#pathandname='abnormalwaves.pkl'\n",
    "#fp = open(pathandname,\"w\")\n",
    "#pickle.dump([mark_nonzeros, mark_suddenjump], fp)\n",
    "fp.close()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simulated waveforms are sampled with a sampling rate equal to 65535 Hz, \n",
    "# coupled with the longest waveform is ~1.66s, this makes the longest waveform contains 1e5 elements. \n",
    "# Since this code will make other waveforms the same length as the longest length, this requires huge amount of memory,\n",
    "# and makes training very slow and difficult. \n",
    "# Therefore, this codes uses scipy.signal.decimate to down sample the waveforms\n",
    "\n",
    "\n",
    "def padandextractwave(waveformfile, waveformfilekey, waveformfamily, strainkey, wavemaxlength, Dist, no_waves_considered, R, abn_index, alpha):\n",
    "    # Number of simulated waveforms considered\n",
    "    actualwavenumber = no_waves_considered - len(abn_index)\n",
    "    noofwaves = len(waveformfamily[0:actualwavenumber])\n",
    "    \n",
    "    msg = 'Reading waveforms from file and downsampling them by a factor of %s............' %(R)\n",
    "    print(msg)\n",
    "    bar = progressbar.ProgressBar(max_value = actualwavenumber)\n",
    "    \n",
    "    # downsample factor, the downsampled waveform will have length = original length / R\n",
    "    \n",
    "    # Vector used to save the downsampled waveform\n",
    "    downsampled_waveforms = np.array([np.zeros(wavemaxlength / R) for i in range(noofwaves)])\n",
    "    wave_take_in = 0\n",
    "    for i, whichsimulation in enumerate(waveformfamily[0:no_waves_considered]):\n",
    "        \n",
    "        if i in abn_index:\n",
    "            continue\n",
    "        \n",
    "        # convert the unit of the waveform from strain*distance to strain\n",
    "        wave = np.array(waveformfile[waveformfilekey][whichsimulation][strainkey]) / Dist\n",
    "        wavelength = len(wave)\n",
    "        windows = signal.tukey(wavelength, alpha = alpha)\n",
    "        wave = windows * wave\n",
    "        # Pad the waveform with zero so that it has the same length as the longest waveform, \n",
    "        # or whatever length is set by wavemaxlength\n",
    "        temporary = np.pad(wave, (0, wavemaxlength - wavelength), 'constant', constant_values = 0)\n",
    "        \n",
    "        # down sample\n",
    "        downsampled_waveforms[wave_take_in] = signal.decimate(temporary, R, ftype='iir')\n",
    "        bar.update(wave_take_in + 1)\n",
    "        wave_take_in += 1\n",
    "    return downsampled_waveforms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (7 of 1758) |                       | Elapsed Time: 0:00:00 ETA:   0:00:28"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading waveforms from file and downsampling them by a factor of 16............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98% (1740 of 1758) |################### | Elapsed Time: 0:00:09 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "## # Since the original longest waveform length may not be dividable by the down sample vector, \n",
    "# this is to ensure that the length will be dividable. \n",
    "R = 16\n",
    "findmax = 56839#108512\n",
    "\n",
    "findmax = np.ceil(findmax/float(R)) * R\n",
    "\n",
    "# the assumed observation/simulation duration for every waveform \n",
    "Tobs = findmax / 65535.0\n",
    "#start = time.time()\n",
    "SNewaves = padandextractwave(waveformfile, waveformfilekey, waveformfamily, u'strain*dist(cm)', int(findmax), Dist, no_waves_considered, R, abn_index, 0.5)\n",
    "#elapsed = time.time() - start\n",
    "#print(elapsed)\n",
    "# Using the downsampled waveform to compute the new sampling rate\n",
    "New_sr = (len(SNewaves[0])) / Tobs\n",
    "# the new spacing in time\n",
    "New_dt = 1.0 / New_sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASDtxt(x):\n",
    "    \"\"\"This function reads the following noise curves given a detector name.\"\"\"\n",
    "    return {\n",
    "        'LET':'./ASD/ET_D.txt',\n",
    "        'LCE':'./ASD/CE.txt',\n",
    "        'H1': './ASD/ligoII_NS.txt',\n",
    "        'L1': './ASD/ligoII_NS.txt',\n",
    "        'V1': './ASD/virgoII.txt',\n",
    "        'I2': './ASD/ligoII_NS.txt',\n",
    "        'KAGRA': './ASD/ligoII_NS.txt',\n",
    "        'ET_1': './ASD/ET_D.txt',\n",
    "        'ET_2': './ASD/ET_D.txt',\n",
    "        'ET_3': './ASD/ET_D.txt',\n",
    "        'A2': './ASD/ligoII_NS.txt',\n",
    "        'A2.5': './ASD/ligoII_NS.txt',\n",
    "    }[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readnos(detector, f_points):\n",
    "    \"\"\"This function interpolates the noise given the frequency samples.\"\"\"\n",
    "    nos_file = ASDtxt(detector)\n",
    "    f_str = []\n",
    "    ASD_str = []\n",
    "    file = open(nos_file, 'r')\n",
    "    readFile = file.readlines()\n",
    "    file.close()\n",
    "    f = []\n",
    "    ASD = []\n",
    "    \n",
    "    for line in readFile:\n",
    "        p = line.split()\n",
    "        f_str.append(float(p[0]))\n",
    "        ASD_str.append(float(p[1]))\n",
    "    f = np.log10(np.array(f_str))\n",
    "    ASD = np.log10(np.array(ASD_str))\n",
    "    nosinterpolate = interpolate.splrep(f, ASD, w=1.0*np.ones(len(ASD)), s=0)\n",
    "    \n",
    "    nos = interpolate.splev(np.log10(f_points), nosinterpolate, der = 0, ext = 3)\n",
    "    nos = 10**nos\n",
    "    \n",
    "    return nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisegenerator(Tobs, det, SR, df, dt):\n",
    "    \"\"\"This function generates noise based on amplitude spectral density\"\"\"\n",
    "    \n",
    "    # The number of time stamps\n",
    "    Ns = Tobs * SR \n",
    "    \n",
    "    # The number of the frequency samples\n",
    "    Nf = int(Ns // 2 + 1)\n",
    "    \n",
    "    # The frequency sample\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # read ASD\n",
    "    ASD = readnos(det, fs)\n",
    "    #plt.loglog(fs, ASD)\n",
    "    #plt.show()\n",
    "    #dd\n",
    "    \n",
    "    PSD = ASD ** 2\n",
    "    # scale the ASD by the observation time, and this will be the highest amplitude of the generated noise\n",
    "    Amp = np.sqrt(0.25 * Tobs * PSD)\n",
    "    \n",
    "    \n",
    "    idx = np.argwhere(PSD==0.0)\n",
    "    Amp[idx] = 0.0\n",
    "    \n",
    "    real_nos = Amp * np.random.normal(0.0, 1.0, Nf)\n",
    "    img_nos = Amp * np.random.normal(0.0, 1.0, Nf)\n",
    "    \n",
    "    # This is to ensure there is no strange behaviour from noise at low frequency.\n",
    "    # This is because the interpolation function will interpolate strange values at frequencies betweem 1 - 10Hz.\n",
    "    #low_cutoff = 20\n",
    "    #high_cutoff = 2048\n",
    "    \n",
    "    #idx_1 =  int(low_cutoff/df)\n",
    "    #real_nos[0:idx_1] = 0\n",
    "    #img_nos[0:idx_1] = 0\n",
    "    #idx_2 = int(high_cutoff/df)\n",
    "    #real_nos[idx_2:] = 0\n",
    "    #img_nos[idx_2:] = 0\n",
    "    \n",
    "    nos = real_nos + 1j * img_nos\n",
    "\n",
    "    \n",
    "    # Fourier transiform converts the generated noise to the tme domain\n",
    "    fftinput = pyfftw.empty_aligned(len(nos), dtype='complex128')\n",
    "    \n",
    "    fft_object = pyfftw.builders.irfft(fftinput)\n",
    "\n",
    "    nos_realization = Ns* fft_object(nos) * df\n",
    "\n",
    "    return ASD, nos_realization, fs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNR_calculator(waves_in_time_domain, dt, Det):\n",
    "    \n",
    "    length = len(waves_in_time_domain)\n",
    "    \n",
    "    df = 1.0 / (length * dt)\n",
    "    \n",
    "    Nf = int((length // 2 + 1))\n",
    "    \n",
    "    fftinput_for_snr = pyfftw.empty_aligned(length, dtype='complex128')     \n",
    "    fft_object_for_snr = pyfftw.builders.rfft(fftinput_for_snr)      \n",
    "     \n",
    "    # frequency samples\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "        \n",
    "    temporary_wave_in_f = fft_object_for_snr(waves_in_time_domain) * dt\n",
    "    \n",
    "    snr = np.sqrt( 4.0 * sum( abs(temporary_wave_in_f) ** 2 / ASD ** 2 ) * df )\n",
    "    \n",
    "    return snr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_to_set_SNR(preset_SNR, SNewaves, dt, Det):\n",
    "    \n",
    "    df = 1.0 / (len(SNewaves[0]) * dt)\n",
    "    fftinput_for_snr = pyfftw.empty_aligned(len(SNewaves[0]), dtype='complex128')     \n",
    "    fft_object_for_snr = pyfftw.builders.rfft(fftinput_for_snr)      \n",
    "    \n",
    "    Nf = int((len(SNewaves[0]) // 2 + 1))\n",
    "    \n",
    "    # frequency samples\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "    msg = 'Rescaling the amplitude of the waveforms so that their optimal SNR is %s.........' %(preset_SNR)\n",
    "    print(msg)\n",
    "    print(\" \")\n",
    "    bar = progressbar.ProgressBar(max_value = len(SNewaves))\n",
    "    \n",
    "    \n",
    "    scaled_snewaves = np.array([np.zeros(len(SNewaves[0])) for i in range(len(SNewaves))])\n",
    "    for i, wave in enumerate(SNewaves):\n",
    "        temporary_wave_in_f = fft_object_for_snr(wave) * dt\n",
    "        temporary_snr = np.sqrt( 4.0 * sum( abs(temporary_wave_in_f) ** 2 / ASD ** 2 ) * df )\n",
    "        SNR_factor = preset_SNR / temporary_snr\n",
    "        \n",
    "        scaled_snewaves[i] = SNR_factor * wave\n",
    "        #print(temporary_snr)\n",
    "        #print(  np.sqrt(4.0 * sum(abs(fft_object_for_snr(SNewaves[i]) * dt) **2 / ASD ** 2) * df))\n",
    "        bar.update(i)\n",
    "    \n",
    "    return scaled_snewaves\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(seed, ts, dt, Sr, percentage, Det, SNewaves, N_rz, multiplication):\n",
    "    \"\"\"This function generates the data for training/validation/testing.\"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # The number of sample will be equal to the number of N_rz(noise realizations)\n",
    "    data = np.array([np.zeros_like(ts) for i in range(N_rz)])\n",
    "    \n",
    "    # Signal to noise ratio\n",
    "    #SNR = np.zeros(N_rz)\n",
    "    \n",
    "    # Number of time stamps\n",
    "    Ns = len(ts)\n",
    "    \n",
    "    # Number of frequency samples\n",
    "    Nf = int(Ns //2 + 1)\n",
    "    \n",
    "    # Observation time\n",
    "    Tobs = ts[-1] + dt\n",
    "    \n",
    "    # spacing in the frequency domain\n",
    "    df = 1.0/Tobs\n",
    "    # frequency samples\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "    \n",
    "    \n",
    "    toolbar_width = N_rz\n",
    "\n",
    "    \n",
    "    \n",
    "    msg = 'Generating noise realizations.......'\n",
    "    print(msg)\n",
    "    print(\" \")\n",
    "    # setup toolbar\n",
    "    bar = progressbar.ProgressBar(max_value=toolbar_width)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Generate noise\n",
    "    for i in range(N_rz):\n",
    "        #if (i+1) % 1000 == 0 & i != N_rz - 1:\n",
    "        #   msg = 'The %s th to %s th noise realizations are now being generated.' %(i+1, i+1000)\n",
    "        #    print(msg)\n",
    "        _, data[i], _ = noisegenerator(Tobs, Det, Sr, df, dt)\n",
    "        bar.update(i+1)\n",
    "\n",
    "\n",
    "\n",
    "    msg = 'Adding noise to signals and converting them back to the time domain after whitening them in the frequency domain.....'\n",
    "    print(msg)\n",
    "    print(\" \")\n",
    "    bar_2 = progressbar.ProgressBar(max_value=toolbar_width)\n",
    "    \n",
    "    \n",
    "    if ts[-1] == signal_duration:   \n",
    "\n",
    "        for i in range(multiplication):\n",
    "            for j in range(len(SNewaves)):\n",
    "\n",
    "                count = i * len(SNewaves) + j\n",
    "                #if (count + 1) % 1000 == 0 and count < 4999:\n",
    "                #    msg = 'The %s th to %s th samples of the data set are now being generated.' %(count + 1,count + 1000)\n",
    "                #    print(msg)\n",
    "                data[count] += SNewaves[j]\n",
    "\n",
    "\n",
    "                fftinput_1 = pyfftw.empty_aligned(len(data[count]), dtype='complex128')\n",
    "                fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "                temporary = fft_object_1(data[count]) * 1.0/Sr\n",
    "                temporary = temporary / ASD \n",
    "\n",
    "\n",
    "                #SNR[count] = np.sqrt(4.0 * sum(abs(temporary[int(100/df): int(500/df)]) ** 2 * df))\n",
    "                #SNR_factor = SNR_set / SNR[count]\n",
    "                #temporary = temporary * SNR_factor\n",
    "                #if SNR_factor > 1:\n",
    "                #    print(SNR_factor,count) \n",
    "                #print(SNR_factor, np.sqrt(4.0 * sum(abs(temporary) ** 2 * df)))\n",
    "                fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "                fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "                data[count] = Ns * fft_object_2(temporary) * df * np.sqrt(2.0/ Sr)\n",
    "                bar_2.update( count + 1)\n",
    "    elif ts[-1] > signal_duration:\n",
    "        for i in range(multiplication):\n",
    "            for j in range(len(SNewaves)):\n",
    "\n",
    "                count = i * len(SNewaves) + j\n",
    "                #if (count + 1) % 1000 == 0 and count < 4999:\n",
    "                #    msg = 'The %s th to %s th samples of the data set are now being generated.' %(count + 1,count + 1000)\n",
    "                #    print(msg)\n",
    "                # This is to draw a random and determine     \n",
    "                random_shift_percentage = np.random.uniform(-percentage, percentage)\n",
    "                original_starting_point = sample_length / 2 - signal_length / 2\n",
    "                shifted_starting_point = int(original_starting_point + ts[-1] * Sr * random_shift_percentage)\n",
    "                \n",
    "                data[count][shifted_starting_point: shifted_starting_point + signal_length] = data[count][shifted_starting_point: shifted_starting_point + signal_length] + SNewaves[j]\n",
    "        \n",
    "                fftinput_1 = pyfftw.empty_aligned(len(data[count]), dtype='complex128')\n",
    "                fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "                temporary = fft_object_1(data[count]) * 1.0 / Sr\n",
    "                temporary = temporary / ASD \n",
    "\n",
    "\n",
    "                #SNR[count] = np.sqrt(4.0 * sum(abs(temporary[int(100/df): int(500/df)]) ** 2 * df))\n",
    "                #SNR_factor = SNR_set / SNR[count]\n",
    "                #temporary[int(100/df): int(500/df)] = temporary[int(100/df): int(500/df)] * SNR_factor\n",
    "                \n",
    "                #if SNR_factor > 1:\n",
    "                #    print(SNR_factor,count) \n",
    "                #print(SNR_factor, np.sqrt(4.0 * sum(abs(temporary) ** 2 * df)))\n",
    "                \n",
    "                fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "                fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "                data[count] = Ns * fft_object_2(temporary) * df * np.sqrt(2.0/ Sr)\n",
    "                bar_2.update( count + 1 )\n",
    "    else:\n",
    "        raise Exception('The sample length should be longer than or equal to the signal length') \n",
    "\n",
    "            \n",
    "    for i in range(multiplication * len(SNewaves), N_rz):\n",
    "        fftinput_1 = pyfftw.empty_aligned(len(data[i]), dtype='complex128')\n",
    "        fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "        temporary = fft_object_1(data[i]) *  1.0 / Sr \n",
    "        temporary = temporary / ASD \n",
    "        \n",
    "        fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "        fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "        data[i] = Ns * fft_object_2(temporary) * df * np.sqrt(2.0/ Sr)\n",
    "        bar_2.update(i + 1)\n",
    "            \n",
    "            \n",
    "    return data #SNR\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten_data(not_whitened_data_in_time_domain, ASD, dt, SR):\n",
    "    \n",
    "    num = len(not_whitened_data_in_time_domain)\n",
    "    signal_len = len(not_whitened_data_in_time_domain[0])\n",
    "    \n",
    "    whitened_data = np.array([np.zeros(signal_len ) for i in range(num)])\n",
    "    fftinput_in_td = pyfftw.empty_aligned(signal_len, dtype='complex128')\n",
    "    fft_object_to_f = pyfftw.builders.rfft(fftinput_in_td)\n",
    "    \n",
    "    \n",
    "    fftinput_in_fd = pyfftw.empty_aligned(signal_len//2 + 1, dtype='complex128')\n",
    "    fft_object_to_t = pyfftw.builders.irfft(fftinput_in_fd)\n",
    "    \n",
    "    for i, nwd in enumerate(not_whitened_data_in_time_domain):\n",
    "        temp = fft_object_to_f(nwd) * dt / ASD\n",
    "        whitened_data[i] = fft_object_to_t(temp) * np.sqrt(2.0/ SR) / dt\n",
    "        \n",
    "    \n",
    "    return whitened_data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd, _, _ = noisegenerator(ts[-1] + New_dt , 'H1', New_sr, 1.0/signal_duration, New_dt)\n",
    "whitened_data = whiten_data(SNewaves, asd, New_dt, New_sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time stamps \n",
    "signal_length = len(SNewaves[0])\n",
    "signal_duration = (signal_length - 1) * New_dt\n",
    "\n",
    "# applying pad to make the sample longer. This is for the purpose of shifting the signal, so that the signal will appear to be in the centre +- user customised percentage\n",
    "# If no padding is to be applied\n",
    "sample_length = signal_length * 2.0\n",
    "\n",
    "# time stamps after pad\n",
    "ts = np.arange(sample_length) * New_dt\n",
    "sample_duration = ts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(sample, label,  shuffle_times, seed):\n",
    "    np.random.seed(seed)\n",
    "    for i in range(shuffle_times):\n",
    "        state = np.random.randint(0,100)\n",
    "        sample, label = shuffle(sample, label, random_state=state)\n",
    "        \n",
    "    return sample, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    model = Sequential()    # define the type of keras model\n",
    "\n",
    "    # add the layers\n",
    "    # conv1\n",
    "    model.add(Conv2D(6, (1,64), activation='elu', input_shape=input_shape))\n",
    "    # maxpool2\n",
    "    model.add(MaxPool2D((1,32)))\n",
    "    \n",
    "    model.add(Conv2D(4, (1,8), activation='elu'))\n",
    "\n",
    "    model.add(MaxPool2D((1,8)))\n",
    "    # conv2\n",
    "    model.add(Conv2D(4, (1,4), activation='elu'))\n",
    "    # maxpool2\n",
    "    model.add(MaxPool2D((1,4)))\n",
    "    # the input the fully connected layer must be 1-D vector\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='elu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    dol = keras.layers.Dropout(0.6, noise_shape=None, seed=10)\n",
    "\n",
    "    model.add(dol)\n",
    "    # add the output layer with softmax actiavtion for classication\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_for_diff_SNRs(degree_of_repeat_for_signal, original_waveforms, shift_percentage, data_generator_seed, Det, N_rz, ts, dt, sr,\n",
    "                       batch_size, epochs, kfold_splits, weight_file_name, validation, SNRs):\n",
    "    \n",
    "    if 'data' in globals():\n",
    "        del data, label\n",
    "    \n",
    "    \n",
    "    \n",
    "    presence = len(original_waveforms) * degree_of_repeat_for_signal  #number of samples that contain noise + signal\n",
    "    data_shuffle_seed = data_generator_seed + 10    \n",
    "    kfold_seed = data_generator_seed + 20  # seed for kfold\n",
    "    tscores = [[] for i in range(len(SNRs))]\n",
    "    history_saver = [[] for i in range(len(SNRs))]\n",
    "    num_classes = 2\n",
    "    counter = 0\n",
    "    test_label_saver_for_ROC = [[] for i in range(len(SNRs))]\n",
    "    signal_preds = [[] for i in range(len(SNRs))]\n",
    "    for SNR_set in SNRs:\n",
    "        \n",
    "        weight_file_name_2 = \"SNR_%s.hdf5\" %(SNR_set)\n",
    "        weight_file_name_3 = ''.join([weight_file_name, weight_file_name_2])\n",
    "        \n",
    "        scaled_waveforms = rescale_to_set_SNR(SNR_set, original_waveforms, dt, Det) \n",
    "        # Number of noise realization. This will be the final number of data samples for training + validation + testing\n",
    "        # waveform No. 193 is problematic\n",
    "        data = data_generator(data_generator_seed, ts, dt, sr, shift_percentage, Det, scaled_waveforms, N_rz, degree_of_repeat_for_signal)\n",
    "        label = np.concatenate((np.ones(presence), np.zeros(N_rz - presence)))\n",
    "    \n",
    "        data, label = shuffle_data(data, label,  1, data_shuffle_seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"This part is dedicated to testing the performance of a network by carrying out a k-fold cross validation\"\"\"\n",
    "\n",
    "        # number of time series per batch\n",
    "        # number of full passes of the dataset\n",
    "        # directory to store results in\n",
    "\n",
    "\n",
    "        kfold = StratifiedKFold(kfold_splits, shuffle = True, random_state = kfold_seed)\n",
    "        keras.backend.set_image_data_format('channels_first')\n",
    "        \n",
    "        modelCheck = ModelCheckpoint(weight_file_name_3, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=0)\n",
    "\n",
    "\n",
    "        index_for_signal = np.array([i for i in range(presence - validation, presence)])\n",
    "        rest_for_signal = np.array([i for i in range(presence - validation)])\n",
    "        index_for_noise = np.array([i for i in range(N_rz - validation, N_rz)])\n",
    "        rest_for_noise = np.array([i for i in range(presence, N_rz - validation)])\n",
    "\n",
    "        save_for_val = np.concatenate([index_for_signal, index_for_noise])\n",
    "        rest = np.concatenate([rest_for_signal, rest_for_noise])\n",
    "        data_for_val = data[save_for_val]\n",
    "        label_for_val = label[save_for_val]\n",
    "\n",
    "        sample_length = len(data[0])\n",
    "        data_for_val = data_for_val.reshape(-1, 1, 1, sample_length)\n",
    "        label_for_val = keras.utils.to_categorical(label_for_val, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        msg = \"Training the network on signals with SNR = %s\" %(SNR_set)\n",
    "        print(msg)        \n",
    "        counter_train = 0\n",
    "        for train, test in kfold.split(data[rest],label[rest]):\n",
    "            msg = ''.join([\"Training for the %s\" %(counter_train + 1), \" th times.\"])\n",
    "            print(msg)\n",
    "\n",
    "            data_for_train = data[rest][train]\n",
    "            label_for_train = label[rest][train]\n",
    "\n",
    "            data_for_test = data[rest][test]\n",
    "            label_for_test = label[rest][test]\n",
    "\n",
    "            data_for_train = data_for_train.reshape(-1, 1, 1, sample_length)\n",
    "            data_for_test = data_for_test.reshape(-1, 1, 1, sample_length)\n",
    "\n",
    "            input_shape = data_for_train.shape[1:]\n",
    "            print(input_shape)\n",
    "            label_for_train = keras.utils.to_categorical(label_for_train , num_classes)\n",
    "            label_for_test = keras.utils.to_categorical(label_for_test, num_classes)\n",
    "\n",
    "            if \"model\" in locals() or \"model\" in globals():\n",
    "                del model\n",
    "            model = make_model(input_shape, num_classes)\n",
    "\n",
    "            # compile the model #adam = keras.optimizers.Adam(lr=0.01)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer= Nadam(), metrics=['accuracy'])\n",
    "\n",
    "            history = model.fit(data_for_train, label_for_train, batch_size=batch_size, epochs=epochs, \n",
    "                                verbose=1, validation_data=(data_for_val, label_for_val), callbacks = [modelCheck], shuffle = False)\n",
    "\n",
    "\n",
    "            model.load_weights(weight_file_name_3)\n",
    "            # evaluate\n",
    "            eval_results = model.evaluate(data_for_test, label_for_test, verbose=1)\n",
    "            print('The result of testing the model against test data is:')\n",
    "            print('Test loss: %s'%(eval_results[0]))\n",
    "            print('Test accuracy %s:' %(eval_results[1]))\n",
    "            print(' ')\n",
    "            tscores[counter].append(eval_results)\n",
    "            history_saver[counter].append(history)\n",
    "            signal_preds[counter].append(model.predict(data_for_test))\n",
    "            test_label_saver_for_ROC[counter].append(label_for_test) \n",
    "            \n",
    "            \n",
    "            counter_train += 1\n",
    "        counter += 1\n",
    "    return tscores, history_saver, signal_preds, test_label_saver_for_ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8% (154 of 1758) |#                    | Elapsed Time: 0:00:00 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaling the amplitude of the waveforms so that their optimal SNR is 6.0.........\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (14 of 30000) |                     | Elapsed Time: 0:00:00 ETA:   0:03:36"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating noise realizations.......\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (112 of 30000) |                    | Elapsed Time: 0:00:00 ETA:   0:00:26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to signals and converting them back to the time domain after whitening them in the frequency domain.....\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (29982 of 30000) |################# | Elapsed Time: 0:00:27 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network on signals with SNR = 6.0\n",
      "Training for the 1 th times.\n",
      "(1, 1, 7106)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 6, 1, 7043)        390       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 6, 1, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 4, 1, 213)         196       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 4, 1, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 4, 1, 23)          68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 4, 1, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,392\n",
      "Trainable params: 1,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25199 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "25199/25199 [==============================] - 9s 364us/step - loss: 0.6856 - acc: 0.5740 - val_loss: 0.6838 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "25199/25199 [==============================] - 7s 277us/step - loss: 0.6789 - acc: 0.5858 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 3/30\n",
      "25199/25199 [==============================] - 7s 283us/step - loss: 0.6525 - acc: 0.6048 - val_loss: 0.5710 - val_acc: 0.6760\n",
      "Epoch 4/30\n",
      "25199/25199 [==============================] - 7s 282us/step - loss: 0.4999 - acc: 0.7496 - val_loss: 0.4661 - val_acc: 0.7620\n",
      "Epoch 5/30\n",
      "25199/25199 [==============================] - 7s 281us/step - loss: 0.4533 - acc: 0.7803 - val_loss: 0.4466 - val_acc: 0.7780\n",
      "Epoch 6/30\n",
      "25199/25199 [==============================] - 7s 279us/step - loss: 0.4299 - acc: 0.7958 - val_loss: 0.4301 - val_acc: 0.7910\n",
      "Epoch 7/30\n",
      "25199/25199 [==============================] - 7s 281us/step - loss: 0.4163 - acc: 0.8051 - val_loss: 0.4232 - val_acc: 0.7975\n",
      "Epoch 8/30\n",
      "25199/25199 [==============================] - 7s 281us/step - loss: 0.4085 - acc: 0.8090 - val_loss: 0.4185 - val_acc: 0.7965\n",
      "Epoch 9/30\n",
      "25199/25199 [==============================] - 7s 279us/step - loss: 0.4026 - acc: 0.8144 - val_loss: 0.4136 - val_acc: 0.7990\n",
      "Epoch 10/30\n",
      "25199/25199 [==============================] - 7s 280us/step - loss: 0.3967 - acc: 0.8153 - val_loss: 0.4175 - val_acc: 0.7980\n",
      "Epoch 11/30\n",
      "25199/25199 [==============================] - 7s 283us/step - loss: 0.3935 - acc: 0.8191 - val_loss: 0.4133 - val_acc: 0.8015\n",
      "Epoch 12/30\n",
      "25199/25199 [==============================] - 7s 278us/step - loss: 0.3914 - acc: 0.8213 - val_loss: 0.4097 - val_acc: 0.8060\n",
      "Epoch 13/30\n",
      "25199/25199 [==============================] - 7s 279us/step - loss: 0.3858 - acc: 0.8248 - val_loss: 0.4213 - val_acc: 0.8020\n",
      "Epoch 14/30\n",
      "25199/25199 [==============================] - 7s 282us/step - loss: 0.3842 - acc: 0.8248 - val_loss: 0.4149 - val_acc: 0.8065\n",
      "Epoch 15/30\n",
      "25199/25199 [==============================] - 7s 277us/step - loss: 0.3816 - acc: 0.8273 - val_loss: 0.4126 - val_acc: 0.8110\n",
      "Epoch 16/30\n",
      "25199/25199 [==============================] - 7s 279us/step - loss: 0.3807 - acc: 0.8305 - val_loss: 0.4202 - val_acc: 0.8100\n",
      "Epoch 17/30\n",
      "25199/25199 [==============================] - 7s 278us/step - loss: 0.3796 - acc: 0.8275 - val_loss: 0.4202 - val_acc: 0.8025\n",
      "Epoch 18/30\n",
      "25199/25199 [==============================] - 7s 279us/step - loss: 0.3775 - acc: 0.8289 - val_loss: 0.4088 - val_acc: 0.8065\n",
      "Epoch 19/30\n",
      "25199/25199 [==============================] - 7s 279us/step - loss: 0.3758 - acc: 0.8298 - val_loss: 0.4102 - val_acc: 0.8120\n",
      "Epoch 20/30\n",
      "25199/25199 [==============================] - 7s 278us/step - loss: 0.3734 - acc: 0.8319 - val_loss: 0.4075 - val_acc: 0.8150\n",
      "Epoch 21/30\n",
      "25199/25199 [==============================] - 7s 278us/step - loss: 0.3734 - acc: 0.8325 - val_loss: 0.4050 - val_acc: 0.8185\n",
      "Epoch 22/30\n",
      "25199/25199 [==============================] - 7s 278us/step - loss: 0.3721 - acc: 0.8316 - val_loss: 0.4046 - val_acc: 0.8130\n",
      "Epoch 23/30\n",
      "25199/25199 [==============================] - 7s 277us/step - loss: 0.3707 - acc: 0.8329 - val_loss: 0.4036 - val_acc: 0.8080\n",
      "Epoch 24/30\n",
      "25199/25199 [==============================] - 7s 279us/step - loss: 0.3697 - acc: 0.8330 - val_loss: 0.4060 - val_acc: 0.8095\n",
      "Epoch 25/30\n",
      "25199/25199 [==============================] - 7s 278us/step - loss: 0.3708 - acc: 0.8341 - val_loss: 0.4151 - val_acc: 0.8070\n",
      "Epoch 26/30\n",
      "25199/25199 [==============================] - 7s 278us/step - loss: 0.3705 - acc: 0.8352 - val_loss: 0.4084 - val_acc: 0.8015\n",
      "Epoch 27/30\n",
      "25199/25199 [==============================] - 7s 280us/step - loss: 0.3693 - acc: 0.8358 - val_loss: 0.4044 - val_acc: 0.8065\n",
      "Epoch 28/30\n",
      "25199/25199 [==============================] - 7s 278us/step - loss: 0.3675 - acc: 0.8373 - val_loss: 0.4023 - val_acc: 0.8105\n",
      "Epoch 29/30\n",
      "25199/25199 [==============================] - 7s 279us/step - loss: 0.3669 - acc: 0.8364 - val_loss: 0.4082 - val_acc: 0.8035\n",
      "Epoch 30/30\n",
      "25199/25199 [==============================] - 7s 283us/step - loss: 0.3682 - acc: 0.8354 - val_loss: 0.4054 - val_acc: 0.8025\n",
      "2801/2801 [==============================] - 0s 132us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.36653023013384245\n",
      "Test accuracy 0.8282756159366007:\n",
      " \n",
      "Training for the 2 th times.\n",
      "(1, 1, 7106)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_34 (Conv2D)           (None, 6, 1, 7043)        390       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 6, 1, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 4, 1, 213)         196       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 4, 1, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 4, 1, 23)          68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 4, 1, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,392\n",
      "Trainable params: 1,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 9s 346us/step - loss: 0.6855 - acc: 0.5739 - val_loss: 0.6818 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.6787 - acc: 0.5863 - val_loss: 0.6831 - val_acc: 0.5795\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25200/25200 [==============================] - 7s 294us/step - loss: 0.6773 - acc: 0.5862 - val_loss: 0.6833 - val_acc: 0.5795\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.5338 - acc: 0.7154 - val_loss: 0.4530 - val_acc: 0.7675\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.4229 - acc: 0.8002 - val_loss: 0.4197 - val_acc: 0.7965\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 7s 297us/step - loss: 0.3982 - acc: 0.8133 - val_loss: 0.4342 - val_acc: 0.7910\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3838 - acc: 0.8228 - val_loss: 0.4147 - val_acc: 0.8045\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 7s 286us/step - loss: 0.3759 - acc: 0.8315 - val_loss: 0.4194 - val_acc: 0.8005\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3702 - acc: 0.8319 - val_loss: 0.4067 - val_acc: 0.8020\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3638 - acc: 0.8350 - val_loss: 0.4118 - val_acc: 0.8055\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3617 - acc: 0.8379 - val_loss: 0.3955 - val_acc: 0.8200\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3562 - acc: 0.8422 - val_loss: 0.3826 - val_acc: 0.8265\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3561 - acc: 0.8421 - val_loss: 0.3920 - val_acc: 0.8155\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3534 - acc: 0.8446 - val_loss: 0.3805 - val_acc: 0.8270\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3488 - acc: 0.8472 - val_loss: 0.3755 - val_acc: 0.8280\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3468 - acc: 0.8465 - val_loss: 0.3682 - val_acc: 0.8320\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3432 - acc: 0.8507 - val_loss: 0.3651 - val_acc: 0.8345\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3413 - acc: 0.8526 - val_loss: 0.3584 - val_acc: 0.8395\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3364 - acc: 0.8544 - val_loss: 0.3702 - val_acc: 0.8330\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.3339 - acc: 0.8556 - val_loss: 0.3477 - val_acc: 0.8490\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 7s 286us/step - loss: 0.3324 - acc: 0.8568 - val_loss: 0.3506 - val_acc: 0.8475\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3305 - acc: 0.8587 - val_loss: 0.3504 - val_acc: 0.8475\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3281 - acc: 0.8606 - val_loss: 0.3458 - val_acc: 0.8510\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3269 - acc: 0.8614 - val_loss: 0.3466 - val_acc: 0.8495\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3229 - acc: 0.8621 - val_loss: 0.3422 - val_acc: 0.8505\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3238 - acc: 0.8638 - val_loss: 0.3473 - val_acc: 0.8495\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3231 - acc: 0.8638 - val_loss: 0.3357 - val_acc: 0.8540\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3224 - acc: 0.8633 - val_loss: 0.3695 - val_acc: 0.8395\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3221 - acc: 0.8644 - val_loss: 0.3567 - val_acc: 0.8425\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3199 - acc: 0.8626 - val_loss: 0.3455 - val_acc: 0.8445\n",
      "2800/2800 [==============================] - 0s 128us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.317604342017855\n",
      "Test accuracy 0.8632142857142857:\n",
      " \n",
      "Training for the 3 th times.\n",
      "(1, 1, 7106)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 6, 1, 7043)        390       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 6, 1, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 4, 1, 213)         196       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 4, 1, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 4, 1, 23)          68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 4, 1, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,392\n",
      "Trainable params: 1,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 9s 345us/step - loss: 0.6831 - acc: 0.5773 - val_loss: 0.6809 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.6765 - acc: 0.5862 - val_loss: 0.6523 - val_acc: 0.5900\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.5253 - acc: 0.7238 - val_loss: 0.4624 - val_acc: 0.7675\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.4375 - acc: 0.7923 - val_loss: 0.4219 - val_acc: 0.7965\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.4071 - acc: 0.8094 - val_loss: 0.4100 - val_acc: 0.7990\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3931 - acc: 0.8192 - val_loss: 0.4095 - val_acc: 0.7980\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3830 - acc: 0.8270 - val_loss: 0.4140 - val_acc: 0.8020\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 7s 277us/step - loss: 0.3746 - acc: 0.8334 - val_loss: 0.4056 - val_acc: 0.8010\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 7s 278us/step - loss: 0.3678 - acc: 0.8346 - val_loss: 0.3989 - val_acc: 0.8045\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3622 - acc: 0.8405 - val_loss: 0.3987 - val_acc: 0.8165\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3567 - acc: 0.8431 - val_loss: 0.3841 - val_acc: 0.8175\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3528 - acc: 0.8433 - val_loss: 0.3817 - val_acc: 0.8210\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3475 - acc: 0.8478 - val_loss: 0.3784 - val_acc: 0.8215\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3431 - acc: 0.8491 - val_loss: 0.3677 - val_acc: 0.8185\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 7s 278us/step - loss: 0.3386 - acc: 0.8523 - val_loss: 0.3647 - val_acc: 0.8285\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3362 - acc: 0.8552 - val_loss: 0.3590 - val_acc: 0.8305\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3322 - acc: 0.8569 - val_loss: 0.3553 - val_acc: 0.8330\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25200/25200 [==============================] - 7s 297us/step - loss: 0.3315 - acc: 0.8593 - val_loss: 0.3512 - val_acc: 0.8380\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 8s 298us/step - loss: 0.3277 - acc: 0.8606 - val_loss: 0.3501 - val_acc: 0.8390\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 8s 299us/step - loss: 0.3256 - acc: 0.8615 - val_loss: 0.3463 - val_acc: 0.8430\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3245 - acc: 0.8618 - val_loss: 0.3542 - val_acc: 0.8380\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.3253 - acc: 0.8634 - val_loss: 0.3552 - val_acc: 0.8360\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 7s 292us/step - loss: 0.3219 - acc: 0.8619 - val_loss: 0.3521 - val_acc: 0.8445\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3199 - acc: 0.8639 - val_loss: 0.3530 - val_acc: 0.8405\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3203 - acc: 0.8637 - val_loss: 0.3491 - val_acc: 0.8410\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3207 - acc: 0.8644 - val_loss: 0.3515 - val_acc: 0.8410\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.3183 - acc: 0.8658 - val_loss: 0.3498 - val_acc: 0.8460\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3163 - acc: 0.8648 - val_loss: 0.3484 - val_acc: 0.8465\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3160 - acc: 0.8669 - val_loss: 0.3441 - val_acc: 0.8500\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 7s 286us/step - loss: 0.3148 - acc: 0.8672 - val_loss: 0.3464 - val_acc: 0.8465\n",
      "2800/2800 [==============================] - 0s 136us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.3114428620679038\n",
      "Test accuracy 0.8639285714285714:\n",
      " \n",
      "Training for the 4 th times.\n",
      "(1, 1, 7106)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 6, 1, 7043)        390       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 6, 1, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 4, 1, 213)         196       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 4, 1, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 4, 1, 23)          68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 4, 1, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,392\n",
      "Trainable params: 1,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 9s 352us/step - loss: 0.6854 - acc: 0.5763 - val_loss: 0.6835 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.6792 - acc: 0.5865 - val_loss: 0.6858 - val_acc: 0.5795\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.6788 - acc: 0.5864 - val_loss: 0.6828 - val_acc: 0.5795\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.6785 - acc: 0.5865 - val_loss: 0.6833 - val_acc: 0.5795\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.6784 - acc: 0.5865 - val_loss: 0.6817 - val_acc: 0.5795\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.6782 - acc: 0.5864 - val_loss: 0.6829 - val_acc: 0.5795\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.6783 - acc: 0.5862 - val_loss: 0.6821 - val_acc: 0.5795\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.6784 - acc: 0.5859 - val_loss: 0.6811 - val_acc: 0.5795\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.6525 - acc: 0.6080 - val_loss: 0.5692 - val_acc: 0.6850\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.4778 - acc: 0.7637 - val_loss: 0.4566 - val_acc: 0.7680\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.4237 - acc: 0.8000 - val_loss: 0.4478 - val_acc: 0.7830\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.4037 - acc: 0.8116 - val_loss: 0.4245 - val_acc: 0.8030\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3919 - acc: 0.8193 - val_loss: 0.4194 - val_acc: 0.8035\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3828 - acc: 0.8262 - val_loss: 0.4019 - val_acc: 0.8120\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.3779 - acc: 0.8292 - val_loss: 0.4014 - val_acc: 0.8140\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3722 - acc: 0.8323 - val_loss: 0.4098 - val_acc: 0.8060\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3697 - acc: 0.8327 - val_loss: 0.4007 - val_acc: 0.8195\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3669 - acc: 0.8372 - val_loss: 0.4039 - val_acc: 0.8145\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3655 - acc: 0.8375 - val_loss: 0.4201 - val_acc: 0.8055\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3645 - acc: 0.8383 - val_loss: 0.4050 - val_acc: 0.8160\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3624 - acc: 0.8388 - val_loss: 0.3919 - val_acc: 0.8215\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.3623 - acc: 0.8391 - val_loss: 0.3930 - val_acc: 0.8205\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3612 - acc: 0.8394 - val_loss: 0.3940 - val_acc: 0.8245\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3593 - acc: 0.8396 - val_loss: 0.3865 - val_acc: 0.8255\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3585 - acc: 0.8413 - val_loss: 0.3882 - val_acc: 0.8240\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3589 - acc: 0.8425 - val_loss: 0.3873 - val_acc: 0.8225\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3557 - acc: 0.8424 - val_loss: 0.3870 - val_acc: 0.8225\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3555 - acc: 0.8435 - val_loss: 0.3843 - val_acc: 0.8210\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 7s 280us/step - loss: 0.3532 - acc: 0.8424 - val_loss: 0.3932 - val_acc: 0.8205\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3530 - acc: 0.8436 - val_loss: 0.3927 - val_acc: 0.8215\n",
      "2800/2800 [==============================] - 0s 123us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.2868974155187607\n",
      "Test accuracy 0.8778571428571429:\n",
      " \n",
      "Training for the 5 th times.\n",
      "(1, 1, 7106)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_43 (Conv2D)           (None, 6, 1, 7043)        390       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 6, 1, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 4, 1, 213)         196       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 4, 1, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 4, 1, 23)          68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 4, 1, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,392\n",
      "Trainable params: 1,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 9s 367us/step - loss: 0.6875 - acc: 0.5745 - val_loss: 0.6803 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 8s 298us/step - loss: 0.6527 - acc: 0.6050 - val_loss: 0.5580 - val_acc: 0.6900\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 8s 299us/step - loss: 0.4889 - acc: 0.7571 - val_loss: 0.4478 - val_acc: 0.7820\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.4225 - acc: 0.7999 - val_loss: 0.4101 - val_acc: 0.8050\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 7s 296us/step - loss: 0.3909 - acc: 0.8196 - val_loss: 0.3825 - val_acc: 0.8170\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3739 - acc: 0.8347 - val_loss: 0.3678 - val_acc: 0.8230\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 8s 299us/step - loss: 0.3612 - acc: 0.8408 - val_loss: 0.3675 - val_acc: 0.8355\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 7s 296us/step - loss: 0.3534 - acc: 0.8443 - val_loss: 0.3615 - val_acc: 0.8385\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3478 - acc: 0.8473 - val_loss: 0.3614 - val_acc: 0.8375\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3424 - acc: 0.8485 - val_loss: 0.3567 - val_acc: 0.8385\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 7s 295us/step - loss: 0.3392 - acc: 0.8521 - val_loss: 0.3545 - val_acc: 0.8445\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 7s 297us/step - loss: 0.3380 - acc: 0.8547 - val_loss: 0.3494 - val_acc: 0.8455\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3322 - acc: 0.8549 - val_loss: 0.3615 - val_acc: 0.8405\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3315 - acc: 0.8578 - val_loss: 0.3516 - val_acc: 0.8460\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3297 - acc: 0.8604 - val_loss: 0.3543 - val_acc: 0.8405\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.3305 - acc: 0.8599 - val_loss: 0.3442 - val_acc: 0.8495\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3283 - acc: 0.8608 - val_loss: 0.3448 - val_acc: 0.8420\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3270 - acc: 0.8615 - val_loss: 0.3435 - val_acc: 0.8470\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3260 - acc: 0.8609 - val_loss: 0.3550 - val_acc: 0.8410\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3261 - acc: 0.8627 - val_loss: 0.3444 - val_acc: 0.8460\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3249 - acc: 0.8625 - val_loss: 0.3393 - val_acc: 0.8460\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3239 - acc: 0.8633 - val_loss: 0.3458 - val_acc: 0.8430\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3231 - acc: 0.8639 - val_loss: 0.3590 - val_acc: 0.8380\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3238 - acc: 0.8637 - val_loss: 0.3502 - val_acc: 0.8470\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3200 - acc: 0.8652 - val_loss: 0.3477 - val_acc: 0.8465\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3211 - acc: 0.8644 - val_loss: 0.3588 - val_acc: 0.8355\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.3203 - acc: 0.8657 - val_loss: 0.3500 - val_acc: 0.8475\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 7s 294us/step - loss: 0.3195 - acc: 0.8646 - val_loss: 0.3629 - val_acc: 0.8400\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 7s 293us/step - loss: 0.3208 - acc: 0.8659 - val_loss: 0.3531 - val_acc: 0.8430\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 7s 286us/step - loss: 0.3189 - acc: 0.8671 - val_loss: 0.3532 - val_acc: 0.8425\n",
      "2800/2800 [==============================] - 0s 136us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.29344554007053375\n",
      "Test accuracy 0.8778571428571429:\n",
      " \n",
      "Training for the 6 th times.\n",
      "(1, 1, 7106)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_46 (Conv2D)           (None, 6, 1, 7043)        390       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 6, 1, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 4, 1, 213)         196       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 4, 1, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 4, 1, 23)          68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 4, 1, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,392\n",
      "Trainable params: 1,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 9s 369us/step - loss: 0.6830 - acc: 0.5782 - val_loss: 0.6835 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.6460 - acc: 0.6104 - val_loss: 0.5364 - val_acc: 0.7065\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.4794 - acc: 0.7610 - val_loss: 0.4571 - val_acc: 0.7610\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.4212 - acc: 0.8008 - val_loss: 0.4349 - val_acc: 0.7915\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.4002 - acc: 0.8133 - val_loss: 0.4193 - val_acc: 0.7980\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3911 - acc: 0.8214 - val_loss: 0.4212 - val_acc: 0.8025\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.3844 - acc: 0.8256 - val_loss: 0.4113 - val_acc: 0.8050\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3788 - acc: 0.8280 - val_loss: 0.4022 - val_acc: 0.8000\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.3732 - acc: 0.8293 - val_loss: 0.3985 - val_acc: 0.8100\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.3684 - acc: 0.8341 - val_loss: 0.3938 - val_acc: 0.8125\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.3647 - acc: 0.8353 - val_loss: 0.3823 - val_acc: 0.8225\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.3593 - acc: 0.8390 - val_loss: 0.3880 - val_acc: 0.8260\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3571 - acc: 0.8404 - val_loss: 0.3906 - val_acc: 0.8285\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.3540 - acc: 0.8437 - val_loss: 0.3895 - val_acc: 0.8340\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3531 - acc: 0.8440 - val_loss: 0.3716 - val_acc: 0.8365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 8s 305us/step - loss: 0.3493 - acc: 0.8458 - val_loss: 0.3767 - val_acc: 0.8355\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 8s 303us/step - loss: 0.3504 - acc: 0.8462 - val_loss: 0.3767 - val_acc: 0.8345\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 8s 306us/step - loss: 0.3492 - acc: 0.8481 - val_loss: 0.3714 - val_acc: 0.8380\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 8s 301us/step - loss: 0.3473 - acc: 0.8486 - val_loss: 0.3700 - val_acc: 0.8405\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 8s 305us/step - loss: 0.3457 - acc: 0.8505 - val_loss: 0.3676 - val_acc: 0.8420\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 8s 298us/step - loss: 0.3466 - acc: 0.8489 - val_loss: 0.3746 - val_acc: 0.8345\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 8s 298us/step - loss: 0.3443 - acc: 0.8496 - val_loss: 0.3673 - val_acc: 0.8425\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 8s 299us/step - loss: 0.3432 - acc: 0.8513 - val_loss: 0.3568 - val_acc: 0.8450\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 8s 298us/step - loss: 0.3436 - acc: 0.8512 - val_loss: 0.3563 - val_acc: 0.8435\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 7s 293us/step - loss: 0.3427 - acc: 0.8517 - val_loss: 0.3563 - val_acc: 0.8435\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 8s 300us/step - loss: 0.3427 - acc: 0.8514 - val_loss: 0.3531 - val_acc: 0.8435\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.3413 - acc: 0.8528 - val_loss: 0.3485 - val_acc: 0.8460\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3416 - acc: 0.8537 - val_loss: 0.3520 - val_acc: 0.8460\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 8s 299us/step - loss: 0.3401 - acc: 0.8525 - val_loss: 0.3471 - val_acc: 0.8490\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3383 - acc: 0.8542 - val_loss: 0.3480 - val_acc: 0.8460\n",
      "2800/2800 [==============================] - 0s 142us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.3071486166545323\n",
      "Test accuracy 0.8664285714285714:\n",
      " \n",
      "Training for the 7 th times.\n",
      "(1, 1, 7106)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_49 (Conv2D)           (None, 6, 1, 7043)        390       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 6, 1, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 4, 1, 213)         196       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 4, 1, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 4, 1, 23)          68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 4, 1, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,392\n",
      "Trainable params: 1,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 9s 370us/step - loss: 0.6839 - acc: 0.5797 - val_loss: 0.6840 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.6790 - acc: 0.5863 - val_loss: 0.6821 - val_acc: 0.5795\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.6657 - acc: 0.5935 - val_loss: 0.6661 - val_acc: 0.6215\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.4980 - acc: 0.7454 - val_loss: 0.4541 - val_acc: 0.7795\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.4335 - acc: 0.7942 - val_loss: 0.4282 - val_acc: 0.7900\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.4118 - acc: 0.8092 - val_loss: 0.4132 - val_acc: 0.8040\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3993 - acc: 0.8154 - val_loss: 0.4068 - val_acc: 0.8070\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3886 - acc: 0.8223 - val_loss: 0.4046 - val_acc: 0.8110\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3849 - acc: 0.8251 - val_loss: 0.3980 - val_acc: 0.8120\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3785 - acc: 0.8261 - val_loss: 0.3998 - val_acc: 0.8070\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3747 - acc: 0.8304 - val_loss: 0.3969 - val_acc: 0.8170\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3736 - acc: 0.8305 - val_loss: 0.3923 - val_acc: 0.8145\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.3718 - acc: 0.8306 - val_loss: 0.3927 - val_acc: 0.8135\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3663 - acc: 0.8338 - val_loss: 0.4001 - val_acc: 0.8150\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3662 - acc: 0.8350 - val_loss: 0.3817 - val_acc: 0.8175\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3634 - acc: 0.8348 - val_loss: 0.3862 - val_acc: 0.8165\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3595 - acc: 0.8390 - val_loss: 0.3913 - val_acc: 0.8185\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3605 - acc: 0.8388 - val_loss: 0.3911 - val_acc: 0.8200\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 7s 279us/step - loss: 0.3585 - acc: 0.8403 - val_loss: 0.3971 - val_acc: 0.8130\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.3582 - acc: 0.8396 - val_loss: 0.3846 - val_acc: 0.8230\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3557 - acc: 0.8421 - val_loss: 0.3964 - val_acc: 0.8145\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3539 - acc: 0.8429 - val_loss: 0.3909 - val_acc: 0.8135\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3560 - acc: 0.8433 - val_loss: 0.3849 - val_acc: 0.8205\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.3522 - acc: 0.8446 - val_loss: 0.3897 - val_acc: 0.8220\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 7s 283us/step - loss: 0.3520 - acc: 0.8444 - val_loss: 0.3777 - val_acc: 0.8290\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3522 - acc: 0.8445 - val_loss: 0.3851 - val_acc: 0.8265\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3519 - acc: 0.8456 - val_loss: 0.3767 - val_acc: 0.8350\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3511 - acc: 0.8446 - val_loss: 0.3712 - val_acc: 0.8365\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 7s 282us/step - loss: 0.3515 - acc: 0.8453 - val_loss: 0.3809 - val_acc: 0.8305\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 7s 281us/step - loss: 0.3497 - acc: 0.8450 - val_loss: 0.3774 - val_acc: 0.8295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 0s 129us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.31041903683117456\n",
      "Test accuracy 0.8660714285714286:\n",
      " \n",
      "Training for the 8 th times.\n",
      "(1, 1, 7106)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_52 (Conv2D)           (None, 6, 1, 7043)        390       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 6, 1, 220)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 4, 1, 213)         196       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling (None, 4, 1, 26)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 4, 1, 23)          68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling (None, 4, 1, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 32)                672       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,392\n",
      "Trainable params: 1,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 10s 379us/step - loss: 0.6843 - acc: 0.5764 - val_loss: 0.6823 - val_acc: 0.5795\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.6791 - acc: 0.5863 - val_loss: 0.6802 - val_acc: 0.5795\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.6172 - acc: 0.6373 - val_loss: 0.5168 - val_acc: 0.7305\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 7s 292us/step - loss: 0.4511 - acc: 0.7811 - val_loss: 0.4185 - val_acc: 0.8030\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3986 - acc: 0.8146 - val_loss: 0.3927 - val_acc: 0.8135\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 7s 290us/step - loss: 0.3789 - acc: 0.8288 - val_loss: 0.3803 - val_acc: 0.8285\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 8s 299us/step - loss: 0.3644 - acc: 0.8379 - val_loss: 0.3837 - val_acc: 0.8245\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.3537 - acc: 0.8449 - val_loss: 0.3829 - val_acc: 0.8245\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3486 - acc: 0.8504 - val_loss: 0.3697 - val_acc: 0.8280\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 7s 292us/step - loss: 0.3425 - acc: 0.8510 - val_loss: 0.3683 - val_acc: 0.8290\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.3401 - acc: 0.8530 - val_loss: 0.3540 - val_acc: 0.8385\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 7s 291us/step - loss: 0.3353 - acc: 0.8555 - val_loss: 0.3668 - val_acc: 0.8330\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3345 - acc: 0.8581 - val_loss: 0.3647 - val_acc: 0.8355\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 7s 295us/step - loss: 0.3312 - acc: 0.8585 - val_loss: 0.3609 - val_acc: 0.8350\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3279 - acc: 0.8606 - val_loss: 0.3543 - val_acc: 0.8455\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 7s 292us/step - loss: 0.3275 - acc: 0.8610 - val_loss: 0.3557 - val_acc: 0.8410\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3257 - acc: 0.8613 - val_loss: 0.3631 - val_acc: 0.8310\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3251 - acc: 0.8631 - val_loss: 0.3570 - val_acc: 0.8445\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 7s 288us/step - loss: 0.3269 - acc: 0.8626 - val_loss: 0.3576 - val_acc: 0.8420\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3236 - acc: 0.8636 - val_loss: 0.3649 - val_acc: 0.8380\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 7s 289us/step - loss: 0.3246 - acc: 0.8641 - val_loss: 0.3582 - val_acc: 0.8430\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3240 - acc: 0.8647 - val_loss: 0.3675 - val_acc: 0.8370\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3223 - acc: 0.8650 - val_loss: 0.3587 - val_acc: 0.8425\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 7s 287us/step - loss: 0.3213 - acc: 0.8654 - val_loss: 0.3676 - val_acc: 0.8380\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 8s 324us/step - loss: 0.3192 - acc: 0.8673 - val_loss: 0.3656 - val_acc: 0.8405\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3216 - acc: 0.8660 - val_loss: 0.3717 - val_acc: 0.8370\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 7s 284us/step - loss: 0.3199 - acc: 0.8660 - val_loss: 0.3602 - val_acc: 0.8390\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 7s 286us/step - loss: 0.3197 - acc: 0.8664 - val_loss: 0.3631 - val_acc: 0.8445\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 7s 285us/step - loss: 0.3173 - acc: 0.8684 - val_loss: 0.3634 - val_acc: 0.8420\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 7s 286us/step - loss: 0.3176 - acc: 0.8666 - val_loss: 0.3629 - val_acc: 0.8400\n",
      "2800/2800 [==============================] - 0s 128us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.2920415664570672\n",
      "Test accuracy 0.8753571428571428:\n",
      " \n",
      "Training for the 9 th times.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ff5c5e66de3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mSNRs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m tscores, history, signal_preds, test_label_saver_for_ROC = kfold_for_diff_SNRs(degree_of_repeat_for_signal, SNewaves, shift_percentage, data_generator_seed, Det, N_rz, ts, New_dt, New_sr,\n\u001b[0;32m---> 18\u001b[0;31m                                                                                batch_size, epochs, kfold_splits, weight_file_name, validation, SNRs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-61a5876ccd2e>\u001b[0m in \u001b[0;36mkfold_for_diff_SNRs\u001b[0;34m(degree_of_repeat_for_signal, original_waveforms, shift_percentage, data_generator_seed, Det, N_rz, ts, dt, sr, batch_size, epochs, kfold_splits, weight_file_name, validation, SNRs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mdata_for_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mlabel_for_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degree_of_repeat_for_signal = 10\n",
    "shift_percentage = 0.2\n",
    "data_generator_seed = 10\n",
    "Det = 'H1'\n",
    "N_rz = 30000\n",
    "\n",
    "batch_size = 30\n",
    "epochs =30\n",
    "kfold_splits= 10\n",
    "weight_file_name = 'results/testing_weights_at_'\n",
    "validation = 10000\n",
    "#SNRs = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "#SNRs = [1.0, 2.0, 3.0, 4.0]\n",
    "#SNRs = [5.0, 6.0, 7.0, 8.0]\n",
    "#SNRs = [9.0,10.0]\n",
    "SNRs = [6.0]\n",
    "tscores, history, signal_preds, test_label_saver_for_ROC = kfold_for_diff_SNRs(degree_of_repeat_for_signal, SNewaves, shift_percentage, data_generator_seed, Det, N_rz, ts, New_dt, New_sr,\n",
    "                                                                               batch_size, epochs, kfold_splits, weight_file_name, validation, SNRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of test loss is 0.3143013902638638\n",
      " \n",
      "The standard deviation of test loss is 0.01718440374798914\n",
      " \n",
      "The mean of test accuracy is 0.8637155958972912\n",
      " \n",
      "The standard deviation of test accuracy is 0.010517432350688537\n"
     ]
    }
   ],
   "source": [
    "t_scores = np.array([np.zeros((10, 2)) for i in range(len(tscores))])\n",
    "for i in range(len(tscores)):\n",
    "    for j in range(10):\n",
    "        t_scores[i][j][0] = tscores[i][j][0]\n",
    "        t_scores[i][j][1] = tscores[i][j][1]\n",
    "\n",
    "msg = 'The mean of test loss is %s' %(np.mean(t_scores[0][:,0]))\n",
    "print(msg)\n",
    "print(' ')\n",
    "\n",
    "msg = 'The standard deviation of test loss is %s' %(np.std(t_scores[0][:,0]))\n",
    "print(msg)\n",
    "print(' ')\n",
    "\n",
    "msg = 'The mean of test accuracy is %s' %(np.mean(t_scores[0][:,1]))\n",
    "print(msg)\n",
    "print(' ')\n",
    "\n",
    "msg = 'The standard deviation of test accuracy is %s' %(np.std(t_scores[0][:,1]))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotlossacc(history, fontsize, epochs):\n",
    "    fig , axs = plt.subplots(2,1, sharex = True)\n",
    "    axs = axs.ravel()\n",
    "    # plot history\n",
    "    counter = 0\n",
    "    for history_i in history:\n",
    "        if counter == 0:\n",
    "            axs[0].plot(np.arange(epochs) + 1, history_i.history['loss'], label = 'Loss', linewidth = 1, color = 'b')\n",
    "            axs[0].plot(np.arange(epochs) + 1, history_i.history['val_loss'], label = 'Validation Loss', linewidth = 1, color = 'r')\n",
    "\n",
    "            axs[1].plot(np.arange(epochs) + 1, history_i.history['acc'], label = 'Accuracy', linewidth = 1, color =  'b')\n",
    "            axs[1].plot(np.arange(epochs) + 1, history_i.history['val_acc'], label = 'Validation Accurarcy', linewidth = 1, color =  'r')\n",
    "            # set labels\n",
    "            axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "            axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "            axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # legends\n",
    "            axs[0].legend(fontsize = fontsize)\n",
    "            axs[1].legend(fontsize = fontsize)\n",
    "        else:\n",
    "            axs[0].plot(np.arange(epochs) + 1, history_i.history['loss'], linewidth = 1, color = 'b')\n",
    "            axs[0].plot(np.arange(epochs) + 1, history_i.history['val_loss'], linewidth = 1, color = 'r')\n",
    "\n",
    "            axs[1].plot(np.arange(epochs) + 1, history_i.history['acc'], linewidth = 1, color = 'b')\n",
    "            axs[1].plot(np.arange(epochs) + 1, history_i.history['val_acc'], linewidth = 1, color = 'r')\n",
    "            # set labels\n",
    "            axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "            axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "            axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # legends\n",
    "            axs[0].legend(fontsize = fontsize)\n",
    "            axs[1].legend(fontsize = fontsize)\n",
    "        counter += 1\n",
    "    # grids\n",
    "    axs[0].grid()\n",
    "    axs[1].grid()\n",
    "    axs[0].set_xlim([1, epochs])\n",
    "    axs[0].set_ylim(bottom = 0)\n",
    "\n",
    "    axs[1].set_xlim([1, epochs])\n",
    "    axs[1].set_ylim(top = 1)\n",
    "\n",
    "    plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "    for ax in axs:\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "#history = history_saver[0][0]\n",
    "plotlossacc(history[0], fontsize, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_shelf(SNR):\n",
    "    nf = 255.0\n",
    "    RGB_choice = {\n",
    "        1 : [255.0, 206.0, 0.0],\n",
    "        2 : [49.0, 51.0, 53.0],\n",
    "        3 : [27.0, 161.0, 226.0],\n",
    "        4 : [222.0, 93.0, 90.0],\n",
    "        5 : [128.0, 211.0, 2.0],\n",
    "        6 : [194.0, 45.0, 140.0],\n",
    "        7 : [227.0, 37.0, 107.0],\n",
    "        8 : [255.0, 146.0, 93.0],\n",
    "        9 : [33.0, 206.0, 142.0],\n",
    "        10: [70.0, 33.0, 180.0]\n",
    "    }\n",
    "    #RGB[SNR]\n",
    "    RGB = [x /nf for x in RGB_choice[SNR]]\n",
    "    return RGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotlossacc_together(history, fontsize, SNR, Loss_top, Loss_bottom, Acc_top, Acc_bottom):\n",
    "    fig , axs = plt.subplots(2,1, sharex = True)\n",
    "    axs = axs.ravel()\n",
    "    # plot history\n",
    "    for history_10 in history:\n",
    "        counter = 0\n",
    "        for history_i in history_10[0:1]:\n",
    "            if counter == 0:\n",
    "                axs[0].plot(np.arange(epochs) + 1, history_i.history['loss'], label = 'Loss at SNR = %s' %(SNR), linewidth = 1, linestyle ='dashdot', color = color_shelf(SNR))\n",
    "                axs[0].plot(np.arange(epochs) + 1, history_i.history['val_loss'], label = 'Validation Loss at SNR = %s' %(SNR), linewidth = 1, linestyle ='-', color = color_shelf(SNR))\n",
    "\n",
    "                axs[1].plot(np.arange(epochs) + 1, history_i.history['acc'], label = 'Accuracy at SNR = %s' %(SNR), linewidth = 1,  linestyle ='dashdot', color =  color_shelf(SNR))\n",
    "                axs[1].plot(np.arange(epochs) + 1, history_i.history['val_acc'], label = 'Validation Accurarcy at SNR = %s' %(SNR), linewidth = 1, linestyle ='-', color =  color_shelf(SNR))\n",
    "                # set labels\n",
    "                axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "                axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "                axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "\n",
    "\n",
    "\n",
    "                # legends\n",
    "                axs[0].legend(fontsize = fontsize - 10)\n",
    "                axs[1].legend(fontsize = fontsize - 10)\n",
    "            else:\n",
    "                axs[0].plot(np.arange(epochs) + 1, history_i.history['loss'], linewidth = 1, linestyle ='dashdot', color = color_shelf(SNR))\n",
    "                axs[0].plot(np.arange(epochs) + 1, history_i.history['val_loss'], linewidth = 1, linestyle ='-', color = color_shelf(SNR))\n",
    "\n",
    "                axs[1].plot(np.arange(epochs) + 1, history_i.history['acc'], linewidth = 1, linestyle ='dashdot', color = color_shelf(SNR))\n",
    "                axs[1].plot(np.arange(epochs) + 1, history_i.history['val_acc'], linewidth = 1, linestyle ='-', color = color_shelf(SNR))\n",
    "                # set labels\n",
    "                axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "                axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "                axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "\n",
    "\n",
    "\n",
    "                # legends\n",
    "                axs[0].legend(fontsize = fontsize - 10)\n",
    "                axs[1].legend(fontsize = fontsize - 10)\n",
    "            counter += 1\n",
    "        SNR +=1    \n",
    "    # grids\n",
    "    axs[0].grid()\n",
    "    axs[1].grid()\n",
    "    axs[0].set_xlim([1, epochs])\n",
    "    #axs[0].set_ylim(top = Loss_top, bottom = Loss_bottom)\n",
    "    axs[0].set_xticks(np.arange(epochs) +1)\n",
    "    axs[1].set_xlim([1, epochs])\n",
    "    #axs[1].set_ylim(top = Acc_top, bottom = Acc_bottom)\n",
    "    axs[1].set_xticks(np.arange(epochs) +1)\n",
    "    plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "    for ax in axs:\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "#history = history_saver[0][0]\n",
    "Loss_top = 0.8\n",
    "Loss_bottom = 0.5\n",
    "Acc_top = 0.8\n",
    "Acc_bottom = 0.3\n",
    "plotlossacc_together(history, fontsize, 1, Loss_top, Loss_bottom, Acc_top, Acc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "#history = history_saver[0][0]\n",
    "Loss_top = 0.8\n",
    "Loss_bottom = 0.3\n",
    "Acc_top = 1.0\n",
    "Acc_bottom = 0.3\n",
    "plotlossacc_together(history[3:7], fontsize, 4, Loss_top, Loss_bottom, Acc_top, Acc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "#history = history_saver[0][0]\n",
    "Loss_top = 0.6\n",
    "Loss_bottom = 0.0\n",
    "Acc_top = 1.0\n",
    "Acc_bottom = 0.6\n",
    "plotlossacc_together(history[7:], fontsize, 8, Loss_top, Loss_bottom, Acc_top, Acc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathandname='1736waveformSNR110randombegin.pkl'\n",
    "fp = open(pathandname,\"w\")\n",
    "pickle.dump([tscores, history, signal_preds, test_label_saver_for_ROC], fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "with open(\"1824waveformSNR8910.pkl\") as f:\n",
    "    tscores, history, signal_preds, test_label_saver_for_ROC = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize =20\n",
    "i = 0\n",
    "SNR = i + 5\n",
    "epochs =20\n",
    "plot_tscores(t_scores[i], fontsize)\n",
    "#plot_roc(test_label_saver_for_ROC[i], signal_preds[i], SNR)\n",
    "#plotlossacc(history[i], fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tscores(tscores, fontsize):\n",
    "    ksplit = len(tscores)    \n",
    "    fig = plt.figure(figsize=(6,2.5), dpi= 100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    plt.scatter(np.arange(ksplit ) + 1, tscores[:,1], color = 'r', s = 200)\n",
    "    plt.grid()\n",
    "    ax = plt.gca()\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    \n",
    "    plt.xlim([1, ksplit])\n",
    "    #plt.ylim([0.94, 1])\n",
    "\n",
    "    #plt.xlabel('K-Fold iteration',fontsize = fontsize)\n",
    "    plt.ylabel('Test accuracy',fontsize = fontsize)\n",
    "\n",
    "    plt.show()\n",
    "    fig = plt.figure(figsize=(6,2.5), dpi= 100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    plt.scatter(np.arange(ksplit ) + 1, tscores[:,0], color = 'r', s = 200)\n",
    "    plt.grid()\n",
    "    ax = plt.gca()\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    \n",
    "    #plt.xlim([bottom = 0])\n",
    "    #plt.ylim([0.94, 1])\n",
    "\n",
    "    plt.xlabel('K-Fold iteration',fontsize = fontsize)\n",
    "    plt.ylabel('Test loss',fontsize = fontsize)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(test_label, signal_preds, SNR, plot):\n",
    "    if plot == True:\n",
    "        fig = plt.figure()\n",
    "    fa = []#[[] for i in range(len(test_label))]\n",
    "    ta = []#[[] for i in range(len(test_label))]\n",
    "    for i, j in zip(test_label, signal_preds):\n",
    "        tem_fa, tem_ta, _ = metrics.roc_curve(i[:,1], j[:,1])\n",
    "        fa.append(tem_fa)\n",
    "        ta.append(tem_ta)\n",
    "        if plot == True:\n",
    "            plt.plot(tem_fa, tem_ta, linewidth = 2, color = 'b')\n",
    "            plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "            plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "            plt.title('ROC curve for SNR %s'%(SNR), fontsize = fontsize)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "            \n",
    "             \n",
    "    if plot == True:            \n",
    "        plt.grid()\n",
    "        ax = plt.gca()\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "        plt.show()\n",
    "    return fa, ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpta(fa, ta, given_FAP):\n",
    "     \n",
    "    ta_fun = interpolate.interp1d(fa, ta)\n",
    "    \n",
    "    ta_interp = ta_fun(given_FAP)\n",
    "    \n",
    "    return ta_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57456645, 0.41542672, 0.41542036, 0.41971365, 0.417178  , 0.4235626 , 0.42090472, 0.41190883, 0.40760758, 0.55043136])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_scores[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_FAR = 0.1;\n",
    "given_FAR2 = 0.01;\n",
    "given_FAR3 = 0.001;\n",
    "\n",
    "SNRs = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n",
    "ta_interp_01 = np.zeros(len(SNRs))\n",
    "ta_interp_001 = np.zeros(len(SNRs))\n",
    "ta_interp_0001 = np.zeros(len(SNRs))\n",
    "mark_best =[]\n",
    "for i in range(len(SNRs)):  \n",
    "    fa, ta = plot_roc(test_label_saver_for_ROC[i], signal_preds[i], SNRs[i], False)\n",
    "    j = np.argmin(t_scores[i][:,0])     \n",
    "    mark_best.append(j)\n",
    "    ta_interp_01[i] = interpta(fa[j], ta[j], given_FAR)\n",
    "    ta_interp_001[i] = interpta(fa[j], ta[j], given_FAR2)\n",
    "    ta_interp_0001[i] = interpta(fa[j], ta[j], given_FAR3)\n",
    "plt.plot(SNRs, ta_interp_01, linewidth = 2.0, color = 'r', label = 'False Alarm Rate = 0.1')\n",
    "plt.plot(SNRs, ta_interp_001, linewidth = 2.0, color = 'b', label = 'False Alarm Rate = 0.01')\n",
    "plt.plot(SNRs, ta_interp_0001, linewidth = 2.0, color = 'g', label = 'False Alarm Rate = 0.001')\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "plt.xlim([1, 10])\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc = 7, fontsize =20)\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_label_saver_for_ROC[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(SNRs)): \n",
    "    tem_fa, tem_ta, _ = metrics.roc_curve(test_label_saver_for_ROC[i][mark_best[i]][:,1], signal_preds[i][mark_best[i]][:,1])\n",
    "    plt.plot(tem_fa, tem_ta, linewidth = 2, color = color_shelf(SNRs[i]) , label = 'SNR = %s' %(SNRs[i]))\n",
    "    plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "    plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "    plt.title('ROC curves for SNR 1-10', fontsize = fontsize)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_interp2 = np.concatenate((ta_interp2, ta_interp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(10)+1, ta_interp2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_label_saver_for_ROC[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.argmin(t_scores[i][:,0])\n",
    "fa, ta, _ = metrics.roc_curve(test_label_saver_for_ROC[0][j][:,1], signal_preds[0][j][:,1])\n",
    "fig = plt.figure()\n",
    "plt.plot(fa, ta, linewidth = 2, color = 'b')\n",
    "plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "plt.title('ROC curve for SNR %s'%(6), fontsize = fontsize)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa, ta, _ = metrics.roc_curve(test_label_saver_for_ROC[0][j][:,1], signal_preds[0][j][:,1])\n",
    "fig = plt.figure()\n",
    "plt.loglog(fa, ta, linewidth = 2, color = 'b')\n",
    "plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "plt.title('ROC curve for SNR %s'%(6), fontsize = fontsize)\n",
    "plt.xlim([0.001, 1])\n",
    "plt.ylim([0.001, 1])\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
