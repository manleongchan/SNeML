{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1337)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2674)\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib\n",
    "matplotlib.use('Qt4Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate, interpolate, signal, optimize, stats\n",
    "import cPickle as pickle\n",
    "import lal\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Dropout, BatchNormalization, Flatten, Activation\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Nadam, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pyfftw\n",
    "import progressbar\n",
    "import time\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "np.set_printoptions(edgeitems=30, linewidth=160)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for reading simulated SNe waveforms\n",
    "# This code will apply shift to the waveform \n",
    "# samples so that the waveform will always be in the certre +- user customized percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the file that contains the simulated CCSN waveforms\n",
    "filename = './Data/GWdatabase.h5'\n",
    "\n",
    "# Read the simulated CCSN waveforms\n",
    "waveformfile = h5py.File(filename, 'r')\n",
    "\n",
    "\n",
    "# The first level keys of the h5 file\n",
    "reduced_data = waveformfile.keys()[0]\n",
    "waveformfilekey = waveformfile.keys()[1]\n",
    "yeofrho = waveformfile.keys()[2]\n",
    "\n",
    "waveformfamily = []\n",
    "waveformfamily_keys = []\n",
    "\n",
    "# Since there are 1824 different simulated CCSN waveform. \n",
    "# Each of which is saved in a different waveformfile key \n",
    "# So the loop below is to retreive all the keys with which the waveform strain data is accessed,\n",
    "# and save it to waveformfamily.\n",
    "# Each waveform family has 5 different keys, so the second part is to retrieve these 5 keys, and save them\n",
    "# to waveformfamily_keys.\n",
    "\n",
    "for i, key in enumerate(waveformfile[waveformfilekey].keys()):\n",
    "    waveformfamily.append(key)\n",
    "    if i == 0:\n",
    "        for j, _ in enumerate(waveformfile[waveformfilekey][waveformfamily[i]].keys()):\n",
    "            waveformfamily_keys.append(waveformfile[waveformfilekey][waveformfamily[i]].keys()[j])\n",
    "originalSNR = np.array(waveformfile[reduced_data][u'SNR(aLIGOfrom10kpc)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56839, 670, 13156, 1416)\n"
     ]
    }
   ],
   "source": [
    "# This is to set some parameters for the training.\n",
    "# Since the waveforms are stored in the unit of strain * distan\n",
    "# the waveform amplitudes need to be divided by a distance.\n",
    "\n",
    "# Convection factor between par sec and meters\n",
    "PctMe = lal.PC_SI\n",
    "\n",
    "# The distance the waveform will be divided by, in centimeters\n",
    "Dist = 10.0 * 1e3 * PctMe * 1e2\n",
    "\n",
    "# Since the waveform samples come in different lengths, \n",
    "# so every waveform sample will be set to the longest length.\n",
    "# findmax/findmin is a variable that saves the longest/shortest length of the waveform samples.\n",
    "# k/kmin is the index referring to the longest/shortest waveform sample.\n",
    "findmax = 0\n",
    "k = 0 \n",
    "findmin = 1e10\n",
    "kmin = 0\n",
    "#length = np.zeros(len(waveformfamily))\n",
    "#waveformfamily = [waveformfamily[0]]\n",
    "with open(\"abnormalwaves.pkl\") as f:\n",
    "    mark_nonzeros, mark_suddenjump = pickle.load(f)\n",
    "abn_index = np.concatenate([mark_nonzeros, mark_suddenjump])    \n",
    "\n",
    "# Since the waveform contains 1824 waveforms, which are different both in the morophology and the duration,\n",
    "# training a network with all these waveforms may make it hard to debug. So one may want to limit the variation\n",
    "# in the waveform samples by limiting the number of waveform samples put in the training. \n",
    "no_waves_considered = 1824\n",
    "#allwave = []\n",
    "#mark_nonzero=[]\n",
    "for i in range(len(waveformfamily[0:no_waves_considered])):\n",
    "    if i in abn_index:\n",
    "        continue\n",
    "    waveformnumber = i\n",
    "\n",
    "    ts = np.array(waveformfile[waveformfilekey][waveformfamily[waveformnumber]][u't-tb(s)']) \n",
    "    #waves = np.array(waveformfile[waveformfilekey][waveformfamily[waveformnumber]][u'strain*dist(cm)']) / Dist \n",
    "    #allwave.append(waves)\n",
    "    if findmax < len(ts):\n",
    "        findmax = len(ts)\n",
    "        k = i\n",
    "    if findmin > len(ts):\n",
    "        findmin = len(ts)\n",
    "        kmin = i\n",
    "\n",
    "print(findmax, k, findmin, kmin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_to_set_SNR_single_waveform(preset_SNR, original_waveform, dt, Det):\n",
    "    \n",
    "    ds = len(original_waveform) * dt\n",
    "    df = 1.0 / ds\n",
    "    \n",
    "    Nf = int((len(original_waveform) // 2 + 1))\n",
    "    fs = np.arange(Nf) * df   \n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "    \n",
    "    fftinput_for_snr = pyfftw.empty_aligned(len(original_waveform), dtype='complex128')     \n",
    "    fft_object_for_snr = pyfftw.builders.rfft(fftinput_for_snr)      \n",
    "    \n",
    "    wave_f = fft_object_for_snr(original_waveform) * dt\n",
    "    \n",
    "    temporary_snr = np.sqrt( 4.0 * sum( abs(wave_f) ** 2 / ASD ** 2 ) * df )\n",
    "    SNR_factor = preset_SNR / temporary_snr\n",
    "\n",
    "    wave = SNR_factor * original_waveform\n",
    "    return wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### This part is for finding \"unnatural/abnomral waves\"\n",
    "# mark_nonzeros = []\n",
    "# mark_suddenjump = []\n",
    "\n",
    "# wave = []\n",
    "# for count, i in enumerate(allwave):\n",
    "#     wave.append(rescale_to_set_SNR_single_waveform(10, i, 1.0/65535, 'H1'))\n",
    "#     if abs(wave[count][0]) > 0.2e-21:\n",
    "#         mark_nonzeros.append(count)\n",
    "#     if np.amax(abs(np.diff(wave[count]))) > 1.0e-21:\n",
    "#         mark_suddenjump.append(count)\n",
    "# #pathandname='abnormalwaves.pkl'\n",
    "# #fp = open(pathandname,\"w\")\n",
    "# #pickle.dump([mark_nonzeros, mark_suddenjump], fp)\n",
    "# fp.close()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simulated waveforms are sampled with a sampling rate equal to 65535 Hz, \n",
    "# coupled with the longest waveform is ~1.66s, this makes the longest waveform contains 1e5 elements. \n",
    "# Since this code will make other waveforms the same length as the longest length, this requires huge amount of memory,\n",
    "# and makes training very slow and difficult. \n",
    "# Therefore, this codes uses scipy.signal.decimate to down sample the waveforms\n",
    "\n",
    "\n",
    "def padandextractwave(waveformfile, waveformfilekey, waveformfamily, strainkey, wavemaxlength, Dist, no_waves_considered, R, abn_index, alpha):\n",
    "    # Number of simulated waveforms considered\n",
    "    actualwavenumber = no_waves_considered - len(abn_index)\n",
    "    noofwaves = len(waveformfamily[0:actualwavenumber])\n",
    "    \n",
    "    msg = 'Reading waveforms from file and downsampling them by a factor of %s............' %(R)\n",
    "    print(msg)\n",
    "    bar = progressbar.ProgressBar(max_value = actualwavenumber)\n",
    "    \n",
    "    # downsample factor, the downsampled waveform will have length = original length / R\n",
    "    \n",
    "    # Vector used to save the downsampled waveform\n",
    "    downsampled_waveforms = np.array([np.zeros(wavemaxlength / R) for i in range(noofwaves)])\n",
    "    wave_take_in = 0\n",
    "    for i, whichsimulation in enumerate(waveformfamily[0:no_waves_considered]):\n",
    "        \n",
    "        if i in abn_index:\n",
    "            continue\n",
    "        \n",
    "        # convert the unit of the waveform from strain*distance to strain\n",
    "        wave = np.array(waveformfile[waveformfilekey][whichsimulation][strainkey]) / Dist\n",
    "        wavelength = len(wave)\n",
    "        windows = signal.tukey(wavelength, alpha = alpha)\n",
    "        wave = windows * wave\n",
    "        # Pad the waveform with zero so that it has the same length as the longest waveform, \n",
    "        # or whatever length is set by wavemaxlength\n",
    "        temporary = np.pad(wave, (0, wavemaxlength - wavelength), 'constant', constant_values = 0)\n",
    "        \n",
    "        # down sample\n",
    "        downsampled_waveforms[wave_take_in] = signal.decimate(temporary, R, ftype='iir')\n",
    "        bar.update(wave_take_in + 1)\n",
    "        wave_take_in += 1\n",
    "    return downsampled_waveforms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% (23 of 1758) |                      | Elapsed Time: 0:00:00 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading waveforms from file and downsampling them by a factor of 16............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98% (1736 of 1758) |################### | Elapsed Time: 0:00:04 ETA:   0:00:00"
     ]
    }
   ],
   "source": [
    "## # Since the original longest waveform length may not be dividable by the down sample vector, \n",
    "# this is to ensure that the length will be dividable. \n",
    "R = 16\n",
    "findmax = 56839#108512\n",
    "\n",
    "findmax = np.ceil(findmax/float(R)) * R\n",
    "\n",
    "# the assumed observation/simulation duration for every waveform \n",
    "Tobs = findmax / 65535.0\n",
    "#start = time.time()\n",
    "SNewaves = padandextractwave(waveformfile, waveformfilekey, waveformfamily, u'strain*dist(cm)', int(findmax), Dist, no_waves_considered, R, abn_index, 0.1)\n",
    "#elapsed = time.time() - start\n",
    "#print(elapsed)\n",
    "# Using the downsampled waveform to compute the new sampling rate\n",
    "New_sr = (len(SNewaves[0])) / Tobs\n",
    "# the new spacing in time\n",
    "New_dt = 1.0 / New_sr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASDtxt(x):\n",
    "    \"\"\"This function reads the following noise curves given a detector name.\"\"\"\n",
    "    return {\n",
    "        'LET':'./ASD/ET_D.txt',\n",
    "        'LCE':'./ASD/CE.txt',\n",
    "        'H1': './ASD/ligoII_NS.txt',\n",
    "        'L1': './ASD/ligoII_NS.txt',\n",
    "        'V1': './ASD/virgoII.txt',\n",
    "        'I2': './ASD/ligoII_NS.txt',\n",
    "        'KAGRA': './ASD/ligoII_NS.txt',\n",
    "        'ET_1': './ASD/ET_D.txt',\n",
    "        'ET_2': './ASD/ET_D.txt',\n",
    "        'ET_3': './ASD/ET_D.txt',\n",
    "        'A2': './ASD/ligoII_NS.txt',\n",
    "        'A2.5': './ASD/ligoII_NS.txt',\n",
    "    }[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readnos(detector, f_points):\n",
    "    \"\"\"This function interpolates the noise given the frequency samples.\"\"\"\n",
    "    nos_file = ASDtxt(detector)\n",
    "    f_str = []\n",
    "    ASD_str = []\n",
    "    file = open(nos_file, 'r')\n",
    "    readFile = file.readlines()\n",
    "    file.close()\n",
    "    f = []\n",
    "    ASD = []\n",
    "    \n",
    "    for line in readFile:\n",
    "        p = line.split()\n",
    "        f_str.append(float(p[0]))\n",
    "        ASD_str.append(float(p[1]))\n",
    "    f = np.log10(np.array(f_str))\n",
    "    ASD = np.log10(np.array(ASD_str))\n",
    "    nosinterpolate = interpolate.splrep(f, ASD, w=1.0*np.ones(len(ASD)), s=0)\n",
    "    \n",
    "    nos = interpolate.splev(np.log10(f_points), nosinterpolate, der = 0, ext = 3)\n",
    "    nos = 10**nos\n",
    "    \n",
    "    return nos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisegenerator(Tobs, det, SR, df, dt):\n",
    "    \"\"\"This function generates noise based on amplitude spectral density\"\"\"\n",
    "    \n",
    "    # The number of time stamps\n",
    "    Ns = Tobs * SR \n",
    "    \n",
    "    # The number of the frequency samples\n",
    "    Nf = int(Ns // 2 + 1)\n",
    "    \n",
    "    # The frequency sample\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # read ASD\n",
    "    ASD = readnos(det, fs)\n",
    "    #plt.loglog(fs, ASD)\n",
    "    #plt.show()\n",
    "    #dd\n",
    "    \n",
    "    PSD = ASD ** 2\n",
    "    # scale the ASD by the observation time, and this will be the highest amplitude of the generated noise\n",
    "    Amp = np.sqrt(0.25 * Tobs * PSD)\n",
    "    \n",
    "    \n",
    "    idx = np.argwhere(PSD==0.0)\n",
    "    Amp[idx] = 0.0\n",
    "    \n",
    "    real_nos = Amp * np.random.normal(0.0, 1.0, Nf)\n",
    "    img_nos = Amp * np.random.normal(0.0, 1.0, Nf)\n",
    "    \n",
    "    # This is to ensure there is no strange behaviour from noise at low frequency.\n",
    "    # This is because the interpolation function will interpolate strange values at frequencies betweem 1 - 10Hz.\n",
    "    #low_cutoff = 20\n",
    "    #high_cutoff = 2048\n",
    "    \n",
    "    #idx_1 =  int(low_cutoff/df)\n",
    "    #real_nos[0:idx_1] = 0\n",
    "    #img_nos[0:idx_1] = 0\n",
    "    #idx_2 = int(high_cutoff/df)\n",
    "    #real_nos[idx_2:] = 0\n",
    "    #img_nos[idx_2:] = 0\n",
    "    \n",
    "    nos = real_nos + 1j * img_nos\n",
    "\n",
    "    \n",
    "    # Fourier transiform converts the generated noise to the tme domain\n",
    "    fftinput = pyfftw.empty_aligned(len(nos), dtype='complex128')\n",
    "    \n",
    "    fft_object = pyfftw.builders.irfft(fftinput)\n",
    "\n",
    "    nos_realization = Ns* fft_object(nos) * df\n",
    "\n",
    "    return ASD, nos_realization, fs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNR_calculator(waves_in_time_domain, dt, Det):\n",
    "    \n",
    "    length = len(waves_in_time_domain)\n",
    "    \n",
    "    df = 1.0 / (length * dt)\n",
    "    \n",
    "    Nf = int((length // 2 + 1))\n",
    "    \n",
    "    fftinput_for_snr = pyfftw.empty_aligned(length, dtype='complex128')     \n",
    "    fft_object_for_snr = pyfftw.builders.rfft(fftinput_for_snr)      \n",
    "     \n",
    "    # frequency samples\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "        \n",
    "    temporary_wave_in_f = fft_object_for_snr(waves_in_time_domain) * dt\n",
    "    \n",
    "    snr = np.sqrt( 4.0 * sum( abs(temporary_wave_in_f) ** 2 / ASD ** 2 ) * df )\n",
    "    \n",
    "    return snr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_to_set_SNR(preset_SNR, SNewaves, dt, Det):\n",
    "    \n",
    "    df = 1.0 / (len(SNewaves[0]) * dt)\n",
    "    fftinput_for_snr = pyfftw.empty_aligned(len(SNewaves[0]), dtype='complex128')     \n",
    "    fft_object_for_snr = pyfftw.builders.rfft(fftinput_for_snr)      \n",
    "    \n",
    "    Nf = int((len(SNewaves[0]) // 2 + 1))\n",
    "    \n",
    "    # frequency samples\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "    msg = 'Rescaling the amplitude of the waveforms so that their optimal SNR is %s.........' %(preset_SNR)\n",
    "    print(msg)\n",
    "    print(\" \")\n",
    "    bar = progressbar.ProgressBar(max_value = len(SNewaves))\n",
    "    \n",
    "    \n",
    "    scaled_snewaves = np.array([np.zeros(len(SNewaves[0])) for i in range(len(SNewaves))])\n",
    "    for i, wave in enumerate(SNewaves):\n",
    "        temporary_wave_in_f = np.fft.rfft(wave)*dt #fft_object_for_snr(wave) * dt\n",
    "        temporary_snr = np.sqrt( 4.0 * sum( abs(temporary_wave_in_f) ** 2 / ASD ** 2 ) * df )\n",
    "        SNR_factor = preset_SNR / temporary_snr\n",
    "        \n",
    "        scaled_snewaves[i] = SNR_factor * wave\n",
    "        #print(temporary_snr)\n",
    "        #print(  np.sqrt(4.0 * sum(abs(fft_object_for_snr(SNewaves[i]) * dt) **2 / ASD ** 2) * df))\n",
    "        bar.update(i)\n",
    "    \n",
    "    return scaled_snewaves\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(seed, ts, dt, Sr, percentage, Det, SNewaves, N_rz, multiplication):\n",
    "    \"\"\"This function generates the data for training/validation/testing.\"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # The number of sample will be equal to the number of N_rz(noise realizations)\n",
    "    data = np.array([np.zeros_like(ts) for i in range(N_rz)])\n",
    "    \n",
    "    # Signal to noise ratio\n",
    "    #SNR = np.zeros(N_rz)\n",
    "    \n",
    "    # Number of time stamps\n",
    "    Ns = len(ts)\n",
    "    \n",
    "    # Number of frequency samples\n",
    "    Nf = int(Ns //2 + 1)\n",
    "    \n",
    "    # Observation time\n",
    "    Tobs = ts[-1] + dt\n",
    "    \n",
    "    # spacing in the frequency domain\n",
    "    df = 1.0/Tobs\n",
    "    # frequency samples\n",
    "    fs = np.arange(Nf) * df\n",
    "    \n",
    "    # Amplitude spectral density\n",
    "    ASD = readnos(Det, fs)\n",
    "    \n",
    "    \n",
    "    toolbar_width = N_rz\n",
    "\n",
    "    \n",
    "    \n",
    "    msg = 'Generating noise realizations.......'\n",
    "    print(msg)\n",
    "    print(\" \")\n",
    "    # setup toolbar\n",
    "    bar = progressbar.ProgressBar(max_value=toolbar_width)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Generate noise\n",
    "    for i in range(N_rz):\n",
    "        #if (i+1) % 1000 == 0 & i != N_rz - 1:\n",
    "        #   msg = 'The %s th to %s th noise realizations are now being generated.' %(i+1, i+1000)\n",
    "        #    print(msg)\n",
    "        _, data[i], _ = noisegenerator(Tobs, Det, Sr, df, dt)\n",
    "        bar.update(i+1)\n",
    "\n",
    "\n",
    "\n",
    "    msg = 'Adding noise to signals and converting them back to the time domain after whitening them in the frequency domain.....'\n",
    "    print(msg)\n",
    "    print(\" \")\n",
    "    bar_2 = progressbar.ProgressBar(max_value=toolbar_width)\n",
    "    \n",
    "    \n",
    "    if ts[-1] == signal_duration:   \n",
    "\n",
    "        for i in range(multiplication):\n",
    "            for j in range(len(SNewaves)):\n",
    "\n",
    "                count = i * len(SNewaves) + j\n",
    "                #if (count + 1) % 1000 == 0 and count < 4999:\n",
    "                #    msg = 'The %s th to %s th samples of the data set are now being generated.' %(count + 1,count + 1000)\n",
    "                #    print(msg)\n",
    "                data[count] += SNewaves[j]\n",
    "\n",
    "\n",
    "                fftinput_1 = pyfftw.empty_aligned(len(data[count]), dtype='complex128')\n",
    "                fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "                temporary = fft_object_1(data[count]) * 1.0/Sr\n",
    "                temporary = temporary / ASD \n",
    "\n",
    "\n",
    "                #SNR[count] = np.sqrt(4.0 * sum(abs(temporary[int(100/df): int(500/df)]) ** 2 * df))\n",
    "                #SNR_factor = SNR_set / SNR[count]\n",
    "                #temporary = temporary * SNR_factor\n",
    "                #if SNR_factor > 1:\n",
    "                #    print(SNR_factor,count) \n",
    "                #print(SNR_factor, np.sqrt(4.0 * sum(abs(temporary) ** 2 * df)))\n",
    "                fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "                fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "                data[count] = Ns * fft_object_2(temporary) * df * np.sqrt(2.0/ Sr)\n",
    "                bar_2.update( count + 1)\n",
    "    elif ts[-1] > signal_duration:\n",
    "        for i in range(multiplication):\n",
    "            for j in range(len(SNewaves)):\n",
    "\n",
    "                count = i * len(SNewaves) + j\n",
    "                #if (count + 1) % 1000 == 0 and count < 4999:\n",
    "                #    msg = 'The %s th to %s th samples of the data set are now being generated.' %(count + 1,count + 1000)\n",
    "                #    print(msg)\n",
    "                # This is to draw a random and determine     \n",
    "                random_shift_percentage = np.random.uniform(-percentage, percentage)\n",
    "                original_starting_point = sample_length / 2 - signal_length / 2\n",
    "                shifted_starting_point = int(original_starting_point + ts[-1] * Sr * random_shift_percentage)\n",
    "                \n",
    "                data[count][shifted_starting_point: shifted_starting_point + signal_length] = data[count][shifted_starting_point: shifted_starting_point + signal_length] + SNewaves[j]\n",
    "        \n",
    "                fftinput_1 = pyfftw.empty_aligned(len(data[count]), dtype='complex128')\n",
    "                fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "                temporary = fft_object_1(data[count]) * 1.0 / Sr\n",
    "                temporary = temporary / ASD \n",
    "\n",
    "\n",
    "                #SNR[count] = np.sqrt(4.0 * sum(abs(temporary[int(100/df): int(500/df)]) ** 2 * df))\n",
    "                #SNR_factor = SNR_set / SNR[count]\n",
    "                #temporary[int(100/df): int(500/df)] = temporary[int(100/df): int(500/df)] * SNR_factor\n",
    "                \n",
    "                #if SNR_factor > 1:\n",
    "                #    print(SNR_factor,count) \n",
    "                #print(SNR_factor, np.sqrt(4.0 * sum(abs(temporary) ** 2 * df)))\n",
    "                \n",
    "                fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "                fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "                data[count] = Ns * fft_object_2(temporary) * df * np.sqrt(2.0/ Sr)\n",
    "                bar_2.update( count + 1 )\n",
    "    else:\n",
    "        raise Exception('The sample length should be longer than or equal to the signal length') \n",
    "\n",
    "            \n",
    "    for i in range(multiplication * len(SNewaves), N_rz):\n",
    "        fftinput_1 = pyfftw.empty_aligned(len(data[i]), dtype='complex128')\n",
    "        fft_object_1 = pyfftw.builders.rfft(fftinput_1)\n",
    "        temporary = fft_object_1(data[i]) *  1.0 / Sr \n",
    "        temporary = temporary / ASD \n",
    "        \n",
    "        fftinput_2 = pyfftw.empty_aligned(len(temporary), dtype='complex128')\n",
    "        fft_object_2 = pyfftw.builders.irfft(fftinput_2)\n",
    "        data[i] = Ns * fft_object_2(temporary) * df * np.sqrt(2.0/ Sr)\n",
    "        bar_2.update(i + 1)\n",
    "            \n",
    "            \n",
    "    return data #SNR\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten_data(not_whitened_data_in_time_domain, ASD, dt, SR):\n",
    "    \n",
    "    num = len(not_whitened_data_in_time_domain)\n",
    "    signal_len = len(not_whitened_data_in_time_domain[0])\n",
    "    \n",
    "    whitened_data = np.array([np.zeros(signal_len ) for i in range(num)])\n",
    "    fftinput_in_td = pyfftw.empty_aligned(signal_len, dtype='complex128')\n",
    "    fft_object_to_f = pyfftw.builders.rfft(fftinput_in_td)\n",
    "    \n",
    "    \n",
    "    fftinput_in_fd = pyfftw.empty_aligned(signal_len//2 + 1, dtype='complex128')\n",
    "    fft_object_to_t = pyfftw.builders.irfft(fftinput_in_fd)\n",
    "    \n",
    "    for i, nwd in enumerate(not_whitened_data_in_time_domain):\n",
    "        temp = fft_object_to_f(nwd) * dt / ASD\n",
    "        whitened_data[i] = fft_object_to_t(temp) * np.sqrt(2.0/ SR) / dt\n",
    "        \n",
    "    \n",
    "    return whitened_data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd, _, _ = noisegenerator(ts[-1] + New_dt , 'H1', New_sr, 1.0/signal_duration, New_dt)\n",
    "# whitened_data = whiten_data(SNewaves, asd, New_dt, New_sr)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
>>>>>>> Stashed changes
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time stamps \n",
    "signal_length = len(SNewaves[0])\n",
    "signal_duration = (signal_length - 1) * New_dt\n",
    "\n",
    "# applying pad to make the sample longer. This is for the purpose of shifting the signal, so that the signal will appear to be in the centre +- user customised percentage\n",
    "# If no padding is to be applied\n",
<<<<<<< Updated upstream
    "sample_length = int(signal_length * 1.7 + 2)\n",
=======
    "sample_length = int(np.ceil(signal_length * 1.7)+1)\n",
>>>>>>> Stashed changes
    "\n",
    "# time stamps after pad\n",
    "ts = np.arange(sample_length) * New_dt\n",
    "sample_duration = ts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(sample, label,  shuffle_times, seed):\n",
    "    np.random.seed(seed)\n",
    "    for i in range(shuffle_times):\n",
    "        state = np.random.randint(0,100)\n",
    "        sample, label = shuffle(sample, label, random_state=state)\n",
    "        \n",
    "    return sample, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    model = Sequential()    # define the type of keras model\n",
    "\n",
    "    # add the layers\n",
    "    # conv1\n",
<<<<<<< Updated upstream
    "    model.add(Conv2D(8, (1,32), activation='elu', input_shape=input_shape, padding='same'))\n",
    "    # maxpool2\n",
    "    model.add(MaxPool2D((1,8)))\n",
    "    \n",
    "    model.add(Conv2D(8, (1,8), activation='elu', padding='same'))\n",
    "\n",
=======
    "    model.add(Conv2D(8, (1,32) , activation='elu', input_shape=input_shape, padding = 'same'))\n",
    "    #model.add(Activation('elu'))\n",
    "    \n",
>>>>>>> Stashed changes
    "    model.add(MaxPool2D((1,8)))\n",
    "    \n",
    "    model.add(Conv2D(8, (1,8), activation='elu', padding = 'same'))\n",
    "    \n",
    "    model.add(MaxPool2D((1,4)))\n",
    "    \n",
    "    # conv2\n",
<<<<<<< Updated upstream
    "    model.add(Conv2D(6, (1,4), activation='elu', padding='same'))\n",
    "    # maxpool2\n",
    "    model.add(MaxPool2D((1,4)))\n",
    "    \n",
    "    model.add(Conv2D(4, (1,4), activation='elu', padding='same'))\n",
    "    # maxpool2\n",
    "    model.add(MaxPool2D((1,2)))\n",
=======
    "    model.add(Conv2D(6, (1,4), activation='elu', padding = 'same'))\n",
    "    \n",
    "    model.add(MaxPool2D((1,4)))\n",
    "    \n",
    "    model.add(Conv2D(4, (1,4), activation='elu', padding = 'same'))\n",
    "    \n",
    "    model.add(MaxPool2D((1,2)))\n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.3))\n",
    "    # maxpool2\n",
    "    #model.add(MaxPool2D((1,4)))\n",
>>>>>>> Stashed changes
    "    # the input the fully connected layer must be 1-D vector\n",
    "    #model.add(Conv2D(32, (1,2), activation='elu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Dropout(0.4))\n",
    "    # maxpool2\n",
    "    #model.add(MaxPool2D((1,2)))\n",
    " \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='elu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    dol = keras.layers.Dropout(0.5, noise_shape=None, seed=10)\n",
<<<<<<< Updated upstream
    "\n",
=======
>>>>>>> Stashed changes
    "    model.add(dol)\n",
    "    # add the output layer with softmax actiavtion for classication\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.set_image_data_format('channels_first')\n",
    "m = make_model((1, 1, sample_length), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_for_diff_SNRs(degree_of_repeat_for_signal, original_waveforms, shift_percentage, data_generator_seed, Det, N_rz, ts, dt, sr,\n",
    "                       batch_size, epochs, kfold_splits, weight_file_name, validation, SNRs):\n",
    "    \n",
    "    if 'data' in globals():\n",
    "        del data, label\n",
    "    \n",
    "    \n",
    "    \n",
    "    presence = len(original_waveforms) * degree_of_repeat_for_signal  #number of samples that contain noise + signal\n",
    "    data_shuffle_seed = data_generator_seed + 10    \n",
    "    kfold_seed = data_generator_seed + 20  # seed for kfold\n",
    "    tscores = [[] for i in range(len(SNRs))]\n",
    "    history_saver = [[] for i in range(len(SNRs))]\n",
    "    num_classes = 2\n",
    "    counter = 0\n",
    "    test_label_saver_for_ROC = [[] for i in range(len(SNRs))]\n",
    "    signal_preds = [[] for i in range(len(SNRs))]\n",
    "    for SNR_set in SNRs:\n",
    "        \n",
    "        weight_file_name_2 = \"SNR_%s.hdf5\" %(SNR_set)\n",
    "        weight_file_name_3 = ''.join([weight_file_name, weight_file_name_2])\n",
    "        \n",
    "        scaled_waveforms = rescale_to_set_SNR(SNR_set, original_waveforms, dt, Det) \n",
    "        # Number of noise realization. This will be the final number of data samples for training + validation + testing\n",
    "        # waveform No. 193 is problematic\n",
    "        data = data_generator(data_generator_seed, ts, dt, sr, shift_percentage, Det, scaled_waveforms, N_rz, degree_of_repeat_for_signal)\n",
    "        label = np.concatenate((np.ones(presence), np.zeros(N_rz - presence)))\n",
    "    \n",
    "        data, label = shuffle_data(data, label,  1, data_shuffle_seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"This part is dedicated to testing the performance of a network by carrying out a k-fold cross validation\"\"\"\n",
    "\n",
    "        # number of time series per batch\n",
    "        # number of full passes of the dataset\n",
    "        # directory to store results in\n",
    "\n",
    "\n",
    "        kfold = StratifiedKFold(kfold_splits, shuffle = True, random_state = kfold_seed)\n",
    "        keras.backend.set_image_data_format('channels_first')\n",
    "        \n",
    "        modelCheck = ModelCheckpoint(weight_file_name_3, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=0)\n",
    "\n",
    "\n",
    "        index_for_signal = np.array([i for i in range(presence - validation, presence)])\n",
    "        rest_for_signal = np.array([i for i in range(presence - validation)])\n",
    "        \n",
    "        index_for_noise = np.array([i for i in range(N_rz - validation, N_rz)])\n",
    "        rest_for_noise = np.array([i for i in range(presence, N_rz - validation)])\n",
    "\n",
    "        save_for_val = np.concatenate([index_for_signal, index_for_noise])\n",
    "        rest = np.concatenate([rest_for_signal, rest_for_noise])\n",
    "        data_for_val = data[save_for_val]\n",
    "        label_for_val = label[save_for_val]\n",
    "\n",
    "        sample_length = len(data[0])\n",
    "        data_for_val = data_for_val.reshape(-1, 1, 1, sample_length)\n",
    "        label_for_val = keras.utils.to_categorical(label_for_val, num_classes)\n",
    "        \n",
    "       \n",
    "        msg = \"Training the network on signals with SNR = %s\" %(SNR_set)\n",
    "        print(msg)        \n",
    "        counter_train = 0\n",
    "        for train, test in kfold.split(data[rest],label[rest]):\n",
    "            msg = ''.join([\"Training for the %s\" %(counter_train + 1), \" th times.\"])\n",
    "            print(msg)\n",
    "\n",
    "            data_for_train = data[rest][train]\n",
    "            label_for_train = label[rest][train]\n",
    "\n",
    "            data_for_test = data[rest][test]\n",
    "            label_for_test = label[rest][test]\n",
    "\n",
    "            data_for_train = data_for_train.reshape(-1, 1, 1, sample_length)\n",
    "            data_for_test = data_for_test.reshape(-1, 1, 1, sample_length)\n",
    "\n",
    "            input_shape = data_for_train.shape[1:]\n",
    "            print(data_for_train.shape, input_shape)\n",
    "            label_for_train = keras.utils.to_categorical(label_for_train , num_classes)\n",
    "            label_for_test = keras.utils.to_categorical(label_for_test, num_classes)\n",
    "  \n",
    "\n",
    "            if \"model\" in locals() or \"model\" in globals():\n",
    "                del model\n",
    "            model = make_model(input_shape, num_classes)\n",
    "\n",
    "            # compile the model #adam = keras.optimizers.Adam(lr=0.01)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer= Nadam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "            history = model.fit(data_for_train, label_for_train, batch_size=batch_size, epochs=epochs, \n",
    "                                verbose=1, validation_data=(data_for_val, label_for_val), callbacks = [modelCheck], shuffle = False)\n",
    "\n",
    "\n",
    "            model.load_weights(weight_file_name_3)\n",
    "            # evaluate\n",
    "            eval_results = model.evaluate(data_for_test, label_for_test, verbose=1)\n",
    "            print('The result of testing the model against test data is:')\n",
    "            print('Test loss: %s'%(eval_results[0]))\n",
    "            print('Test accuracy %s:' %(eval_results[1]))\n",
    "            print(' ')\n",
    "            tscores[counter].append(eval_results)\n",
    "            history_saver[counter].append(history)\n",
    "            signal_preds[counter].append(model.predict(data_for_test))\n",
    "            test_label_saver_for_ROC[counter].append(label_for_test) \n",
    "            \n",
    "            \n",
    "            counter_train += 1\n",
    "        counter += 1\n",
    "    return tscores, history_saver, signal_preds, test_label_saver_for_ROC\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 21,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      " 31% (557 of 1758) |######               | Elapsed Time: 0:00:00 ETA:   0:00:00"
=======
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 1758) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
>>>>>>> Stashed changes
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Rescaling the amplitude of the waveforms so that their optimal SNR is 5.0.........\n",
=======
      "Rescaling the amplitude of the waveforms so that their optimal SNR is 1.0.........\n",
>>>>>>> Stashed changes
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      " 91% (1603 of 1758) |##################  | Elapsed Time: 0:00:00 ETA:   0:00:00"
=======
      "  0% (21 of 50000) |                     | Elapsed Time: 0:00:00 ETA:   0:03:58"
>>>>>>> Stashed changes
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating noise realizations.......\n",
      " \n"
     ]
    },
    {
<<<<<<< Updated upstream
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (6040) into shape (6041)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fcbe561ea453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mSNRs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m tscores, history, signal_preds, test_label_saver_for_ROC = kfold_for_diff_SNRs(degree_of_repeat_for_signal, SNewaves, shift_percentage, data_generator_seed, Det, N_rz, ts, New_dt, New_sr,\n\u001b[0;32m---> 18\u001b[0;31m                                                                                batch_size, epochs, kfold_splits, weight_file_name, validation, SNRs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-61a5876ccd2e>\u001b[0m in \u001b[0;36mkfold_for_diff_SNRs\u001b[0;34m(degree_of_repeat_for_signal, original_waveforms, shift_percentage, data_generator_seed, Det, N_rz, ts, dt, sr, batch_size, epochs, kfold_splits, weight_file_name, validation, SNRs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Number of noise realization. This will be the final number of data samples for training + validation + testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# waveform No. 193 is problematic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_percentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_waveforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_rz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree_of_repeat_for_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_rz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpresence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-3431627ba060>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(seed, ts, dt, Sr, percentage, Det, SNewaves, N_rz, multiplication)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#   msg = 'The %s th to %s th noise realizations are now being generated.' %(i+1, i+1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#    print(msg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoisegenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (6040) into shape (6041)"
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (142 of 50000) |                    | Elapsed Time: 0:00:00 ETA:   0:00:35"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to signals and converting them back to the time domain after whitening them in the frequency domain.....\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (49951 of 50000) |################# | Elapsed Time: 0:00:35 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network on signals with SNR = 1.0\n",
      "Training for the 1 th times.\n",
      "((43199, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43199 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43199/43199 [==============================] - 15s 356us/step - loss: 0.7011 - acc: 0.4990 - val_loss: 0.6911 - val_acc: 0.5235\n",
      "Epoch 2/30\n",
      "43199/43199 [==============================] - 13s 307us/step - loss: 0.6943 - acc: 0.5067 - val_loss: 0.6917 - val_acc: 0.5260\n",
      "Epoch 3/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6930 - acc: 0.5129 - val_loss: 0.6926 - val_acc: 0.5190\n",
      "Epoch 4/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6926 - acc: 0.5119 - val_loss: 0.6927 - val_acc: 0.5200\n",
      "Epoch 5/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6923 - acc: 0.5177 - val_loss: 0.6935 - val_acc: 0.5155\n",
      "Epoch 6/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6916 - acc: 0.5203 - val_loss: 0.6938 - val_acc: 0.5180\n",
      "Epoch 7/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6911 - acc: 0.5258 - val_loss: 0.6930 - val_acc: 0.5115\n",
      "Epoch 8/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6900 - acc: 0.5299 - val_loss: 0.6963 - val_acc: 0.5135\n",
      "Epoch 9/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6893 - acc: 0.5342 - val_loss: 0.6975 - val_acc: 0.4990\n",
      "Epoch 10/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6950 - val_acc: 0.5120\n",
      "Epoch 11/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6870 - acc: 0.5447 - val_loss: 0.6959 - val_acc: 0.5065\n",
      "Epoch 12/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6859 - acc: 0.5470 - val_loss: 0.6989 - val_acc: 0.5010\n",
      "Epoch 13/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6844 - acc: 0.5530 - val_loss: 0.6971 - val_acc: 0.5060\n",
      "Epoch 14/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6832 - acc: 0.5568 - val_loss: 0.6981 - val_acc: 0.4975\n",
      "Epoch 15/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6819 - acc: 0.5617 - val_loss: 0.6965 - val_acc: 0.5125\n",
      "Epoch 16/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6815 - acc: 0.5612 - val_loss: 0.6966 - val_acc: 0.5095\n",
      "Epoch 17/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6808 - acc: 0.5632 - val_loss: 0.6971 - val_acc: 0.5155\n",
      "Epoch 18/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6797 - acc: 0.5655 - val_loss: 0.6982 - val_acc: 0.5070\n",
      "Epoch 19/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6788 - acc: 0.5685 - val_loss: 0.6983 - val_acc: 0.5200\n",
      "Epoch 20/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6783 - acc: 0.5699 - val_loss: 0.6986 - val_acc: 0.5070\n",
      "Epoch 21/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6778 - acc: 0.5711 - val_loss: 0.6981 - val_acc: 0.5045\n",
      "Epoch 22/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6772 - acc: 0.5722 - val_loss: 0.6994 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6769 - acc: 0.5727 - val_loss: 0.6999 - val_acc: 0.5015\n",
      "Epoch 24/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6762 - acc: 0.5740 - val_loss: 0.7001 - val_acc: 0.4980\n",
      "Epoch 25/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6756 - acc: 0.5754 - val_loss: 0.6989 - val_acc: 0.5070\n",
      "Epoch 26/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6753 - acc: 0.5756 - val_loss: 0.6975 - val_acc: 0.5040\n",
      "Epoch 27/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6752 - acc: 0.5759 - val_loss: 0.7010 - val_acc: 0.5045\n",
      "Epoch 28/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6745 - acc: 0.5786 - val_loss: 0.7005 - val_acc: 0.5060\n",
      "Epoch 29/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6744 - acc: 0.5757 - val_loss: 0.7005 - val_acc: 0.4950\n",
      "Epoch 30/30\n",
      "43199/43199 [==============================] - 13s 307us/step - loss: 0.6740 - acc: 0.5782 - val_loss: 0.7017 - val_acc: 0.4860\n",
      "4801/4801 [==============================] - 1s 166us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6931895456371693\n",
      "Test accuracy 0.5042699437617163:\n",
      " \n",
      "Training for the 2 th times.\n",
      "((43199, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43199 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43199/43199 [==============================] - 14s 320us/step - loss: 0.7029 - acc: 0.4985 - val_loss: 0.6927 - val_acc: 0.5155\n",
      "Epoch 2/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6944 - acc: 0.5050 - val_loss: 0.6928 - val_acc: 0.5150\n",
      "Epoch 3/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6931 - acc: 0.5111 - val_loss: 0.6925 - val_acc: 0.5055\n",
      "Epoch 4/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6925 - acc: 0.5176 - val_loss: 0.6930 - val_acc: 0.5070\n",
      "Epoch 5/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6924 - acc: 0.5156 - val_loss: 0.6931 - val_acc: 0.5095\n",
      "Epoch 6/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6919 - acc: 0.5227 - val_loss: 0.6918 - val_acc: 0.5310\n",
      "Epoch 7/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6914 - acc: 0.5251 - val_loss: 0.6933 - val_acc: 0.5030\n",
      "Epoch 8/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6904 - acc: 0.5323 - val_loss: 0.6947 - val_acc: 0.5090\n",
      "Epoch 9/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6900 - acc: 0.5327 - val_loss: 0.6975 - val_acc: 0.5040\n",
      "Epoch 10/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6883 - acc: 0.5409 - val_loss: 0.6992 - val_acc: 0.4890\n",
      "Epoch 11/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6872 - acc: 0.5467 - val_loss: 0.7018 - val_acc: 0.4945\n",
      "Epoch 12/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6853 - acc: 0.5506 - val_loss: 0.7013 - val_acc: 0.4945\n",
      "Epoch 13/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6842 - acc: 0.5539 - val_loss: 0.7081 - val_acc: 0.4960\n",
      "Epoch 14/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6832 - acc: 0.5550 - val_loss: 0.7114 - val_acc: 0.4915\n",
      "Epoch 15/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6819 - acc: 0.5606 - val_loss: 0.7084 - val_acc: 0.4895\n",
      "Epoch 16/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6807 - acc: 0.5634 - val_loss: 0.7068 - val_acc: 0.5025\n",
      "Epoch 17/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6804 - acc: 0.5673 - val_loss: 0.7071 - val_acc: 0.5065\n",
      "Epoch 18/30\n",
      "43199/43199 [==============================] - 13s 310us/step - loss: 0.6792 - acc: 0.5693 - val_loss: 0.7097 - val_acc: 0.5050\n",
      "Epoch 19/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6782 - acc: 0.5680 - val_loss: 0.7084 - val_acc: 0.5050\n",
      "Epoch 20/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6784 - acc: 0.5701 - val_loss: 0.7105 - val_acc: 0.5075\n",
      "Epoch 21/30\n",
      "43199/43199 [==============================] - 13s 307us/step - loss: 0.6780 - acc: 0.5706 - val_loss: 0.7092 - val_acc: 0.5010\n",
      "Epoch 22/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6774 - acc: 0.5742 - val_loss: 0.7092 - val_acc: 0.4955\n",
      "Epoch 23/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6768 - acc: 0.5751 - val_loss: 0.7117 - val_acc: 0.5030\n",
      "Epoch 24/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6763 - acc: 0.5750 - val_loss: 0.7084 - val_acc: 0.5005\n",
      "Epoch 25/30\n",
      "43199/43199 [==============================] - 13s 307us/step - loss: 0.6761 - acc: 0.5772 - val_loss: 0.7134 - val_acc: 0.4945\n",
      "Epoch 26/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6753 - acc: 0.5809 - val_loss: 0.7184 - val_acc: 0.4915\n",
      "Epoch 27/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6750 - acc: 0.5768 - val_loss: 0.7229 - val_acc: 0.4845\n",
      "Epoch 28/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6747 - acc: 0.5784 - val_loss: 0.7201 - val_acc: 0.4765\n",
      "Epoch 29/30\n",
      "43199/43199 [==============================] - 13s 309us/step - loss: 0.6744 - acc: 0.5827 - val_loss: 0.7179 - val_acc: 0.4830\n",
      "Epoch 30/30\n",
      "43199/43199 [==============================] - 13s 308us/step - loss: 0.6740 - acc: 0.5853 - val_loss: 0.7215 - val_acc: 0.4860\n",
      "4801/4801 [==============================] - 1s 149us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6937759397824936\n",
      "Test accuracy 0.5069777129764632:\n",
      " \n",
      "Training for the 3 th times.\n",
      "((43200, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43200/43200 [==============================] - 14s 319us/step - loss: 0.7012 - acc: 0.5033 - val_loss: 0.6929 - val_acc: 0.5145\n",
      "Epoch 2/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6934 - acc: 0.5110 - val_loss: 0.6933 - val_acc: 0.5150\n",
      "Epoch 3/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6929 - acc: 0.5083 - val_loss: 0.6923 - val_acc: 0.5195\n",
      "Epoch 4/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6924 - acc: 0.5166 - val_loss: 0.6936 - val_acc: 0.5010\n",
      "Epoch 5/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6915 - acc: 0.5207 - val_loss: 0.6944 - val_acc: 0.4945\n",
      "Epoch 6/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6909 - acc: 0.5285 - val_loss: 0.6941 - val_acc: 0.5125\n",
      "Epoch 7/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6903 - acc: 0.5305 - val_loss: 0.6937 - val_acc: 0.5105\n",
      "Epoch 8/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6891 - acc: 0.5370 - val_loss: 0.6950 - val_acc: 0.5035\n",
      "Epoch 9/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6885 - acc: 0.5404 - val_loss: 0.6947 - val_acc: 0.5170\n",
      "Epoch 10/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6875 - acc: 0.5426 - val_loss: 0.6946 - val_acc: 0.5110\n",
      "Epoch 11/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6866 - acc: 0.5474 - val_loss: 0.6978 - val_acc: 0.4935\n",
      "Epoch 12/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6857 - acc: 0.5509 - val_loss: 0.6982 - val_acc: 0.5045\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6848 - acc: 0.5527 - val_loss: 0.7018 - val_acc: 0.4895\n",
      "Epoch 14/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6837 - acc: 0.5558 - val_loss: 0.7012 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6826 - acc: 0.5603 - val_loss: 0.7027 - val_acc: 0.4970\n",
      "Epoch 16/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6816 - acc: 0.5630 - val_loss: 0.7064 - val_acc: 0.4935\n",
      "Epoch 17/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6808 - acc: 0.5631 - val_loss: 0.7066 - val_acc: 0.5000\n",
      "Epoch 18/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6800 - acc: 0.5684 - val_loss: 0.7067 - val_acc: 0.4935\n",
      "Epoch 19/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6790 - acc: 0.5690 - val_loss: 0.7117 - val_acc: 0.4840\n",
      "Epoch 20/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6789 - acc: 0.5707 - val_loss: 0.7127 - val_acc: 0.4875\n",
      "Epoch 21/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6781 - acc: 0.5709 - val_loss: 0.7119 - val_acc: 0.5075\n",
      "Epoch 22/30\n",
      "43200/43200 [==============================] - 14s 318us/step - loss: 0.6769 - acc: 0.5741 - val_loss: 0.7127 - val_acc: 0.5030\n",
      "Epoch 23/30\n",
      "43200/43200 [==============================] - 14s 314us/step - loss: 0.6761 - acc: 0.5763 - val_loss: 0.7107 - val_acc: 0.5010\n",
      "Epoch 24/30\n",
      "43200/43200 [==============================] - 14s 314us/step - loss: 0.6761 - acc: 0.5751 - val_loss: 0.7128 - val_acc: 0.4915\n",
      "Epoch 25/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6756 - acc: 0.5761 - val_loss: 0.7136 - val_acc: 0.4950\n",
      "Epoch 26/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6750 - acc: 0.5785 - val_loss: 0.7113 - val_acc: 0.4890\n",
      "Epoch 27/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6741 - acc: 0.5798 - val_loss: 0.7164 - val_acc: 0.5035\n",
      "Epoch 28/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6739 - acc: 0.5822 - val_loss: 0.7148 - val_acc: 0.4985\n",
      "Epoch 29/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6734 - acc: 0.5826 - val_loss: 0.7134 - val_acc: 0.5005\n",
      "Epoch 30/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6735 - acc: 0.5816 - val_loss: 0.7153 - val_acc: 0.4900\n",
      "4800/4800 [==============================] - 1s 148us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6898961706956228\n",
      "Test accuracy 0.53:\n",
      " \n",
      "Training for the 4 th times.\n",
      "((43200, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43200/43200 [==============================] - 14s 323us/step - loss: 0.6986 - acc: 0.5022 - val_loss: 0.6931 - val_acc: 0.5125\n",
      "Epoch 2/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6940 - acc: 0.5078 - val_loss: 0.6930 - val_acc: 0.5105\n",
      "Epoch 3/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6931 - acc: 0.5069 - val_loss: 0.6933 - val_acc: 0.5150\n",
      "Epoch 4/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6928 - acc: 0.5145 - val_loss: 0.6933 - val_acc: 0.5165\n",
      "Epoch 5/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6923 - acc: 0.5162 - val_loss: 0.6942 - val_acc: 0.5090\n",
      "Epoch 6/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6917 - acc: 0.5181 - val_loss: 0.6945 - val_acc: 0.5220\n",
      "Epoch 7/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6915 - acc: 0.5238 - val_loss: 0.6956 - val_acc: 0.5040\n",
      "Epoch 8/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6907 - acc: 0.5294 - val_loss: 0.6969 - val_acc: 0.4985\n",
      "Epoch 9/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6897 - acc: 0.5341 - val_loss: 0.6945 - val_acc: 0.5045\n",
      "Epoch 10/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6880 - acc: 0.5387 - val_loss: 0.6978 - val_acc: 0.5030\n",
      "Epoch 11/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6866 - acc: 0.5466 - val_loss: 0.6999 - val_acc: 0.4945\n",
      "Epoch 12/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6860 - acc: 0.5486 - val_loss: 0.6980 - val_acc: 0.5185\n",
      "Epoch 13/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6847 - acc: 0.5537 - val_loss: 0.6995 - val_acc: 0.5095\n",
      "Epoch 14/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6836 - acc: 0.5569 - val_loss: 0.7017 - val_acc: 0.5005\n",
      "Epoch 15/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6830 - acc: 0.5568 - val_loss: 0.7031 - val_acc: 0.5040\n",
      "Epoch 16/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6825 - acc: 0.5591 - val_loss: 0.7052 - val_acc: 0.4975\n",
      "Epoch 17/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6814 - acc: 0.5620 - val_loss: 0.7087 - val_acc: 0.5135\n",
      "Epoch 18/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6812 - acc: 0.5648 - val_loss: 0.7109 - val_acc: 0.5010\n",
      "Epoch 19/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6805 - acc: 0.5670 - val_loss: 0.7091 - val_acc: 0.4980\n",
      "Epoch 20/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6792 - acc: 0.5681 - val_loss: 0.7109 - val_acc: 0.5000\n",
      "Epoch 21/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6788 - acc: 0.5689 - val_loss: 0.7107 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6781 - acc: 0.5687 - val_loss: 0.7093 - val_acc: 0.4930\n",
      "Epoch 23/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6776 - acc: 0.5701 - val_loss: 0.7090 - val_acc: 0.4970\n",
      "Epoch 24/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6779 - acc: 0.5721 - val_loss: 0.7130 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6772 - acc: 0.5734 - val_loss: 0.7099 - val_acc: 0.5065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6769 - acc: 0.5769 - val_loss: 0.7114 - val_acc: 0.5030\n",
      "Epoch 27/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6764 - acc: 0.5741 - val_loss: 0.7139 - val_acc: 0.4965\n",
      "Epoch 28/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6766 - acc: 0.5737 - val_loss: 0.7186 - val_acc: 0.4945\n",
      "Epoch 29/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6753 - acc: 0.5771 - val_loss: 0.7147 - val_acc: 0.4845\n",
      "Epoch 30/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6756 - acc: 0.5790 - val_loss: 0.7178 - val_acc: 0.4800\n",
      "4800/4800 [==============================] - 1s 161us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6893667364120484\n",
      "Test accuracy 0.5452083333333333:\n",
      " \n",
      "Training for the 5 th times.\n",
      "((43200, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43200/43200 [==============================] - 14s 324us/step - loss: 0.7000 - acc: 0.4987 - val_loss: 0.6924 - val_acc: 0.5145\n",
      "Epoch 2/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6939 - acc: 0.5070 - val_loss: 0.6933 - val_acc: 0.5135\n",
      "Epoch 3/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6929 - acc: 0.5114 - val_loss: 0.6929 - val_acc: 0.5180\n",
      "Epoch 4/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6926 - acc: 0.5137 - val_loss: 0.6941 - val_acc: 0.4965\n",
      "Epoch 5/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6922 - acc: 0.5203 - val_loss: 0.6940 - val_acc: 0.5045\n",
      "Epoch 6/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6917 - acc: 0.5223 - val_loss: 0.6948 - val_acc: 0.4895\n",
      "Epoch 7/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6908 - acc: 0.5288 - val_loss: 0.6963 - val_acc: 0.4960\n",
      "Epoch 8/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6903 - acc: 0.5310 - val_loss: 0.6980 - val_acc: 0.4995\n",
      "Epoch 9/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6894 - acc: 0.5327 - val_loss: 0.6980 - val_acc: 0.5005\n",
      "Epoch 10/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6882 - acc: 0.5380 - val_loss: 0.6974 - val_acc: 0.5015\n",
      "Epoch 11/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6872 - acc: 0.5436 - val_loss: 0.7001 - val_acc: 0.5030\n",
      "Epoch 12/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6862 - acc: 0.5483 - val_loss: 0.7008 - val_acc: 0.5015\n",
      "Epoch 13/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6847 - acc: 0.5522 - val_loss: 0.7006 - val_acc: 0.5065\n",
      "Epoch 14/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6841 - acc: 0.5541 - val_loss: 0.7028 - val_acc: 0.5010\n",
      "Epoch 15/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6824 - acc: 0.5593 - val_loss: 0.7037 - val_acc: 0.5020\n",
      "Epoch 16/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6822 - acc: 0.5599 - val_loss: 0.7022 - val_acc: 0.5135\n",
      "Epoch 17/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6811 - acc: 0.5630 - val_loss: 0.7022 - val_acc: 0.5145\n",
      "Epoch 18/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6797 - acc: 0.5646 - val_loss: 0.7064 - val_acc: 0.5160\n",
      "Epoch 19/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6793 - acc: 0.5686 - val_loss: 0.7064 - val_acc: 0.5110\n",
      "Epoch 20/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6789 - acc: 0.5686 - val_loss: 0.7097 - val_acc: 0.4970\n",
      "Epoch 21/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6781 - acc: 0.5701 - val_loss: 0.7070 - val_acc: 0.5130\n",
      "Epoch 22/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6769 - acc: 0.5731 - val_loss: 0.7113 - val_acc: 0.5025\n",
      "Epoch 23/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6766 - acc: 0.5754 - val_loss: 0.7112 - val_acc: 0.5035\n",
      "Epoch 24/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6761 - acc: 0.5754 - val_loss: 0.7106 - val_acc: 0.4995\n",
      "Epoch 25/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6765 - acc: 0.5742 - val_loss: 0.7105 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6761 - acc: 0.5773 - val_loss: 0.7134 - val_acc: 0.4925\n",
      "Epoch 27/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6753 - acc: 0.5784 - val_loss: 0.7107 - val_acc: 0.5100\n",
      "Epoch 28/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6749 - acc: 0.5788 - val_loss: 0.7107 - val_acc: 0.4975\n",
      "Epoch 29/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6750 - acc: 0.5785 - val_loss: 0.7105 - val_acc: 0.4980\n",
      "Epoch 30/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6748 - acc: 0.5786 - val_loss: 0.7119 - val_acc: 0.4930\n",
      "4800/4800 [==============================] - 1s 149us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6894684704144796\n",
      "Test accuracy 0.5420833333333334:\n",
      " \n",
      "Training for the 6 th times.\n",
      "((43200, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43200/43200 [==============================] - 14s 327us/step - loss: 0.7002 - acc: 0.4992 - val_loss: 0.6912 - val_acc: 0.5235\n",
      "Epoch 2/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6935 - acc: 0.5128 - val_loss: 0.6914 - val_acc: 0.5185\n",
      "Epoch 3/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6930 - acc: 0.5118 - val_loss: 0.6914 - val_acc: 0.5130\n",
      "Epoch 4/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6918 - acc: 0.5171 - val_loss: 0.6917 - val_acc: 0.5095\n",
      "Epoch 5/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6924 - acc: 0.5174 - val_loss: 0.6945 - val_acc: 0.5020\n",
      "Epoch 6/30\n",
      "43200/43200 [==============================] - 14s 315us/step - loss: 0.6914 - acc: 0.5252 - val_loss: 0.6929 - val_acc: 0.5155\n",
      "Epoch 7/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6910 - acc: 0.5264 - val_loss: 0.6938 - val_acc: 0.5080\n",
      "Epoch 8/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6899 - acc: 0.5335 - val_loss: 0.6941 - val_acc: 0.5090\n",
      "Epoch 9/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6891 - acc: 0.5345 - val_loss: 0.6945 - val_acc: 0.5150\n",
      "Epoch 10/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6875 - acc: 0.5445 - val_loss: 0.6946 - val_acc: 0.5065\n",
      "Epoch 11/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6864 - acc: 0.5485 - val_loss: 0.6964 - val_acc: 0.5020\n",
      "Epoch 12/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6851 - acc: 0.5515 - val_loss: 0.6973 - val_acc: 0.5050\n",
      "Epoch 13/30\n",
      "43200/43200 [==============================] - 14s 315us/step - loss: 0.6843 - acc: 0.5560 - val_loss: 0.6975 - val_acc: 0.5070\n",
      "Epoch 14/30\n",
      "43200/43200 [==============================] - 14s 313us/step - loss: 0.6834 - acc: 0.5577 - val_loss: 0.6987 - val_acc: 0.4955\n",
      "Epoch 15/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6822 - acc: 0.5627 - val_loss: 0.6969 - val_acc: 0.5035\n",
      "Epoch 16/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6811 - acc: 0.5628 - val_loss: 0.7013 - val_acc: 0.5085\n",
      "Epoch 17/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6811 - acc: 0.5644 - val_loss: 0.7002 - val_acc: 0.5140\n",
      "Epoch 18/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6794 - acc: 0.5697 - val_loss: 0.7008 - val_acc: 0.5110\n",
      "Epoch 19/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6786 - acc: 0.5691 - val_loss: 0.7039 - val_acc: 0.5030\n",
      "Epoch 20/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6781 - acc: 0.5701 - val_loss: 0.7055 - val_acc: 0.5080\n",
      "Epoch 21/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6772 - acc: 0.5737 - val_loss: 0.7021 - val_acc: 0.5225\n",
      "Epoch 22/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6770 - acc: 0.5746 - val_loss: 0.7054 - val_acc: 0.5105\n",
      "Epoch 23/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6763 - acc: 0.5753 - val_loss: 0.7030 - val_acc: 0.5215\n",
      "Epoch 24/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6758 - acc: 0.5764 - val_loss: 0.7058 - val_acc: 0.5150\n",
      "Epoch 25/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6753 - acc: 0.5769 - val_loss: 0.7055 - val_acc: 0.5185\n",
      "Epoch 26/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6752 - acc: 0.5784 - val_loss: 0.7046 - val_acc: 0.5225\n",
      "Epoch 27/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6751 - acc: 0.5788 - val_loss: 0.7069 - val_acc: 0.5110\n",
      "Epoch 28/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6747 - acc: 0.5784 - val_loss: 0.7103 - val_acc: 0.5130\n",
      "Epoch 29/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6741 - acc: 0.5792 - val_loss: 0.7054 - val_acc: 0.5230\n",
      "Epoch 30/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6739 - acc: 0.5818 - val_loss: 0.7115 - val_acc: 0.5210\n",
      "4800/4800 [==============================] - 1s 160us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6889280033111572\n",
      "Test accuracy 0.5383333333333333:\n",
      " \n",
      "Training for the 7 th times.\n",
      "((43200, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43200/43200 [==============================] - 14s 328us/step - loss: 0.6972 - acc: 0.5009 - val_loss: 0.6928 - val_acc: 0.5165\n",
      "Epoch 2/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6938 - acc: 0.5063 - val_loss: 0.6932 - val_acc: 0.5125\n",
      "Epoch 3/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6930 - acc: 0.5080 - val_loss: 0.6926 - val_acc: 0.5235\n",
      "Epoch 4/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6925 - acc: 0.5169 - val_loss: 0.6933 - val_acc: 0.5170\n",
      "Epoch 5/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6919 - acc: 0.5173 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6916 - acc: 0.5233 - val_loss: 0.6942 - val_acc: 0.5100\n",
      "Epoch 7/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6903 - acc: 0.5299 - val_loss: 0.6942 - val_acc: 0.5190\n",
      "Epoch 8/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6894 - acc: 0.5367 - val_loss: 0.6967 - val_acc: 0.4990\n",
      "Epoch 9/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6883 - acc: 0.5400 - val_loss: 0.6970 - val_acc: 0.4970\n",
      "Epoch 10/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6871 - acc: 0.5441 - val_loss: 0.6979 - val_acc: 0.5040\n",
      "Epoch 11/30\n",
      "43200/43200 [==============================] - 14s 315us/step - loss: 0.6866 - acc: 0.5490 - val_loss: 0.7021 - val_acc: 0.4990\n",
      "Epoch 12/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6854 - acc: 0.5511 - val_loss: 0.7008 - val_acc: 0.5120\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6839 - acc: 0.5543 - val_loss: 0.7013 - val_acc: 0.5085\n",
      "Epoch 14/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6834 - acc: 0.5591 - val_loss: 0.7030 - val_acc: 0.4995\n",
      "Epoch 15/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6820 - acc: 0.5612 - val_loss: 0.7000 - val_acc: 0.5170\n",
      "Epoch 16/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6809 - acc: 0.5647 - val_loss: 0.7029 - val_acc: 0.5150\n",
      "Epoch 17/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6802 - acc: 0.5622 - val_loss: 0.7024 - val_acc: 0.4965\n",
      "Epoch 18/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6795 - acc: 0.5653 - val_loss: 0.7035 - val_acc: 0.5015\n",
      "Epoch 19/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6793 - acc: 0.5684 - val_loss: 0.7004 - val_acc: 0.5025\n",
      "Epoch 20/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6779 - acc: 0.5725 - val_loss: 0.7024 - val_acc: 0.5035\n",
      "Epoch 21/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6769 - acc: 0.5733 - val_loss: 0.7035 - val_acc: 0.5045\n",
      "Epoch 22/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6767 - acc: 0.5736 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6760 - acc: 0.5766 - val_loss: 0.7030 - val_acc: 0.5135\n",
      "Epoch 24/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6762 - acc: 0.5740 - val_loss: 0.7062 - val_acc: 0.5005\n",
      "Epoch 25/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6757 - acc: 0.5768 - val_loss: 0.7061 - val_acc: 0.5030\n",
      "Epoch 26/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6754 - acc: 0.5765 - val_loss: 0.7047 - val_acc: 0.4965\n",
      "Epoch 27/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6750 - acc: 0.5801 - val_loss: 0.7076 - val_acc: 0.5065\n",
      "Epoch 28/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6744 - acc: 0.5801 - val_loss: 0.7068 - val_acc: 0.5015\n",
      "Epoch 29/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6736 - acc: 0.5824 - val_loss: 0.7089 - val_acc: 0.5045\n",
      "Epoch 30/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6739 - acc: 0.5830 - val_loss: 0.7049 - val_acc: 0.5010\n",
      "4800/4800 [==============================] - 1s 158us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6893348185221354\n",
      "Test accuracy 0.534375:\n",
      " \n",
      "Training for the 8 th times.\n",
      "((43200, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43200 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43200/43200 [==============================] - 14s 329us/step - loss: 0.7002 - acc: 0.5042 - val_loss: 0.6931 - val_acc: 0.5120\n",
      "Epoch 2/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6945 - acc: 0.5058 - val_loss: 0.6935 - val_acc: 0.5085\n",
      "Epoch 3/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6935 - acc: 0.5074 - val_loss: 0.6938 - val_acc: 0.4970\n",
      "Epoch 4/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6926 - acc: 0.5152 - val_loss: 0.6934 - val_acc: 0.5085\n",
      "Epoch 5/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6924 - acc: 0.5164 - val_loss: 0.6939 - val_acc: 0.5025\n",
      "Epoch 6/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6918 - acc: 0.5180 - val_loss: 0.6949 - val_acc: 0.5040\n",
      "Epoch 7/30\n",
      "43200/43200 [==============================] - 13s 310us/step - loss: 0.6912 - acc: 0.5269 - val_loss: 0.6945 - val_acc: 0.5005\n",
      "Epoch 8/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6901 - acc: 0.5324 - val_loss: 0.6934 - val_acc: 0.5170\n",
      "Epoch 9/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6893 - acc: 0.5356 - val_loss: 0.6944 - val_acc: 0.5180\n",
      "Epoch 10/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6888 - acc: 0.5421 - val_loss: 0.6966 - val_acc: 0.5085\n",
      "Epoch 11/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6863 - acc: 0.5495 - val_loss: 0.6978 - val_acc: 0.5170\n",
      "Epoch 12/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6855 - acc: 0.5501 - val_loss: 0.6998 - val_acc: 0.5210\n",
      "Epoch 13/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6842 - acc: 0.5548 - val_loss: 0.7007 - val_acc: 0.5185\n",
      "Epoch 14/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6836 - acc: 0.5560 - val_loss: 0.7030 - val_acc: 0.5060\n",
      "Epoch 15/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6825 - acc: 0.5608 - val_loss: 0.7004 - val_acc: 0.5105\n",
      "Epoch 16/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6812 - acc: 0.5614 - val_loss: 0.7068 - val_acc: 0.5140\n",
      "Epoch 17/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6808 - acc: 0.5640 - val_loss: 0.7083 - val_acc: 0.5085\n",
      "Epoch 18/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6795 - acc: 0.5665 - val_loss: 0.7069 - val_acc: 0.5030\n",
      "Epoch 19/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6790 - acc: 0.5708 - val_loss: 0.7071 - val_acc: 0.5060\n",
      "Epoch 20/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6786 - acc: 0.5717 - val_loss: 0.7065 - val_acc: 0.5180\n",
      "Epoch 21/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6773 - acc: 0.5713 - val_loss: 0.7101 - val_acc: 0.5150\n",
      "Epoch 22/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6773 - acc: 0.5718 - val_loss: 0.7071 - val_acc: 0.4960\n",
      "Epoch 23/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6763 - acc: 0.5719 - val_loss: 0.7089 - val_acc: 0.5125\n",
      "Epoch 24/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6764 - acc: 0.5769 - val_loss: 0.7103 - val_acc: 0.5065\n",
      "Epoch 25/30\n",
      "43200/43200 [==============================] - 13s 308us/step - loss: 0.6755 - acc: 0.5756 - val_loss: 0.7093 - val_acc: 0.5140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6754 - acc: 0.5754 - val_loss: 0.7094 - val_acc: 0.5065\n",
      "Epoch 27/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6749 - acc: 0.5777 - val_loss: 0.7071 - val_acc: 0.5005\n",
      "Epoch 28/30\n",
      "43200/43200 [==============================] - 13s 312us/step - loss: 0.6741 - acc: 0.5793 - val_loss: 0.7104 - val_acc: 0.5050\n",
      "Epoch 29/30\n",
      "43200/43200 [==============================] - 13s 311us/step - loss: 0.6738 - acc: 0.5813 - val_loss: 0.7068 - val_acc: 0.5120\n",
      "Epoch 30/30\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 0.6728 - acc: 0.5799 - val_loss: 0.7089 - val_acc: 0.5095\n",
      "4800/4800 [==============================] - 1s 160us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6901006559530894\n",
      "Test accuracy 0.531875:\n",
      " \n",
      "Training for the 9 th times.\n",
      "((43201, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43201 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43201/43201 [==============================] - 14s 334us/step - loss: 0.6979 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.5180\n",
      "Epoch 2/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6941 - acc: 0.5030 - val_loss: 0.6933 - val_acc: 0.5150\n",
      "Epoch 3/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6931 - acc: 0.5103 - val_loss: 0.6933 - val_acc: 0.5195\n",
      "Epoch 4/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6925 - acc: 0.5183 - val_loss: 0.6940 - val_acc: 0.5145\n",
      "Epoch 5/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6921 - acc: 0.5199 - val_loss: 0.6939 - val_acc: 0.5205\n",
      "Epoch 6/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6918 - acc: 0.5212 - val_loss: 0.6955 - val_acc: 0.5040\n",
      "Epoch 7/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6907 - acc: 0.5276 - val_loss: 0.6971 - val_acc: 0.5025\n",
      "Epoch 8/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6896 - acc: 0.5329 - val_loss: 0.6992 - val_acc: 0.4815\n",
      "Epoch 9/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6884 - acc: 0.5415 - val_loss: 0.7031 - val_acc: 0.4775\n",
      "Epoch 10/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6872 - acc: 0.5467 - val_loss: 0.7053 - val_acc: 0.4870\n",
      "Epoch 11/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6857 - acc: 0.5508 - val_loss: 0.7112 - val_acc: 0.4790\n",
      "Epoch 12/30\n",
      "43201/43201 [==============================] - 14s 313us/step - loss: 0.6840 - acc: 0.5545 - val_loss: 0.7110 - val_acc: 0.4845\n",
      "Epoch 13/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6833 - acc: 0.5573 - val_loss: 0.7129 - val_acc: 0.4815\n",
      "Epoch 14/30\n",
      "43201/43201 [==============================] - 14s 313us/step - loss: 0.6827 - acc: 0.5581 - val_loss: 0.7105 - val_acc: 0.4860\n",
      "Epoch 15/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6814 - acc: 0.5588 - val_loss: 0.7152 - val_acc: 0.4920\n",
      "Epoch 16/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6802 - acc: 0.5662 - val_loss: 0.7135 - val_acc: 0.4985\n",
      "Epoch 17/30\n",
      "43201/43201 [==============================] - 14s 313us/step - loss: 0.6795 - acc: 0.5672 - val_loss: 0.7139 - val_acc: 0.4935\n",
      "Epoch 18/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6790 - acc: 0.5697 - val_loss: 0.7147 - val_acc: 0.4850\n",
      "Epoch 19/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6781 - acc: 0.5718 - val_loss: 0.7119 - val_acc: 0.4970\n",
      "Epoch 20/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6782 - acc: 0.5712 - val_loss: 0.7116 - val_acc: 0.4880\n",
      "Epoch 21/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6773 - acc: 0.5726 - val_loss: 0.7132 - val_acc: 0.4930\n",
      "Epoch 22/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6765 - acc: 0.5743 - val_loss: 0.7111 - val_acc: 0.4980\n",
      "Epoch 23/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6761 - acc: 0.5734 - val_loss: 0.7129 - val_acc: 0.4965\n",
      "Epoch 24/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6751 - acc: 0.5761 - val_loss: 0.7149 - val_acc: 0.4880\n",
      "Epoch 25/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6748 - acc: 0.5783 - val_loss: 0.7105 - val_acc: 0.4865\n",
      "Epoch 26/30\n",
      "43201/43201 [==============================] - 13s 310us/step - loss: 0.6745 - acc: 0.5785 - val_loss: 0.7117 - val_acc: 0.4950\n",
      "Epoch 27/30\n",
      "43201/43201 [==============================] - 13s 310us/step - loss: 0.6742 - acc: 0.5785 - val_loss: 0.7106 - val_acc: 0.4945\n",
      "Epoch 28/30\n",
      "43201/43201 [==============================] - 13s 310us/step - loss: 0.6733 - acc: 0.5819 - val_loss: 0.7119 - val_acc: 0.5045\n",
      "Epoch 29/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6734 - acc: 0.5804 - val_loss: 0.7124 - val_acc: 0.4930\n",
      "Epoch 30/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6729 - acc: 0.5836 - val_loss: 0.7095 - val_acc: 0.5115\n",
      "4799/4799 [==============================] - 1s 162us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6900693623317632\n",
      "Test accuracy 0.538237132810508:\n",
      " \n",
      "Training for the 10 th times.\n",
      "((43201, 1, 1, 6042), (1, 1, 6042))\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 8, 1, 6042)        264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 8, 1, 755)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 8, 1, 755)         520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 8, 1, 188)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 6, 1, 188)         198       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 6, 1, 47)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 4, 1, 47)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 4, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 92)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                2976      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,124\n",
      "Trainable params: 4,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43201 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "43201/43201 [==============================] - 15s 336us/step - loss: 0.7019 - acc: 0.4986 - val_loss: 0.7040 - val_acc: 0.5180\n",
      "Epoch 2/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6946 - acc: 0.5032 - val_loss: 0.6995 - val_acc: 0.5180\n",
      "Epoch 3/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6935 - acc: 0.5066 - val_loss: 0.6922 - val_acc: 0.5150\n",
      "Epoch 4/30\n",
      "43201/43201 [==============================] - 14s 313us/step - loss: 0.6928 - acc: 0.5136 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 5/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6924 - acc: 0.5180 - val_loss: 0.6929 - val_acc: 0.5140\n",
      "Epoch 6/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6922 - acc: 0.5178 - val_loss: 0.6910 - val_acc: 0.5210\n",
      "Epoch 7/30\n",
      "43201/43201 [==============================] - 14s 314us/step - loss: 0.6915 - acc: 0.5211 - val_loss: 0.6933 - val_acc: 0.5105\n",
      "Epoch 8/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6909 - acc: 0.5297 - val_loss: 0.6950 - val_acc: 0.5105\n",
      "Epoch 9/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6903 - acc: 0.5315 - val_loss: 0.6947 - val_acc: 0.5230\n",
      "Epoch 10/30\n",
      "43201/43201 [==============================] - 13s 310us/step - loss: 0.6891 - acc: 0.5380 - val_loss: 0.6952 - val_acc: 0.5065\n",
      "Epoch 11/30\n",
      "43201/43201 [==============================] - 13s 311us/step - loss: 0.6876 - acc: 0.5437 - val_loss: 0.6957 - val_acc: 0.5165\n",
      "Epoch 12/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6870 - acc: 0.5454 - val_loss: 0.6968 - val_acc: 0.5120\n",
      "Epoch 13/30\n",
      "43201/43201 [==============================] - 13s 312us/step - loss: 0.6857 - acc: 0.5504 - val_loss: 0.6965 - val_acc: 0.5190\n",
      "Epoch 14/30\n",
      "43201/43201 [==============================] - 14s 313us/step - loss: 0.6848 - acc: 0.5557 - val_loss: 0.6960 - val_acc: 0.5280\n",
      "Epoch 15/30\n",
      "43201/43201 [==============================] - 15s 336us/step - loss: 0.6840 - acc: 0.5535 - val_loss: 0.6963 - val_acc: 0.5165\n",
      "Epoch 16/30\n",
      "43201/43201 [==============================] - 14s 316us/step - loss: 0.6824 - acc: 0.5597 - val_loss: 0.6993 - val_acc: 0.5235\n",
      "Epoch 17/30\n",
      "43201/43201 [==============================] - 14s 315us/step - loss: 0.6818 - acc: 0.5621 - val_loss: 0.6969 - val_acc: 0.5185\n",
      "Epoch 18/30\n",
      "43201/43201 [==============================] - 14s 315us/step - loss: 0.6806 - acc: 0.5651 - val_loss: 0.7016 - val_acc: 0.5065\n",
      "Epoch 19/30\n",
      "43201/43201 [==============================] - 14s 315us/step - loss: 0.6808 - acc: 0.5658 - val_loss: 0.7013 - val_acc: 0.5115\n",
      "Epoch 20/30\n",
      "43201/43201 [==============================] - 14s 315us/step - loss: 0.6797 - acc: 0.5667 - val_loss: 0.7050 - val_acc: 0.5030\n",
      "Epoch 21/30\n",
      "43201/43201 [==============================] - 14s 314us/step - loss: 0.6792 - acc: 0.5687 - val_loss: 0.7013 - val_acc: 0.5080\n",
      "Epoch 22/30\n",
      "43201/43201 [==============================] - 14s 314us/step - loss: 0.6788 - acc: 0.5690 - val_loss: 0.7039 - val_acc: 0.5035\n",
      "Epoch 23/30\n",
      "43201/43201 [==============================] - 14s 315us/step - loss: 0.6780 - acc: 0.5721 - val_loss: 0.7094 - val_acc: 0.5045\n",
      "Epoch 24/30\n",
      "43201/43201 [==============================] - 14s 314us/step - loss: 0.6768 - acc: 0.5725 - val_loss: 0.7080 - val_acc: 0.4990\n",
      "Epoch 25/30\n",
      "43201/43201 [==============================] - 14s 315us/step - loss: 0.6762 - acc: 0.5762 - val_loss: 0.7078 - val_acc: 0.5010\n",
      "Epoch 26/30\n",
      "43201/43201 [==============================] - 14s 314us/step - loss: 0.6757 - acc: 0.5761 - val_loss: 0.7061 - val_acc: 0.5095\n",
      "Epoch 27/30\n",
      "43201/43201 [==============================] - 14s 313us/step - loss: 0.6752 - acc: 0.5761 - val_loss: 0.7059 - val_acc: 0.5005\n",
      "Epoch 28/30\n",
      "43201/43201 [==============================] - 14s 313us/step - loss: 0.6743 - acc: 0.5805 - val_loss: 0.7065 - val_acc: 0.5045\n",
      "Epoch 29/30\n",
      "43201/43201 [==============================] - 14s 313us/step - loss: 0.6742 - acc: 0.5813 - val_loss: 0.7089 - val_acc: 0.5125\n",
      "Epoch 30/30\n",
      "43201/43201 [==============================] - 14s 314us/step - loss: 0.6736 - acc: 0.5823 - val_loss: 0.7071 - val_acc: 0.5010\n",
      "4799/4799 [==============================] - 1s 153us/step\n",
      "The result of testing the model against test data is:\n",
      "Test loss: 0.6898365846696708\n",
      "Test accuracy 0.5303188163207257:\n",
      " \n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "degree_of_repeat_for_signal = 14\n",
    "shift_percentage = 0.2\n",
    "data_generator_seed = 10\n",
    "Det = 'H1'\n",
    "N_rz = 50000\n",
    "\n",
    "batch_size = 64\n",
<<<<<<< Updated upstream
    "epochs =30\n",
=======
    "epochs = 30\n",
>>>>>>> Stashed changes
    "kfold_splits= 10\n",
    "weight_file_name = 'results/testing_weights_at_'\n",
    "validation = 2000\n",
    "#SNRs = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "#SNRs = [1.0, 2.0, 3.0, 4.0]\n",
    "#SNRs = [5.0, 6.0, 7.0, 8.0]\n",
    "#SNRs = [9.0,10.0]\n",
<<<<<<< Updated upstream
    "SNRs = [5.0]\n",
=======
    "SNRs = [1.0]\n",
>>>>>>> Stashed changes
    "tscores, history, signal_preds, test_label_saver_for_ROC = kfold_for_diff_SNRs(degree_of_repeat_for_signal, SNewaves, shift_percentage, data_generator_seed, Det, N_rz, ts, New_dt, New_sr,\n",
    "                                                                               batch_size, epochs, kfold_splits, weight_file_name, validation, SNRs)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
=======
   "execution_count": 23,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "The mean of test loss is 0.3143013902638638\n",
      " \n",
      "The standard deviation of test loss is 0.01718440374798914\n",
      " \n",
      "The mean of test accuracy is 0.8637155958972912\n",
      " \n",
      "The standard deviation of test accuracy is 0.010517432350688537\n"
=======
      "The mean of test_loss is 0.690396628772963\n",
      "The std of test_loss is 0.0015869907497294824\n",
      "The mean of test_acc is 0.5301678605869414\n",
      "The std of test_acc is 0.013149649290014713\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "t_scores = np.array([np.zeros((kfold_splits, 2)) for i in range(len(tscores))])\n",
    "for i in range(len(tscores)):\n",
    "    for j in range(kfold_splits):\n",
    "        t_scores[i][j][0] = tscores[i][j][0]\n",
    "        t_scores[i][j][1] = tscores[i][j][1]\n",
    "\n",
    "msg = 'The mean of test_loss is %s' %(np.mean(t_scores[0][:,0]))\n",
    "print(msg)\n",
    "#print(' ')\n",
    "\n",
    "msg = 'The std of test_loss is %s' %(np.std(t_scores[0][:,0]))\n",
    "print(msg)\n",
    "#print(' ')\n",
    "\n",
    "msg = 'The mean of test_acc is %s' %(np.mean(t_scores[0][:,1]))\n",
    "print(msg)\n",
    "#print(' ')\n",
    "\n",
    "msg = 'The std of test_acc is %s' %(np.std(t_scores[0][:,1]))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 21,
=======
   "execution_count": 24,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotlossacc(history, fontsize, epochs):\n",
    "    fig , axs = plt.subplots(2,1, sharex = True)\n",
    "    axs = axs.ravel()\n",
    "    # plot history\n",
    "    counter = 0\n",
    "    for history_i in history:\n",
    "        if counter == 0:\n",
    "            axs[0].plot(np.arange(epochs) + 1, history_i.history['loss'], label = 'Loss', linewidth = 1, color = 'b')\n",
    "            axs[0].plot(np.arange(epochs) + 1, history_i.history['val_loss'], label = 'Validation Loss', linewidth = 1, color = 'r')\n",
    "\n",
    "            axs[1].plot(np.arange(epochs) + 1, history_i.history['acc'], label = 'Accuracy', linewidth = 1, color =  'b')\n",
    "            axs[1].plot(np.arange(epochs) + 1, history_i.history['val_acc'], label = 'Validation Accurarcy', linewidth = 1, color =  'r')\n",
    "            # set labels\n",
    "            axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "            axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "            axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # legends\n",
    "            axs[0].legend(fontsize = fontsize)\n",
    "            axs[1].legend(fontsize = fontsize)\n",
    "        else:\n",
    "            axs[0].plot(np.arange(epochs) + 1, history_i.history['loss'], linewidth = 1, color = 'b')\n",
    "            axs[0].plot(np.arange(epochs) + 1, history_i.history['val_loss'], linewidth = 1, color = 'r')\n",
    "\n",
    "            axs[1].plot(np.arange(epochs) + 1, history_i.history['acc'], linewidth = 1, color = 'b')\n",
    "            axs[1].plot(np.arange(epochs) + 1, history_i.history['val_acc'], linewidth = 1, color = 'r')\n",
    "            # set labels\n",
    "            axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "            axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "            axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # legends\n",
    "            axs[0].legend(fontsize = fontsize)\n",
    "            axs[1].legend(fontsize = fontsize)\n",
    "        counter += 1\n",
    "    # grids\n",
    "    axs[0].grid()\n",
    "    axs[1].grid()\n",
    "    axs[0].set_xlim([1, epochs])\n",
    "    axs[0].set_ylim(bottom = 0)\n",
    "\n",
    "    axs[1].set_xlim([1, epochs])\n",
    "    axs[1].set_ylim(top = 1)\n",
    "\n",
    "    plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "    for ax in axs:\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 22,
=======
   "execution_count": 25,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "#history = history_saver[0][0]\n",
    "plotlossacc(history[0], fontsize, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_shelf(SNR):\n",
    "    nf = 255.0\n",
    "    RGB_choice = {\n",
    "        1 : [255.0, 206.0, 0.0],\n",
    "        2 : [49.0, 51.0, 53.0],\n",
    "        3 : [27.0, 161.0, 226.0],\n",
    "        4 : [222.0, 93.0, 90.0],\n",
    "        5 : [128.0, 211.0, 2.0],\n",
    "        6 : [194.0, 45.0, 140.0],\n",
    "        7 : [227.0, 37.0, 107.0],\n",
    "        8 : [255.0, 146.0, 93.0],\n",
    "        9 : [33.0, 206.0, 142.0],\n",
    "        10: [70.0, 33.0, 180.0]\n",
    "    }\n",
    "    #RGB[SNR]\n",
    "    RGB = [x /nf for x in RGB_choice[SNR]]\n",
    "    return RGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotlossacc_together(history, fontsize, SNR, Loss_top, Loss_bottom, Acc_top, Acc_bottom):\n",
    "    fig , axs = plt.subplots(2,1, sharex = True)\n",
    "    axs = axs.ravel()\n",
    "    # plot history\n",
    "    for history_10 in history:\n",
    "        counter = 0\n",
    "        for history_i in history_10[0:1]:\n",
    "            if counter == 0:\n",
    "                axs[0].plot(np.arange(epochs) + 1, history_i.history['loss'], label = 'Loss at SNR = %s' %(SNR), linewidth = 1, linestyle ='dashdot', color = color_shelf(SNR))\n",
    "                axs[0].plot(np.arange(epochs) + 1, history_i.history['val_loss'], label = 'Validation Loss at SNR = %s' %(SNR), linewidth = 1, linestyle ='-', color = color_shelf(SNR))\n",
    "\n",
    "                axs[1].plot(np.arange(epochs) + 1, history_i.history['acc'], label = 'Accuracy at SNR = %s' %(SNR), linewidth = 1,  linestyle ='dashdot', color =  color_shelf(SNR))\n",
    "                axs[1].plot(np.arange(epochs) + 1, history_i.history['val_acc'], label = 'Validation Accurarcy at SNR = %s' %(SNR), linewidth = 1, linestyle ='-', color =  color_shelf(SNR))\n",
    "                # set labels\n",
    "                axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "                axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "                axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "\n",
    "\n",
    "\n",
    "                # legends\n",
    "                axs[0].legend(fontsize = fontsize - 10)\n",
    "                axs[1].legend(fontsize = fontsize - 10)\n",
    "            else:\n",
    "                axs[0].plot(np.arange(epochs) + 1, history_i.history['loss'], linewidth = 1, linestyle ='dashdot', color = color_shelf(SNR))\n",
    "                axs[0].plot(np.arange(epochs) + 1, history_i.history['val_loss'], linewidth = 1, linestyle ='-', color = color_shelf(SNR))\n",
    "\n",
    "                axs[1].plot(np.arange(epochs) + 1, history_i.history['acc'], linewidth = 1, linestyle ='dashdot', color = color_shelf(SNR))\n",
    "                axs[1].plot(np.arange(epochs) + 1, history_i.history['val_acc'], linewidth = 1, linestyle ='-', color = color_shelf(SNR))\n",
    "                # set labels\n",
    "                axs[0].set_ylabel('Loss', fontsize = fontsize)\n",
    "                axs[1].set_xlabel('Epoch', fontsize = fontsize)\n",
    "                axs[1].set_ylabel('Acc', fontsize = fontsize)\n",
    "\n",
    "\n",
    "\n",
    "                # legends\n",
    "                axs[0].legend(fontsize = fontsize - 10)\n",
    "                axs[1].legend(fontsize = fontsize - 10)\n",
    "            counter += 1\n",
    "        SNR +=1    \n",
    "    # grids\n",
    "    axs[0].grid()\n",
    "    axs[1].grid()\n",
    "    axs[0].set_xlim([1, epochs])\n",
    "    #axs[0].set_ylim(top = Loss_top, bottom = Loss_bottom)\n",
    "    axs[0].set_xticks(np.arange(epochs) +1)\n",
    "    axs[1].set_xlim([1, epochs])\n",
    "    #axs[1].set_ylim(top = Acc_top, bottom = Acc_bottom)\n",
    "    axs[1].set_xticks(np.arange(epochs) +1)\n",
    "    plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "    for ax in axs:\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "#history = history_saver[0][0]\n",
    "Loss_top = 0.8\n",
    "Loss_bottom = 0.5\n",
    "Acc_top = 0.8\n",
    "Acc_bottom = 0.3\n",
    "plotlossacc_together(history, fontsize, 1, Loss_top, Loss_bottom, Acc_top, Acc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "#history = history_saver[0][0]\n",
    "Loss_top = 0.8\n",
    "Loss_bottom = 0.3\n",
    "Acc_top = 1.0\n",
    "Acc_bottom = 0.3\n",
    "plotlossacc_together(history[3:7], fontsize, 4, Loss_top, Loss_bottom, Acc_top, Acc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "#history = history_saver[0][0]\n",
    "Loss_top = 0.6\n",
    "Loss_bottom = 0.0\n",
    "Acc_top = 1.0\n",
    "Acc_bottom = 0.6\n",
    "plotlossacc_together(history[7:], fontsize, 8, Loss_top, Loss_bottom, Acc_top, Acc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathandname='1736waveformSNR1len17randombeginepoch30.pkl'\n",
    "fp = open(pathandname,\"w\")\n",
    "pickle.dump([tscores, history, signal_preds, test_label_saver_for_ROC], fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "with open(\"1824waveformSNR8910.pkl\") as f:\n",
    "    tscores, history, signal_preds, test_label_saver_for_ROC = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize =20\n",
    "i = 0\n",
    "SNR = i + 5\n",
    "epochs =20\n",
    "plot_tscores(t_scores[i], fontsize)\n",
    "#plot_roc(test_label_saver_for_ROC[i], signal_preds[i], SNR)\n",
    "#plotlossacc(history[i], fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tscores(tscores, fontsize):\n",
    "    ksplit = len(tscores)    \n",
    "    fig = plt.figure(figsize=(6,2.5), dpi= 100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    plt.scatter(np.arange(ksplit ) + 1, tscores[:,1], color = 'r', s = 200)\n",
    "    plt.grid()\n",
    "    ax = plt.gca()\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    \n",
    "    plt.xlim([1, ksplit])\n",
    "    #plt.ylim([0.94, 1])\n",
    "\n",
    "    #plt.xlabel('K-Fold iteration',fontsize = fontsize)\n",
    "    plt.ylabel('Test accuracy',fontsize = fontsize)\n",
    "\n",
    "    plt.show()\n",
    "    fig = plt.figure(figsize=(6,2.5), dpi= 100, facecolor='w', edgecolor='k')\n",
    "\n",
    "    plt.scatter(np.arange(ksplit ) + 1, tscores[:,0], color = 'r', s = 200)\n",
    "    plt.grid()\n",
    "    ax = plt.gca()\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(fontsize)\n",
    "        tick.label1.set_fontweight('normal')\n",
    "    \n",
    "    #plt.xlim([bottom = 0])\n",
    "    #plt.ylim([0.94, 1])\n",
    "\n",
    "    plt.xlabel('K-Fold iteration',fontsize = fontsize)\n",
    "    plt.ylabel('Test loss',fontsize = fontsize)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(test_label, signal_preds, SNR, plot):\n",
    "    if plot == True:\n",
    "        fig = plt.figure()\n",
    "    fa = []#[[] for i in range(len(test_label))]\n",
    "    ta = []#[[] for i in range(len(test_label))]\n",
    "    for i, j in zip(test_label, signal_preds):\n",
    "        tem_fa, tem_ta, _ = metrics.roc_curve(i[:,1], j[:,1])\n",
    "        fa.append(tem_fa)\n",
    "        ta.append(tem_ta)\n",
    "        if plot == True:\n",
    "            plt.plot(tem_fa, tem_ta, linewidth = 2, color = 'b')\n",
    "            plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "            plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "            plt.title('ROC curve for SNR %s'%(SNR), fontsize = fontsize)\n",
    "            plt.xlim([0, 1])\n",
    "            plt.ylim([0, 1])\n",
    "            plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "            \n",
    "             \n",
    "    if plot == True:            \n",
    "        plt.grid()\n",
    "        ax = plt.gca()\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label1.set_fontsize(fontsize)\n",
    "            tick.label1.set_fontweight('normal')\n",
    "        plt.show()\n",
    "    return fa, ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpta(fa, ta, given_FAP):\n",
    "     \n",
    "    ta_fun = interpolate.interp1d(fa, ta)\n",
    "    \n",
    "    ta_interp = ta_fun(given_FAP)\n",
    "    \n",
    "    return ta_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57456645, 0.41542672, 0.41542036, 0.41971365, 0.417178  , 0.4235626 , 0.42090472, 0.41190883, 0.40760758, 0.55043136])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_scores[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_FAR = 0.1;\n",
    "given_FAR2 = 0.01;\n",
    "given_FAR3 = 0.001;\n",
    "\n",
    "SNRs = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])\n",
    "ta_interp_01 = np.zeros(len(SNRs))\n",
    "ta_interp_001 = np.zeros(len(SNRs))\n",
    "ta_interp_0001 = np.zeros(len(SNRs))\n",
    "mark_best =[]\n",
    "for i in range(len(SNRs)):  \n",
    "    fa, ta = plot_roc(test_label_saver_for_ROC[i], signal_preds[i], SNRs[i], False)\n",
    "    j = np.argmin(t_scores[i][:,0])     \n",
    "    mark_best.append(j)\n",
    "    ta_interp_01[i] = interpta(fa[j], ta[j], given_FAR)\n",
    "    ta_interp_001[i] = interpta(fa[j], ta[j], given_FAR2)\n",
    "    ta_interp_0001[i] = interpta(fa[j], ta[j], given_FAR3)\n",
    "plt.plot(SNRs, ta_interp_01, linewidth = 2.0, color = 'r', label = 'False Alarm Rate = 0.1')\n",
    "plt.plot(SNRs, ta_interp_001, linewidth = 2.0, color = 'b', label = 'False Alarm Rate = 0.01')\n",
    "plt.plot(SNRs, ta_interp_0001, linewidth = 2.0, color = 'g', label = 'False Alarm Rate = 0.001')\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "plt.xlim([1, 10])\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc = 7, fontsize =20)\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_label_saver_for_ROC[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(SNRs)): \n",
    "    tem_fa, tem_ta, _ = metrics.roc_curve(test_label_saver_for_ROC[i][mark_best[i]][:,1], signal_preds[i][mark_best[i]][:,1])\n",
    "    plt.plot(tem_fa, tem_ta, linewidth = 2, color = color_shelf(SNRs[i]) , label = 'SNR = %s' %(SNRs[i]))\n",
    "    plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "    plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "    plt.title('ROC curves for SNR 1-10', fontsize = fontsize)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_interp2 = np.concatenate((ta_interp2, ta_interp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(10)+1, ta_interp2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.5362678864740342, 0.7173718978753794],\n",
       "  [0.5454910509926932, 0.7017857142857142],\n",
       "  [0.5556323049749647, 0.7101785714285714],\n",
       "  [0.5019249211038862, 0.7367857142857143],\n",
       "  [0.4887421712542798, 0.7449544561741757]]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tscores"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 23,
=======
   "execution_count": 28,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.argmin(t_scores[i][:,0])\n",
    "fa, ta, _ = metrics.roc_curve(test_label_saver_for_ROC[0][j][:,1], signal_preds[0][j][:,1])\n",
    "fig = plt.figure()\n",
    "plt.plot(fa, ta, linewidth = 2, color = 'b')\n",
    "plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "plt.title('ROC curve for SNR %s'%(5), fontsize = fontsize)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 24,
=======
   "execution_count": 33,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "fa, ta, _ = metrics.roc_curve(test_label_saver_for_ROC[0][j][:,1], signal_preds[0][j][:,1])\n",
    "fig = plt.figure()\n",
    "plt.loglog(fa, ta, linewidth = 2, color = 'b')\n",
    "plt.xlabel('False alarm probability',fontsize = fontsize)\n",
    "plt.ylabel('True alarm probability',fontsize = fontsize)\n",
    "plt.title('ROC curve for SNR %s'%(5), fontsize = fontsize)\n",
    "plt.xlim([0.001, 1])\n",
    "plt.ylim([0.001, 1])\n",
    "plt.subplots_adjust(left = 0.1, bottom = 0.1, right = 0.90, top = 0.95)\n",
    "\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(fontsize)\n",
    "    tick.label1.set_fontweight('normal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
